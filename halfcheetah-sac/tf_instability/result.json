{"episode_reward_max": -548.6084578723553, "episode_reward_min": -548.6084578723553, "episode_reward_mean": -548.6084578723553, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-548.6084578723553], "episode_lengths": [1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2648399307296707, "mean_processing_ms": 0.3791207914704924, "mean_inference_ms": 1.305492965134231}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 500, "num_steps_trained": 0, "num_steps_sampled": 1000, "sample_time_ms": 1.709, "replay_time_ms": 0.0, "grad_time_ms": 0.0, "update_time_ms": 0.003, "opt_peak_throughput": 0.0, "opt_samples": 0.0, "learner": {}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 1000, "episodes_total": 1, "training_iteration": 1, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_14-38-13", "timestamp": 1587047893, "time_this_iter_s": 3.363579273223877, "time_total_s": 3.363579273223877, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 3.363579273223877, "timesteps_since_restore": 1000, "iterations_since_restore": 1, "perf": {"cpu_util_percent": 89.3, "ram_util_percent": 9.58}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -325.77396087127937, "episode_reward_min": -579.2548494853795, "episode_reward_mean": -422.62692208126674, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-404.95899020386395, -468.0147212623178, -388.3275187990437, -467.9878895178665, -579.2548494853795, -393.0828110370479, -404.7994090732628, -354.5481042398681, -325.77396087127937, -439.52096632273725], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2692115508297803, "mean_processing_ms": 0.12212764643488043, "mean_inference_ms": 1.2116672253444212}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -372.06952674818945, "episode_reward_min": -548.6084578723553, "episode_reward_mean": -460.3389923102724, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-372.06952674818945, -548.6084578723553], "episode_lengths": [1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2661337667092498, "mean_processing_ms": 0.37961884608618157, "mean_inference_ms": 1.2737786850670925}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 1000, "num_steps_trained": 0, "num_steps_sampled": 2000, "sample_time_ms": 2.609, "replay_time_ms": 0.0, "grad_time_ms": 0.0, "update_time_ms": 0.004, "opt_peak_throughput": 0.0, "opt_samples": 0.0, "learner": {}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 2000, "episodes_total": 2, "training_iteration": 2, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_14-38-34", "timestamp": 1587047914, "time_this_iter_s": 2.7821826934814453, "time_total_s": 6.145761966705322, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 6.145761966705322, "timesteps_since_restore": 2000, "iterations_since_restore": 2, "perf": {"cpu_util_percent": 91.96451612903225, "ram_util_percent": 10.551612903225813}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -280.0589894696885, "episode_reward_min": -519.3999129748953, "episode_reward_mean": -401.3350378702885, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-519.3999129748953, -280.0589894696885, -436.3687382485304, -515.5716862986537, -444.78248673517083, -349.86484590078095, -384.278176917173, -354.9768087100852, -298.0279854113446, -430.020748036562], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2693929205440973, "mean_processing_ms": 0.12344440503881178, "mean_inference_ms": 1.2051153943500643}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -372.06952674818945, "episode_reward_min": -548.6084578723553, "episode_reward_mean": -446.0604184268673, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-417.5032706600572, -548.6084578723553, -372.06952674818945], "episode_lengths": [1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26681885897011665, "mean_processing_ms": 0.3794351456517569, "mean_inference_ms": 1.2570925698872384}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 1500, "num_steps_trained": 0, "num_steps_sampled": 3000, "sample_time_ms": 2.452, "replay_time_ms": 0.0, "grad_time_ms": 0.0, "update_time_ms": 0.003, "opt_peak_throughput": 0.0, "opt_samples": 0.0, "learner": {}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 3000, "episodes_total": 3, "training_iteration": 3, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_14-38-53", "timestamp": 1587047933, "time_this_iter_s": 2.8109216690063477, "time_total_s": 8.95668363571167, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 8.95668363571167, "timesteps_since_restore": 3000, "iterations_since_restore": 3, "perf": {"cpu_util_percent": 92.0148148148148, "ram_util_percent": 10.600000000000001}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -367.94324923195865, "episode_reward_min": -528.4315913367624, "episode_reward_mean": -442.18273512378, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-396.3290941771239, -445.3914458103221, -497.41661881248723, -468.0821689507214, -413.52864775139494, -459.26102123612947, -470.35388816777476, -528.4315913367624, -375.08962576312587, -367.94324923195865], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26653645090339845, "mean_processing_ms": 0.12161152302664538, "mean_inference_ms": 1.184849122226932}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -372.06952674818945, "episode_reward_min": -692.9160608187037, "episode_reward_mean": -507.77432902482644, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-692.9160608187037, -548.6084578723553, -372.06952674818945, -417.5032706600572], "episode_lengths": [1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2680531153478064, "mean_processing_ms": 0.3812019985996226, "mean_inference_ms": 1.2513236237978975}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 2000, "num_steps_trained": 0, "num_steps_sampled": 4000, "sample_time_ms": 2.41, "replay_time_ms": 0.0, "grad_time_ms": 0.0, "update_time_ms": 0.003, "opt_peak_throughput": 0.0, "opt_samples": 0.0, "learner": {}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 4000, "episodes_total": 4, "training_iteration": 4, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_14-39-11", "timestamp": 1587047951, "time_this_iter_s": 2.9505481719970703, "time_total_s": 11.90723180770874, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 11.90723180770874, "timesteps_since_restore": 4000, "iterations_since_restore": 4, "perf": {"cpu_util_percent": 92.00384615384615, "ram_util_percent": 10.676923076923076}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -329.22022557638616, "episode_reward_min": -584.1631909796532, "episode_reward_mean": -446.95007081854027, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-358.26097507520956, -584.1631909796532, -463.1727595240567, -455.4250372355091, -329.22022557638616, -473.4513437559589, -537.5536480843268, -343.37525692649103, -441.931340567299, -482.94693046051157], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26570995766914907, "mean_processing_ms": 0.12140996318880175, "mean_inference_ms": 1.1796389474609144}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -372.06952674818945, "episode_reward_min": -692.9160608187037, "episode_reward_mean": -533.4256162476047, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-636.0307651387184, -548.6084578723553, -372.06952674818945, -417.5032706600572, -692.9160608187037], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26774434335212216, "mean_processing_ms": 0.38056600390811623, "mean_inference_ms": 1.240985155043831}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 2500, "num_steps_trained": 0, "num_steps_sampled": 5000, "sample_time_ms": 2.091, "replay_time_ms": 0.0, "grad_time_ms": 0.0, "update_time_ms": 0.003, "opt_peak_throughput": 0.0, "opt_samples": 0.0, "learner": {}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 5000, "episodes_total": 5, "training_iteration": 5, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_14-39-29", "timestamp": 1587047969, "time_this_iter_s": 2.514247179031372, "time_total_s": 14.421478986740112, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 14.421478986740112, "timesteps_since_restore": 5000, "iterations_since_restore": 5, "perf": {"cpu_util_percent": 91.91153846153846, "ram_util_percent": 10.699999999999998}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -338.9690232511104, "episode_reward_min": -545.78837628894, "episode_reward_mean": -438.6543771009334, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-383.88588281061044, -454.4112535300462, -408.6043378662651, -525.2813590637215, -454.16108713728784, -545.78837628894, -466.35809192266515, -338.9690232511104, -373.4368892576353, -435.6474698810525], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2647226079602553, "mean_processing_ms": 0.12092455294448798, "mean_inference_ms": 1.17473683260252}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -368.7060056720575, "episode_reward_min": -692.9160608187037, "episode_reward_mean": -497.44512580754525, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-368.7060056720575, -372.06952674818945, -417.5032706600572, -692.9160608187037, -636.0307651387184], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2677651148579075, "mean_processing_ms": 0.37969711093061786, "mean_inference_ms": 1.2165877804286214}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 3000, "num_steps_trained": 0, "num_steps_sampled": 6000, "sample_time_ms": 2.671, "replay_time_ms": 0.0, "grad_time_ms": 0.0, "update_time_ms": 0.004, "opt_peak_throughput": 0.0, "opt_samples": 0.0, "learner": {}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 6000, "episodes_total": 6, "training_iteration": 6, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_14-39-48", "timestamp": 1587047988, "time_this_iter_s": 2.620630979537964, "time_total_s": 17.042109966278076, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 17.042109966278076, "timesteps_since_restore": 6000, "iterations_since_restore": 6, "perf": {"cpu_util_percent": 92.03076923076922, "ram_util_percent": 10.699999999999998}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -407.65861453406114, "episode_reward_min": -648.3951611012081, "episode_reward_mean": -481.6487966307116, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-413.49371559775557, -527.4086313652939, -407.65861453406114, -553.304319121584, -477.01784946836955, -480.41579411652856, -648.3951611012081, -411.9120384241076, -443.9100310073317, -452.9718115708755], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26494450069356346, "mean_processing_ms": 0.12105568351611458, "mean_inference_ms": 1.1775891615274328}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -368.7060056720575, "episode_reward_min": -692.9160608187037, "episode_reward_mean": -517.6827728912438, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-473.2577621666825, -417.5032706600572, -692.9160608187037, -636.0307651387184, -368.7060056720575], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2673996555921677, "mean_processing_ms": 0.3786322119434139, "mean_inference_ms": 1.2043419435103984}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 3500, "num_steps_trained": 0, "num_steps_sampled": 7000, "sample_time_ms": 2.33, "replay_time_ms": 0.0, "grad_time_ms": 0.0, "update_time_ms": 0.003, "opt_peak_throughput": 0.0, "opt_samples": 0.0, "learner": {}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 7000, "episodes_total": 7, "training_iteration": 7, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_14-40-06", "timestamp": 1587048006, "time_this_iter_s": 2.765897035598755, "time_total_s": 19.80800700187683, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 19.80800700187683, "timesteps_since_restore": 7000, "iterations_since_restore": 7, "perf": {"cpu_util_percent": 92.0153846153846, "ram_util_percent": 10.703846153846152}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -307.99941293590405, "episode_reward_min": -510.8244750567399, "episode_reward_mean": -417.6777668057818, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-335.8705289491427, -490.29554316176564, -438.35523156273507, -438.49234440681244, -401.4804234603863, -307.99941293590405, -460.1850996498665, -430.93160641802757, -510.8244750567399, -362.34300245643766], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26454551361524736, "mean_processing_ms": 0.12078081678845891, "mean_inference_ms": 1.1777115458071319}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -361.6499302461126, "episode_reward_min": -692.9160608187037, "episode_reward_mean": -506.5121048084549, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-361.6499302461126, -692.9160608187037, -636.0307651387184, -368.7060056720575, -473.2577621666825], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26766329040384956, "mean_processing_ms": 0.37882650600441053, "mean_inference_ms": 1.1993825177742548}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 4000, "num_steps_trained": 0, "num_steps_sampled": 8000, "sample_time_ms": 2.297, "replay_time_ms": 0.0, "grad_time_ms": 0.0, "update_time_ms": 0.003, "opt_peak_throughput": 0.0, "opt_samples": 0.0, "learner": {}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 8000, "episodes_total": 8, "training_iteration": 8, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_14-40-25", "timestamp": 1587048025, "time_this_iter_s": 3.1051478385925293, "time_total_s": 22.91315484046936, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 22.91315484046936, "timesteps_since_restore": 8000, "iterations_since_restore": 8, "perf": {"cpu_util_percent": 92.01111111111112, "ram_util_percent": 10.800000000000002}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -325.1097929734993, "episode_reward_min": -636.370254680728, "episode_reward_mean": -415.3721849302826, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-346.24900081613714, -636.370254680728, -378.08274916111503, -325.1097929734993, -390.97518140479116, -372.56688900614756, -518.5455363640721, -377.1303568455384, -406.70304432716085, -401.98904372363637], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26420550299227835, "mean_processing_ms": 0.12054178837268012, "mean_inference_ms": 1.1751933958579273}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -361.6499302461126, "episode_reward_min": -636.0307651387184, "episode_reward_mean": -450.14542389241524, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-411.0826562385054, -636.0307651387184, -368.7060056720575, -473.2577621666825, -361.6499302461126], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26721474261788136, "mean_processing_ms": 0.3776746319023433, "mean_inference_ms": 1.1922244948753948}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 4500, "num_steps_trained": 0, "num_steps_sampled": 9000, "sample_time_ms": 1.778, "replay_time_ms": 0.0, "grad_time_ms": 0.0, "update_time_ms": 0.003, "opt_peak_throughput": 0.0, "opt_samples": 0.0, "learner": {}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 9000, "episodes_total": 9, "training_iteration": 9, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_14-40-43", "timestamp": 1587048043, "time_this_iter_s": 2.8015096187591553, "time_total_s": 25.714664459228516, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 25.714664459228516, "timesteps_since_restore": 9000, "iterations_since_restore": 9, "perf": {"cpu_util_percent": 92.0653846153846, "ram_util_percent": 10.800000000000002}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -325.9356593340471, "episode_reward_min": -475.41500499196485, "episode_reward_mean": -399.5921154596757, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-325.9356593340471, -362.9329886556145, -396.89008147939836, -419.88751583390587, -475.41500499196485, -452.24798203210304, -426.5131827731369, -368.03215346312874, -367.70713807333203, -400.3594479601258], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2639511953683574, "mean_processing_ms": 0.12054264918296695, "mean_inference_ms": 1.175368640546294}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -361.6499302461126, "episode_reward_min": -473.2577621666825, "episode_reward_mean": -406.6593814837628, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-418.600553095456, -368.7060056720575, -473.2577621666825, -361.6499302461126, -411.0826562385054], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26782036929598496, "mean_processing_ms": 0.3781957033918251, "mean_inference_ms": 1.1910862439957612}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 5000, "num_steps_trained": 0, "num_steps_sampled": 10000, "sample_time_ms": 2.237, "replay_time_ms": 0.0, "grad_time_ms": 0.0, "update_time_ms": 0.003, "opt_peak_throughput": 0.0, "opt_samples": 0.0, "learner": {}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 10000, "episodes_total": 10, "training_iteration": 10, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_14-41-02", "timestamp": 1587048062, "time_this_iter_s": 2.725681781768799, "time_total_s": 28.440346240997314, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 28.440346240997314, "timesteps_since_restore": 10000, "iterations_since_restore": 10, "perf": {"cpu_util_percent": 92.04074074074076, "ram_util_percent": 10.800000000000002}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -365.8708018470139, "episode_reward_min": -755.5084115734583, "episode_reward_mean": -543.4363532989357, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-617.1988654970318, -365.8708018470139, -398.9916574215979, -688.9823856987792, -585.3547817097666, -679.3002916164414, -453.5285864605352, -755.5084115734583, -506.80298553092064, -382.82476563381334], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26382373200193776, "mean_processing_ms": 0.12047257829042193, "mean_inference_ms": 1.1744046239614203}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -361.6499302461126, "episode_reward_min": -473.2577621666825, "episode_reward_mean": -415.6716605936548, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-413.76740122151716, -473.2577621666825, -361.6499302461126, -411.0826562385054, -418.600553095456], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2711326864563897, "mean_processing_ms": 0.3817121911438936, "mean_inference_ms": 1.2101960243903076}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 5500, "num_steps_trained": 254464, "num_steps_sampled": 11000, "sample_time_ms": 3.348, "replay_time_ms": 23.926, "grad_time_ms": 61.822, "update_time_ms": 0.005, "opt_peak_throughput": 4140.891, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[7.1969657]\n [5.6266203]\n [5.596156 ]\n [5.478788 ]\n [7.226878 ]\n [5.7101736]\n [7.828367 ]\n [6.425722 ]\n [6.706726 ]\n [6.097624 ]\n [5.5781903]\n [5.8775916]\n [6.1143513]\n [6.5474334]\n [4.9705324]\n [7.9348426]\n [6.9906826]\n [6.6181083]\n [6.2420616]\n [7.7515407]\n [5.0136003]\n [7.9130654]\n [6.58978  ]\n [5.678516 ]\n [4.525701 ]\n [4.8263645]\n [6.6667256]\n [7.058288 ]\n [6.31133  ]\n [6.3537755]\n [6.0889354]\n [4.4608307]\n [3.8969855]\n [6.3759665]\n [5.444393 ]\n [7.1079383]\n [4.844768 ]\n [8.412414 ]\n [5.3251266]\n [7.145836 ]\n [4.266337 ]\n [4.6305075]\n [5.689916 ]\n [7.3436236]\n [5.614971 ]\n [6.920319 ]\n [7.1997643]\n [5.1142583]\n [5.495715 ]\n [6.569085 ]\n [5.9749775]\n [6.2144117]\n [5.912915 ]\n [5.0211105]\n [7.4290843]\n [5.865971 ]\n [6.4121356]\n [6.9409156]\n [4.5039625]\n [6.0977526]\n [6.277742 ]\n [5.984858 ]\n [5.618305 ]\n [6.4791665]\n [5.977819 ]\n [6.367404 ]\n [4.8943963]\n [7.431589 ]\n [5.44818  ]\n [5.155996 ]\n [7.738754 ]\n [6.377075 ]\n [6.352451 ]\n [4.146781 ]\n [6.351927 ]\n [5.9274764]\n [6.7275076]\n [7.3500547]\n [6.7162046]\n [4.923216 ]\n [5.406113 ]\n [6.3295765]\n [6.928152 ]\n [5.684403 ]\n [6.277742 ]\n [6.642158 ]\n [6.870499 ]\n [5.6266203]\n [6.007073 ]\n [7.493647 ]\n [7.0102124]\n [5.609669 ]\n [6.88539  ]\n [7.359376 ]\n [2.7194102]\n [7.066866 ]\n [4.9618893]\n [6.3519726]\n [5.8216605]\n [7.291111 ]\n [8.442508 ]\n [4.7036805]\n [7.0879245]\n [7.5070114]\n [8.299725 ]\n [8.552794 ]\n [6.005603 ]\n [5.613555 ]\n [7.7568126]\n [4.841666 ]\n [8.187203 ]\n [7.283105 ]\n [5.383137 ]\n [6.275373 ]\n [5.004696 ]\n [5.7390428]\n [4.10408  ]\n [8.338989 ]\n [5.2331834]\n [7.3967047]\n [7.549851 ]\n [7.3509564]\n [6.4578204]\n [5.5400987]\n [5.1584983]\n [6.2123933]\n [5.4594345]\n [4.587904 ]\n [5.5313406]\n [5.8005905]\n [7.4843397]\n [7.7862153]\n [4.909751 ]\n [6.7623415]\n [8.860296 ]\n [5.8646445]\n [4.435819 ]\n [6.581501 ]\n [8.443731 ]\n [4.5683203]\n [6.3811884]\n [7.1068726]\n [6.6214137]\n [3.9047456]\n [4.7774835]\n [6.018167 ]\n [7.498549 ]\n [5.661183 ]\n [5.1355233]\n [4.31551  ]\n [5.4308786]\n [5.1238346]\n [5.6358128]\n [5.835955 ]\n [7.3287435]\n [7.9465075]\n [7.873264 ]\n [5.058479 ]\n [4.9586077]\n [4.9499264]\n [7.103594 ]\n [7.657969 ]\n [6.169945 ]\n [6.8057017]\n [4.9779167]\n [6.7570314]\n [7.6154966]\n [6.500336 ]\n [7.0764146]\n [4.5672455]\n [5.258764 ]\n [5.5125933]\n [6.95183  ]\n [7.298451 ]\n [5.3881397]\n [4.4212737]\n [5.0099535]\n [6.585868 ]\n [6.092145 ]\n [6.1305614]\n [3.3418787]\n [8.090153 ]\n [8.682672 ]\n [8.917596 ]\n [6.66643  ]\n [4.1945715]\n [5.4657044]\n [6.6145954]\n [5.304855 ]\n [5.2869773]\n [6.8557568]\n [5.9497857]\n [6.5806303]\n [4.639965 ]\n [7.4609256]\n [6.88455  ]\n [6.4113874]\n [5.2480884]\n [7.4266076]\n [7.225574 ]\n [7.414974 ]\n [5.193196 ]\n [6.598441 ]\n [6.4044557]\n [6.163154 ]\n [6.286347 ]\n [6.3776994]\n [6.791454 ]\n [7.006762 ]\n [4.692505 ]\n [5.8348646]\n [5.813503 ]\n [6.9812965]\n [7.3473935]\n [7.8434157]\n [8.184997 ]\n [8.312848 ]\n [6.079144 ]\n [6.9523625]\n [6.346057 ]\n [4.75286  ]\n [7.9882364]\n [2.7194102]\n [7.676095 ]\n [6.712789 ]\n [5.4035177]\n [7.971372 ]\n [7.456583 ]\n [5.784323 ]\n [5.4194427]\n [6.1417265]\n [6.2272196]\n [8.197945 ]\n [4.895286 ]\n [5.597499 ]\n [5.697358 ]\n [8.64762  ]\n [3.9232666]\n [5.804634 ]\n [7.5380692]\n [6.181029 ]\n [5.042463 ]\n [7.5725203]\n [5.7180786]\n [8.104992 ]\n [6.673164 ]\n [4.985055 ]\n [5.465504 ]\n [8.694721 ]\n [4.565056 ]\n [5.5324907]\n [5.8201127]\n [5.9695706]\n [7.0283704]\n [5.6407423]\n [6.1998353]]", "q_t_selected": "[7.1969657 5.6266203 5.596156  5.478788  7.226878  5.7101736 7.828367\n 6.425722  6.706726  6.097624  5.5781903 5.8775916 6.1143513 6.5474334\n 4.9705324 7.9348426 6.9906826 6.6181083 6.2420616 7.7515407 5.0136003\n 7.9130654 6.58978   5.678516  4.525701  4.8263645 6.6667256 7.058288\n 6.31133   6.3537755 6.0889354 4.4608307 3.8969855 6.3759665 5.444393\n 7.1079383 4.844768  8.412414  5.3251266 7.145836  4.266337  4.6305075\n 5.689916  7.3436236 5.614971  6.920319  7.1997643 5.1142583 5.495715\n 6.569085  5.9749775 6.2144117 5.912915  5.0211105 7.4290843 5.865971\n 6.4121356 6.9409156 4.5039625 6.0977526 6.277742  5.984858  5.618305\n 6.4791665 5.977819  6.367404  4.8943963 7.431589  5.44818   5.155996\n 7.738754  6.377075  6.352451  4.146781  6.351927  5.9274764 6.7275076\n 7.3500547 6.7162046 4.923216  5.406113  6.3295765 6.928152  5.684403\n 6.277742  6.642158  6.870499  5.6266203 6.007073  7.493647  7.0102124\n 5.609669  6.88539   7.359376  2.7194102 7.066866  4.9618893 6.3519726\n 5.8216605 7.291111  8.442508  4.7036805 7.0879245 7.5070114 8.299725\n 8.552794  6.005603  5.613555  7.7568126 4.841666  8.187203  7.283105\n 5.383137  6.275373  5.004696  5.7390428 4.10408   8.338989  5.2331834\n 7.3967047 7.549851  7.3509564 6.4578204 5.5400987 5.1584983 6.2123933\n 5.4594345 4.587904  5.5313406 5.8005905 7.4843397 7.7862153 4.909751\n 6.7623415 8.860296  5.8646445 4.435819  6.581501  8.443731  4.5683203\n 6.3811884 7.1068726 6.6214137 3.9047456 4.7774835 6.018167  7.498549\n 5.661183  5.1355233 4.31551   5.4308786 5.1238346 5.6358128 5.835955\n 7.3287435 7.9465075 7.873264  5.058479  4.9586077 4.9499264 7.103594\n 7.657969  6.169945  6.8057017 4.9779167 6.7570314 7.6154966 6.500336\n 7.0764146 4.5672455 5.258764  5.5125933 6.95183   7.298451  5.3881397\n 4.4212737 5.0099535 6.585868  6.092145  6.1305614 3.3418787 8.090153\n 8.682672  8.917596  6.66643   4.1945715 5.4657044 6.6145954 5.304855\n 5.2869773 6.8557568 5.9497857 6.5806303 4.639965  7.4609256 6.88455\n 6.4113874 5.2480884 7.4266076 7.225574  7.414974  5.193196  6.598441\n 6.4044557 6.163154  6.286347  6.3776994 6.791454  7.006762  4.692505\n 5.8348646 5.813503  6.9812965 7.3473935 7.8434157 8.184997  8.312848\n 6.079144  6.9523625 6.346057  4.75286   7.9882364 2.7194102 7.676095\n 6.712789  5.4035177 7.971372  7.456583  5.784323  5.4194427 6.1417265\n 6.2272196 8.197945  4.895286  5.597499  5.697358  8.64762   3.9232666\n 5.804634  7.5380692 6.181029  5.042463  7.5725203 5.7180786 8.104992\n 6.673164  4.985055  5.465504  8.694721  4.565056  5.5324907 5.8201127\n 5.9695706 7.0283704 5.6407423 6.1998353]", "twin_q_t_selected": "[7.555225  5.918284  5.8916655 5.918431  7.2392335 5.610869  8.491411\n 7.4222026 6.6937075 6.461364  5.6594    6.800596  5.9116254 6.0323534\n 5.0815516 7.9234934 6.4701805 7.168585  5.8460093 7.684129  4.303729\n 8.073545  7.6267614 5.560474  5.1121116 5.497787  5.9111066 7.032048\n 6.376542  6.611134  6.145303  4.1899652 4.117502  6.4838896 5.5715075\n 6.977418  4.7369843 7.93614   5.111693  7.39979   4.693286  5.1074967\n 5.2300797 7.319463  5.819861  6.71548   7.310277  5.1686187 6.2301946\n 6.460164  5.726846  6.719585  6.1523952 5.1787605 6.808462  5.907613\n 6.2261305 6.677925  5.064808  5.3684306 7.076007  6.4142294 5.4290996\n 6.5934052 5.6621323 6.451333  4.5011015 7.0952835 5.8291435 5.5183144\n 7.631691  5.921586  6.4584775 3.6996164 6.9251122 6.0785413 7.120286\n 7.3957834 6.9195786 4.875217  5.525694  6.613306  7.5466022 5.1194468\n 7.076007  6.657729  7.526726  5.918284  6.1520457 7.8153615 7.1145477\n 6.0654993 6.6492114 7.4155283 3.6119897 7.338752  5.221383  6.742767\n 5.9962792 6.4608064 8.718673  4.373197  6.9710217 7.5178747 8.026983\n 8.254836  6.586085  6.245247  7.736954  4.7916746 8.838261  6.8594136\n 5.6818776 5.8453784 5.4846435 5.985993  4.8117614 7.8576155 5.6825266\n 7.1600842 7.413906  6.889219  5.824526  5.1794167 5.414582  5.9523296\n 5.281283  4.509315  5.3429074 5.7735367 7.8476114 7.4166884 5.066173\n 6.622179  8.21965   5.526626  4.6095004 6.813751  7.874184  4.865691\n 6.6966047 6.7405043 6.397088  3.575612  5.06679   6.0112143 7.735127\n 6.039948  4.3754234 5.046978  5.4293656 5.342646  5.8681083 5.2328777\n 7.6868606 8.516273  7.6439414 5.1249394 5.2713523 5.000931  7.0076475\n 7.463869  6.526364  6.46691   5.4539995 6.4265904 7.1529007 6.7300935\n 6.32766   4.680767  5.3179092 5.482748  6.7814846 7.517348  6.0225706\n 4.3102174 5.086535  7.020998  5.978469  5.903583  3.679501  7.7939506\n 7.49767   9.753308  6.716756  4.495441  6.0276732 6.040177  5.4758053\n 5.08261   6.0864167 6.426606  7.077512  4.787133  6.637501  6.1189046\n 5.656315  5.1532207 7.4602795 6.942743  7.2792673 5.3418384 6.5506926\n 6.6385984 5.8880324 6.5707793 6.4659944 6.1804314 6.1069207 4.5957756\n 5.707362  6.0500164 6.4418063 7.6389647 7.350534  7.884823  8.672209\n 5.5975604 7.227943  7.2029686 4.869094  8.538599  3.6119897 7.3104644\n 6.259046  5.7316113 8.558222  7.773371  6.024897  5.550241  6.262102\n 6.1502004 7.6697683 5.05148   5.798417  5.4292693 8.05854   4.1682653\n 5.805231  7.859463  6.2855897 4.859123  7.825411  5.8449554 8.429015\n 7.1275268 5.2359314 5.5490775 8.553881  4.5678945 6.4813147 5.710309\n 6.6251493 7.3823104 5.363783  5.3863435]", "q_t_selected_target": "[ 5.946825   4.4251285  4.969386   4.253948   7.0880485  5.525172\n  5.234021   6.1681504  5.392961   7.147935   4.975655   6.8371773\n  5.4156165  7.2048106  5.919798   7.001265   6.080077   7.2348022\n  6.589256   5.171138   5.3802905  6.9814873  7.2609954  5.264928\n  3.066135   7.0517077  5.98372    6.992804   4.753373   6.052016\n  6.138496   5.1373076  2.5750375  5.9314446  5.897786   9.219434\n  4.5397935  5.9933662  4.6801724  9.023416   4.0909605  3.615788\n  5.6766973  7.8358727  4.319104   5.260873   6.5064516  5.5155516\n  6.970695   6.4178734  6.9100347  6.5929604  7.319468   5.9795594\n  7.03621    6.942126   4.396111   6.007248   3.8697257  6.8709555\n  8.077246   6.3590937  7.0430803  7.3171244  5.9105415  6.5630445\n  5.9119487  6.9307404  5.749233   3.9284768  9.433157   5.4474673\n  5.7850227  4.542      6.2547736  5.8271017  6.7007437  7.3034487\n  7.7752323  3.6656027  6.0883183  5.540965   6.0465465  3.2203474\n  8.125486   7.1538534  8.034574   4.668881   6.6988277  7.436179\n  7.7619743  6.826971   3.0872283  6.5914674  1.371494   8.967309\n  5.8378544  6.8283944  3.601656   5.76116    6.6740503  7.166336\n  6.1349573  8.210821   8.550887   6.2823343  4.1341043  5.5840597\n  9.473002   4.510533   7.0132565  7.955517   3.8358912  5.4287996\n  6.092765   5.1913576  3.4339614  8.036663   5.1868234  8.513626\n  8.106562   6.2819433  5.4748087  5.1780543  4.2072096  6.716219\n  6.025146   5.5070233  5.934932   6.1331806  8.700588   9.52703\n  3.6817813  7.8377924 10.630589   5.932682   5.099264   5.0019665\n  7.7451043  5.007721   7.112015   7.2497106  6.306234   2.1398275\n  3.8002477  6.2835617  7.7610373  4.1476536  6.489164   4.526793\n  6.424586   5.350074   5.514901   5.254376   8.168683   6.472095\n  7.3136554  7.414417   4.566667   5.3569965  6.190219   6.956824\n  7.3696675  6.265305   4.084302   5.6147013  8.642164   7.204614\n  7.6017365  2.4853745  5.537454   5.5638757  7.216469   7.181789\n  5.5725265  3.9528713  6.181713   6.701308   5.898496   6.309206\n  3.9873445  7.644106   9.004287  11.150114   7.412858   5.23619\n  5.673854   7.7836347  5.178873   6.1522098  6.795858   3.9264808\n  7.4565206  4.8087735  3.7041898  6.2660913  4.8250103  5.428603\n  7.479734   5.344496   8.156687   5.2175426  6.4869304  8.333307\n  6.1140437  5.7012086  8.010432   7.6771183  8.751924   4.338642\n  7.4542456  5.275642   7.3736353  4.9211087  8.598452   6.7608337\n  9.828799   6.388332   7.8338103  6.808815   5.1830573  8.928366\n  2.498225   7.700927   6.940664   4.6020355  8.473109   7.6322994\n  4.175562   4.8065577  5.432874   6.1040535  8.188561   2.92127\n  5.725401   5.602799   7.750785   4.8781633  4.857401   7.366542\n  6.9497423  6.138593   6.60377    5.6842723  5.981282   6.481504\n  5.1445804  5.9088373  8.19902    4.6112947  4.570001   6.876468\n  6.147898   8.127304   3.9995441  5.734805 ]", "q_tp1_best_masked": "[6.525482  5.3871946 5.8739357 5.279577  6.801819  5.845988  6.737089\n 7.4318433 5.7852745 6.834695  6.638407  6.996647  6.2227974 6.993203\n 6.385122  6.6209617 6.133223  6.7987876 6.590413  6.422907  6.693942\n 6.697603  7.07454   6.3743777 4.2248487 7.5519133 6.361431  7.0007486\n 6.2309847 6.545886  7.1033773 6.1855474 4.745159  6.713993  6.3445654\n 8.169502  6.044919  5.956312  6.5433555 7.9709167 5.3061533 4.9553785\n 6.03625   7.5364876 5.6617107 6.049473  6.28804   6.4906425 7.6564803\n 6.548519  7.655055  7.601175  7.050388  6.3340874 6.7173014 7.2318115\n 6.1782236 6.291714  5.6105003 7.0454845 7.6992497 6.798765  7.534377\n 6.4923716 6.807739  6.872347  6.923207  6.1941576 6.23678   5.261273\n 8.5778475 7.0764894 6.302101  6.298683  6.093239  6.3198075 6.9072723\n 6.9774055 7.1058826 5.242011  6.5604057 6.542965  6.544668  4.742923\n 7.747978  7.312258  7.6196384 5.633409  7.4470215 7.1648836 7.8348036\n 6.971964  5.2247    6.7836866 4.4403734 8.385719  6.2686253 7.3208923\n 4.871973  5.7443795 7.336725  7.180393  6.892969  7.3396435 7.9557934\n 5.950506  5.0336103 6.6334095 8.97674   5.044657  6.589064  7.4631424\n 4.8443003 6.475489  6.350364  6.3742647 5.193574  7.8404145 5.634799\n 7.731963  8.217808  6.0266123 5.956816  6.350877  5.6493616 6.797406\n 5.8367376 6.6701193 7.219499  6.787105  8.314495  8.474585  5.1019645\n 7.687108  8.901312  6.8078365 6.5928426 5.472329  7.306849  5.4205885\n 7.7257223 7.1268196 6.4990425 4.5604634 5.334711  6.5123096 7.395872\n 5.1313977 6.925421  5.993268  6.4369144 5.9431686 6.964307  5.868202\n 7.951954  6.6767235 6.892097  7.288515  5.554098  6.307474  6.3608847\n 6.601199  7.6752973 6.132928  6.2915325 6.4957886 8.207494  7.3611393\n 7.651075  4.54941   6.570656  6.74718   6.901204  7.6341524 6.564101\n 5.67486   7.311971  6.2728176 6.821837  6.5826635 5.4599037 6.783715\n 8.397106  9.9294405 7.733865  6.6382103 5.9472227 7.1270785 6.3066683\n 6.976722  7.2915545 5.069389  7.561528  6.121085  4.944304  6.582244\n 5.8917007 6.3411436 7.2698717 5.608592  7.3830943 6.3348236 6.996187\n 8.627519  6.5732613 6.244667  8.215506  7.2386656 8.241884  5.5807137\n 7.286805  6.732612  7.0341806 5.6982937 8.142537  6.2502604 8.157461\n 6.8349996 7.301009  6.4344807 5.9868402 8.463892  5.5784855 7.1508217\n 7.2612987 6.3418827 7.9727244 7.5334034 5.938815  5.7068644 6.0750113\n 5.8733463 7.5570316 4.7479467 6.3738556 5.969923  6.985672  6.154849\n 6.0661564 6.6265154 6.3564243 7.1664968 6.86648   6.425729  6.8239756\n 6.8596735 5.566882  6.6244335 7.4815807 5.5942965 5.520301  7.120553\n 6.586455  7.677294  5.456576  6.5907507]", "policy_t": "[[ 0.19711101  0.21668899 -0.94332564 -0.13465732 -0.3552732   0.944365  ]\n [ 0.2571963   0.619472   -0.08252537 -0.10100091  0.5418124  -0.8895855 ]\n [ 0.10309243 -0.11152065  0.47786164 -0.23217964 -0.43562293 -0.04264683]\n ...\n [ 0.9451058   0.7960141  -0.6847021  -0.6729571  -0.420694   -0.7390633 ]\n [ 0.20488262  0.8414296   0.7718308  -0.83915883  0.23591101 -0.89777946]\n [ 0.4619887   0.9747127   0.35503674 -0.05530632  0.86164474  0.519969  ]]", "td_error": "[1.4292703  1.3473237  0.7745247  1.4446611  0.14500737 0.13534904\n 2.925868   0.7558119  1.3072557  0.8684411  0.6431401  0.49808335\n 0.5973718  0.91491723 0.8937559  0.92790294 0.6503544  0.3414557\n 0.5452204  2.5466971  0.7216258  1.0118182  0.5184908  0.35456705\n 1.7527714  1.889632   0.37780952 0.05236411 1.5905628  0.430439\n 0.0281837  0.8119097  1.4322064  0.49848342 0.38983583 2.1767557\n 0.25108266 2.1809106  0.53823733 1.7506027  0.38885093 1.2532141\n 0.22991824 0.50432944 1.3983119  1.5570266  0.748569   0.37411308\n 1.1077402  0.09675121 1.0591228  0.2525866  1.2868128  0.8796239\n 0.31031108 1.0553339  1.923022   0.8021724  0.9146595  1.1378639\n 1.4003713  0.21468568 1.519378   0.7808385  0.15784335 0.15367603\n 1.2141998  0.33269596 0.19048166 1.4086783  1.7479346  0.7018633\n 0.62044144 0.6188011  0.3837459  0.17590714 0.22315311 0.06947041\n 0.9573407  1.2336137  0.6224148  0.9304762  1.1908307  2.1815774\n 1.448612   0.5039098  0.8359611  1.1035712  0.6192684  0.21832514\n 0.69959426 0.9893868  3.6800723  0.79598475 1.7942059  1.7645001\n 0.7462182  0.2810247  2.307314   1.1147988  1.9065399  2.6278973\n 0.89451575 0.6983781  0.3875332  2.121481   2.1617396  0.3453412\n 1.726119   0.30613756 1.4994755  0.88425756 1.6966162  0.63157606\n 0.8480952  0.6711602  1.0239594  0.2406869  0.27103162 1.2352316\n 0.62468314 0.8381443  0.66636443 0.18170333 1.0793304  0.6338575\n 0.6547873  0.95841384 0.49780822 0.34611702 1.0346127  1.9255781\n 1.3061807  1.1455321  2.0906153  0.23704672 0.57660437 1.6956596\n 0.4138534  0.29071522 0.5731182  0.32602215 0.203017   1.6003513\n 1.1218891  0.26887107 0.14419937 1.7029119  1.7336905  0.3657341\n 0.9944637  0.11683345 0.23705935 0.3015387  0.66088104 1.759295\n 0.44494724 2.3227077  0.5483129  0.38156796 0.86540174 0.6040952\n 1.0215132  0.37100077 1.1316562  0.9771097  1.2579656  0.58939934\n 0.8996992  2.1386318  0.24911761 0.06620502 0.34981155 0.22611046\n 0.31721544 0.41287422 1.1334689  0.21756506 0.13681078 0.2921338\n 0.47665465 0.29794574 0.9141159  1.814662   0.7212651  0.8911836\n 0.2809844  1.4562485  0.21145701 0.96741605 0.38467002 2.2617152\n 0.6274495  0.09522438 3.3450234  0.38282275 1.2088408  0.22794867\n 0.03629041 1.7396626  0.809566   0.07432127 0.08763647 1.8117802\n 0.13756084 0.7273545  1.5885854  1.1911757  2.1950822  0.30549812\n 1.6831322  0.6561177  0.66208386 2.5720704  1.0014768  1.274076\n 1.3362708  0.5499797  0.7436576  0.42845583 0.37208033 0.664948\n 0.667475   0.20764709 0.45474625 0.96552896 0.29342484 0.1583941\n 1.7290483  0.67828417 0.7690401  0.08465648 0.26408815 2.052113\n 0.1004591  0.13404441 0.6022954  0.83239734 0.9475317  0.33222437\n 0.71643305 1.1878002  1.0951958  0.09724474 2.2857213  0.41884136\n 0.12543821 0.40154648 0.42528057 0.04481959 1.4369016  1.1112573\n 0.3277893  0.9219637  1.5027184  0.4067459 ]", "mean_td_error": 0.8792904615402222, "actor_loss": -9.075571060180664, "critic_loss": 0.6253136396408081, "alpha_loss": -3.0153846740722656, "alpha_value": 0.7378185391426086, "target_entropy": -6, "mean_q": 6.236472129821777, "max_q": 8.917595863342285, "min_q": 2.7194101810455322, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 11000, "episodes_total": 11, "training_iteration": 11, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_14-42-44", "timestamp": 1587048164, "time_this_iter_s": 86.31267142295837, "time_total_s": 114.75301766395569, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 114.75301766395569, "timesteps_since_restore": 11000, "iterations_since_restore": 11, "perf": {"cpu_util_percent": 92.1548611111111, "ram_util_percent": 11.021527777777777}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -174.55826933898697, "episode_reward_min": -383.95646758839825, "episode_reward_mean": -269.42851766161345, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-245.03060131805677, -213.05653792546852, -191.3376937316171, -362.3523194519042, -174.55826933898697, -187.7717781354521, -377.45058998532454, -383.95646758839825, -315.41719009051076, -243.35372905041496], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2632781889898968, "mean_processing_ms": 0.1203088898830889, "mean_inference_ms": 1.1719287169514891}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -213.5742182258041, "episode_reward_min": -418.600553095456, "episode_reward_mean": -363.73495180547906, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-213.5742182258041, -361.6499302461126, -411.0826562385054, -418.600553095456, -413.76740122151716], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.27580540398462683, "mean_processing_ms": 0.38759568687518753, "mean_inference_ms": 1.2447376597760367}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 6000, "num_steps_trained": 510464, "num_steps_sampled": 12000, "sample_time_ms": 2.767, "replay_time_ms": 23.167, "grad_time_ms": 58.362, "update_time_ms": 0.005, "opt_peak_throughput": 4386.449, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[ 7.391116 ]\n [ 8.350364 ]\n [10.361994 ]\n [11.028475 ]\n [ 7.4240437]\n [10.658261 ]\n [10.667141 ]\n [ 7.8419986]\n [10.995722 ]\n [10.189787 ]\n [ 7.6816244]\n [ 9.913637 ]\n [ 9.034861 ]\n [13.318498 ]\n [ 6.2364755]\n [ 9.833154 ]\n [12.715238 ]\n [ 9.431137 ]\n [ 7.9143147]\n [ 9.100948 ]\n [ 8.27346  ]\n [10.023747 ]\n [ 8.434995 ]\n [12.815711 ]\n [10.383416 ]\n [ 8.425577 ]\n [10.1428175]\n [10.996573 ]\n [10.713232 ]\n [ 7.2854257]\n [ 9.4266405]\n [ 7.9022775]\n [ 9.831214 ]\n [11.280633 ]\n [ 9.078873 ]\n [10.31532  ]\n [ 8.170631 ]\n [ 9.3587055]\n [10.2387495]\n [10.663352 ]\n [14.879642 ]\n [ 9.869588 ]\n [ 7.813685 ]\n [ 7.5355916]\n [ 8.073071 ]\n [ 8.781163 ]\n [ 8.090624 ]\n [ 8.566694 ]\n [ 9.099518 ]\n [ 7.7508783]\n [11.009711 ]\n [ 8.425577 ]\n [ 8.572949 ]\n [10.836345 ]\n [ 9.936294 ]\n [ 8.389316 ]\n [ 8.986641 ]\n [ 6.7688437]\n [ 8.909075 ]\n [ 8.887826 ]\n [10.413125 ]\n [ 7.1596866]\n [10.162676 ]\n [ 6.782174 ]\n [10.691516 ]\n [10.162101 ]\n [ 9.420098 ]\n [10.658261 ]\n [ 8.88477  ]\n [ 9.251779 ]\n [11.358846 ]\n [ 9.034504 ]\n [10.21644  ]\n [10.796634 ]\n [ 8.67676  ]\n [ 9.00271  ]\n [10.850784 ]\n [ 7.636617 ]\n [ 9.213304 ]\n [ 8.545329 ]\n [ 7.982029 ]\n [ 8.824532 ]\n [11.435931 ]\n [ 8.686239 ]\n [ 8.118059 ]\n [ 8.153281 ]\n [ 9.16584  ]\n [ 8.84429  ]\n [ 7.3050294]\n [ 9.503289 ]\n [10.368259 ]\n [10.713552 ]\n [ 8.725804 ]\n [ 6.6320577]\n [11.116314 ]\n [ 8.986814 ]\n [ 9.501082 ]\n [ 9.972298 ]\n [10.240842 ]\n [10.889851 ]\n [ 8.908095 ]\n [ 8.807196 ]\n [ 7.98901  ]\n [ 9.175022 ]\n [10.280718 ]\n [ 8.456374 ]\n [ 8.5034895]\n [ 9.8912115]\n [12.755909 ]\n [ 9.644646 ]\n [ 9.191887 ]\n [11.695898 ]\n [ 6.9703245]\n [ 5.5341845]\n [11.013508 ]\n [ 7.2610135]\n [ 8.576901 ]\n [ 7.9861536]\n [10.520036 ]\n [11.29882  ]\n [10.514744 ]\n [12.900667 ]\n [ 8.912637 ]\n [12.110659 ]\n [ 8.935581 ]\n [ 8.679852 ]\n [ 8.761967 ]\n [ 8.54363  ]\n [ 8.203865 ]\n [11.2323475]\n [11.806817 ]\n [ 8.572069 ]\n [10.372692 ]\n [12.256816 ]\n [ 8.567922 ]\n [ 7.698838 ]\n [ 9.447365 ]\n [ 9.083432 ]\n [ 8.839131 ]\n [ 9.408069 ]\n [ 7.834513 ]\n [ 8.205618 ]\n [10.992133 ]\n [ 9.669103 ]\n [ 9.417093 ]\n [10.227124 ]\n [ 4.8993196]\n [10.619326 ]\n [ 5.576148 ]\n [ 7.228013 ]\n [10.155312 ]\n [10.211051 ]\n [ 7.89009  ]\n [12.668659 ]\n [ 8.327033 ]\n [ 9.963749 ]\n [ 9.909342 ]\n [11.11674  ]\n [ 9.847928 ]\n [ 9.474469 ]\n [10.514744 ]\n [11.303598 ]\n [ 8.531114 ]\n [ 9.74981  ]\n [14.005271 ]\n [ 7.4205537]\n [10.465075 ]\n [ 9.297762 ]\n [10.607135 ]\n [ 8.220866 ]\n [10.106325 ]\n [11.020455 ]\n [ 7.7374253]\n [ 8.572949 ]\n [ 9.342945 ]\n [11.680966 ]\n [ 9.32688  ]\n [11.280945 ]\n [ 9.10347  ]\n [ 7.940628 ]\n [11.932697 ]\n [ 6.630588 ]\n [ 8.489243 ]\n [ 8.090548 ]\n [ 8.662579 ]\n [10.046943 ]\n [ 9.588243 ]\n [ 9.343155 ]\n [ 8.863066 ]\n [ 6.895473 ]\n [ 7.1769476]\n [10.341577 ]\n [10.841885 ]\n [ 7.307772 ]\n [10.396754 ]\n [ 9.606932 ]\n [ 9.066656 ]\n [11.29882  ]\n [10.682187 ]\n [ 7.689327 ]\n [ 7.2773385]\n [ 8.84902  ]\n [ 8.203592 ]\n [ 9.456106 ]\n [ 7.6193132]\n [ 8.200816 ]\n [ 8.808235 ]\n [11.69182  ]\n [10.941387 ]\n [ 8.853752 ]\n [11.990643 ]\n [ 9.0968685]\n [ 7.11327  ]\n [ 9.191702 ]\n [ 7.4824405]\n [ 7.8726773]\n [ 8.344769 ]\n [ 9.919114 ]\n [ 7.909647 ]\n [ 9.343298 ]\n [10.235217 ]\n [ 6.7188005]\n [ 6.9742184]\n [ 8.573759 ]\n [ 9.316333 ]\n [ 7.3327537]\n [ 9.675173 ]\n [13.339001 ]\n [ 7.5735593]\n [ 8.5445175]\n [ 9.647    ]\n [ 7.836283 ]\n [11.557287 ]\n [10.445864 ]\n [ 9.127172 ]\n [10.612322 ]\n [11.197116 ]\n [ 8.004183 ]\n [10.041069 ]\n [ 9.272094 ]\n [ 9.2185755]\n [ 6.844442 ]\n [12.261602 ]\n [ 9.263358 ]\n [11.482601 ]\n [ 7.0075355]\n [11.046843 ]\n [12.303396 ]\n [10.102385 ]\n [ 9.271562 ]\n [ 7.1230097]\n [ 7.7655287]\n [ 8.371605 ]\n [ 7.4434285]\n [ 6.000671 ]\n [ 7.2970734]]", "q_t_selected": "[ 7.391116   8.350364  10.361994  11.028475   7.4240437 10.658261\n 10.667141   7.8419986 10.995722  10.189787   7.6816244  9.913637\n  9.034861  13.318498   6.2364755  9.833154  12.715238   9.431137\n  7.9143147  9.100948   8.27346   10.023747   8.434995  12.815711\n 10.383416   8.425577  10.1428175 10.996573  10.713232   7.2854257\n  9.4266405  7.9022775  9.831214  11.280633   9.078873  10.31532\n  8.170631   9.3587055 10.2387495 10.663352  14.879642   9.869588\n  7.813685   7.5355916  8.073071   8.781163   8.090624   8.566694\n  9.099518   7.7508783 11.009711   8.425577   8.572949  10.836345\n  9.936294   8.389316   8.986641   6.7688437  8.909075   8.887826\n 10.413125   7.1596866 10.162676   6.782174  10.691516  10.162101\n  9.420098  10.658261   8.88477    9.251779  11.358846   9.034504\n 10.21644   10.796634   8.67676    9.00271   10.850784   7.636617\n  9.213304   8.545329   7.982029   8.824532  11.435931   8.686239\n  8.118059   8.153281   9.16584    8.84429    7.3050294  9.503289\n 10.368259  10.713552   8.725804   6.6320577 11.116314   8.986814\n  9.501082   9.972298  10.240842  10.889851   8.908095   8.807196\n  7.98901    9.175022  10.280718   8.456374   8.5034895  9.8912115\n 12.755909   9.644646   9.191887  11.695898   6.9703245  5.5341845\n 11.013508   7.2610135  8.576901   7.9861536 10.520036  11.29882\n 10.514744  12.900667   8.912637  12.110659   8.935581   8.679852\n  8.761967   8.54363    8.203865  11.2323475 11.806817   8.572069\n 10.372692  12.256816   8.567922   7.698838   9.447365   9.083432\n  8.839131   9.408069   7.834513   8.205618  10.992133   9.669103\n  9.417093  10.227124   4.8993196 10.619326   5.576148   7.228013\n 10.155312  10.211051   7.89009   12.668659   8.327033   9.963749\n  9.909342  11.11674    9.847928   9.474469  10.514744  11.303598\n  8.531114   9.74981   14.005271   7.4205537 10.465075   9.297762\n 10.607135   8.220866  10.106325  11.020455   7.7374253  8.572949\n  9.342945  11.680966   9.32688   11.280945   9.10347    7.940628\n 11.932697   6.630588   8.489243   8.090548   8.662579  10.046943\n  9.588243   9.343155   8.863066   6.895473   7.1769476 10.341577\n 10.841885   7.307772  10.396754   9.606932   9.066656  11.29882\n 10.682187   7.689327   7.2773385  8.84902    8.203592   9.456106\n  7.6193132  8.200816   8.808235  11.69182   10.941387   8.853752\n 11.990643   9.0968685  7.11327    9.191702   7.4824405  7.8726773\n  8.344769   9.919114   7.909647   9.343298  10.235217   6.7188005\n  6.9742184  8.573759   9.316333   7.3327537  9.675173  13.339001\n  7.5735593  8.5445175  9.647      7.836283  11.557287  10.445864\n  9.127172  10.612322  11.197116   8.004183  10.041069   9.272094\n  9.2185755  6.844442  12.261602   9.263358  11.482601   7.0075355\n 11.046843  12.303396  10.102385   9.271562   7.1230097  7.7655287\n  8.371605   7.4434285  6.000671   7.2970734]", "twin_q_t_selected": "[ 6.7098503  8.640686  10.2252445 10.495247   7.7984457 10.82724\n 11.180508   8.324671  11.073808  10.06903    8.020583   9.652977\n  8.643143  13.709931   7.008094  10.584894  13.151634   9.216227\n  7.6727896  9.378268   8.6328335 10.68979    8.696679  12.391086\n 10.564862   8.637493   9.429948  10.597494  10.268157   7.4651184\n  9.665595   8.208336   9.397704  11.044752   8.825445  10.10591\n  8.472907   8.845432   9.840417  10.398776  15.404087  10.344724\n  7.805148   7.6376524  7.938262   9.138916   8.18178    8.953614\n  8.607642   7.5656075 10.930175   8.637493   8.781859  10.624176\n  9.390244   7.416712   9.031763   7.6256447 10.095353   8.459124\n 10.309304   7.3220086 10.294069   6.3131905 11.438768  10.765666\n 10.303461  10.82724    8.889515   9.532626  11.076174   9.715059\n 10.158711  11.269151   8.605169   8.694609  10.440556   7.974277\n  9.691031   8.302615   7.679206   8.666867  11.806542   8.515763\n  8.315911   8.28796    9.7624035  8.448822   7.442607   9.3843775\n  9.763051  11.146908   8.318292   6.4370065 10.686747   9.428303\n 10.043041  10.868309  10.552294  10.797394   9.704738   8.216096\n  7.871985   9.133223  11.092647   8.273532   8.130213  10.155488\n 12.923022  10.033692   9.345235  10.685117   6.648658   5.20906\n 11.119393   7.7876453  9.399375   8.380197  10.715141  11.624757\n 10.747418  13.327339   8.484355  12.640849   9.249842   8.054905\n  8.496317   8.268537   8.031465  11.352143  12.414413   8.847025\n 10.253308  12.09644    9.362571   8.087836  10.294051   8.833101\n  8.427172   9.052743   8.7176695  8.301917  11.170678   8.906751\n  8.900912  10.589953   5.4700236 10.62767    5.3727546  7.698601\n  9.293686  10.820263   7.4524264 12.862573   7.856791  10.086652\n 10.355419  11.381252   9.889767   9.848542  10.747418  11.411995\n  8.728158   9.347256  13.623256   7.203967  11.123941  10.049822\n 10.849063   7.991371   9.690869  10.804456   8.239635   8.781859\n 10.018708  11.350812   9.42989   11.02456    8.56144    7.7897162\n 11.734901   6.478512   7.5678024  7.881927   8.569477   9.088077\n 10.457775   9.615597   9.514875   7.1312723  7.481395  10.488628\n 11.021535   7.0150185  9.913936   9.129649   8.583102  11.624757\n 10.641257   7.849501   7.738637   9.20616    9.687469   9.516831\n  7.986169   8.834398   8.84763   11.482018  10.95076    9.521251\n 10.695189   9.77555    6.776625   9.991278   6.6237903  8.038197\n  8.134656   9.775481   7.5166297 10.529407  10.101486   7.2315946\n  7.359409   8.331474   9.960096   7.201639   9.419519  12.518205\n  7.987637   8.350906   9.309292   8.317709  11.847063  10.6247635\n  8.765103  10.801408  11.683032   7.655823  10.049731   9.87274\n  8.97189    6.961509  12.270472   9.138081  10.843302   8.099289\n 10.920729  12.261379  10.032426   9.105745   8.036008   7.9204607\n  7.9024105  7.992153   6.230217   8.544977 ]", "q_t_selected_target": "[ 7.9486723  8.778293  11.713736  10.7072735  8.751404  10.996316\n 12.397503   8.335335  12.213578   9.282505   7.652196  10.281116\n  7.73711   12.882143   6.4606533  8.387946  12.644595   9.726219\n  7.8167872  9.609016   7.861533   9.315501   8.520903  11.773363\n 11.471372   8.295536  10.628858   9.851933  12.01841    7.8167744\n 10.001436   7.822102   8.929388  12.177241   9.562382   9.615712\n  8.642305   9.522759  10.27487   11.431852  16.715237   9.616751\n  8.40279    8.807316   8.860782   8.934654   9.146362   9.320268\n  9.0886965  8.272969  11.876107   8.774541   9.158268  11.208767\n 11.944662   8.014542   8.843926   6.9749713 10.723754   9.237397\n 11.761246   7.4116745 11.023225   6.325896  11.542676  11.50508\n  9.889454  10.661435   9.379234  10.287238  12.228791   8.849679\n  9.537443   9.234643   8.413889   8.554449   9.001482   7.8183804\n  8.80162    8.208162   8.580079   8.068372  11.6854925  8.49838\n  8.938181   9.202614   9.84107    8.497408   7.4545145  9.6691065\n 11.731461  11.015153   8.568317   6.6131682 10.725074  10.954322\n  8.707524  10.217253  10.898808   9.6063     8.980242   9.065236\n  8.4143305  9.861927   9.763643   7.8501463  7.9050674 12.303579\n 11.390439   8.892153  11.493964   9.649111   7.913804   6.8729267\n 11.754947   7.985274  10.156416   8.49008   10.24142   11.066846\n  9.835791  11.677054   9.712406  13.368973   9.678064   9.714978\n  9.049088   9.315099   7.5460453 12.451601  10.255051   8.458354\n  8.379092  11.5868025  8.144953   9.341655   9.227873   9.6702795\n  8.308562  10.272646   6.994966   8.707454  12.572651   9.85581\n 10.916656   9.914764   4.579997  10.327821   6.0838532  8.691909\n 10.268987  11.086236   7.672031  14.352556   9.1009     9.370241\n 11.501186  11.060691   9.941645   9.222855   9.475092  12.479792\n  8.782723  10.354158  11.882456   8.937121  10.958928   9.749381\n 10.481031   7.6364055  9.206245  11.902663   8.417175   9.225087\n 11.858299  10.125514   8.613201   9.464353   8.386679   9.348838\n 11.813951   6.368484   7.3933     8.301699  10.282833   8.249348\n 11.28858   10.830775   9.513978   7.4238176  8.484551  10.936449\n 10.579763   6.4728856 10.153224   7.941965   8.927674  11.147919\n  7.1813354  7.7058134  7.465891   8.46095    8.596723  10.47412\n  8.423894   7.8237944 10.473263  11.78968   11.1661825 10.020451\n 11.708039  10.892128   6.549412   7.783656   8.220486   8.750537\n  8.695596   9.537366   8.045134  10.740407  11.107387   8.110309\n  7.4412613  9.970149  10.850951   5.3902397  8.604186  12.5752125\n  9.424683   7.9688916  9.4963045  9.097901  11.4911585 10.181309\n 10.225928   9.251862  12.707528   8.1176605  9.379435   8.826561\n  9.375589   6.2829266 13.014779   8.797959  12.562099   6.469408\n 13.913099  13.8150625 10.500363   9.072741   8.889612   8.013664\n  9.713742   7.0258517  5.422955   7.390812 ]", "q_tp1_best_masked": "[ 9.29393    9.378565  12.206987  10.938265   9.709264  11.510285\n 11.409984   9.594768  12.806362   9.80615    9.235922  10.257808\n  8.907148  11.512096   8.686301   9.734495  11.488916   9.496319\n  9.356218   9.8910885  8.666437   9.147446   9.937933  11.067417\n 12.085648   9.3072    10.895993   9.406771  11.5426655  9.3918085\n 10.340918   8.195175   8.526481  12.233898  10.793563   9.403225\n  9.731909  10.374155  10.811746  10.80748   14.816636   9.077235\n  9.004663  10.076725  10.647064  10.295404  10.571315  10.650792\n 10.431824  10.25838   11.541571   9.791043  10.39179   11.811551\n 11.814089   9.407299   9.385407   8.349767  10.572275  10.825791\n 10.9347925  9.641078  10.714908   8.210342  11.806741  11.089512\n  9.694372  11.172022  10.185461  10.690294  12.12224    8.763908\n  9.640968  11.085034   8.629516   9.066785   9.35296    9.167128\n  9.196218   8.876413   9.5922575  8.493224  12.175836   8.657356\n  9.472424   9.660564   9.780186   8.45635    8.29856    9.947719\n 11.734474  10.637731  10.420812   7.5359964 10.527685  11.422108\n  9.02921   10.6682825 11.095039   9.262164   9.423981   9.78167\n  9.758629   9.912178   9.857375   8.721365   8.605799  11.971359\n 11.101583   8.830194  11.4910755  9.845549   9.809308   8.844775\n 11.55369   10.013575  10.119432   9.010735   9.713164  11.128468\n 10.376411  11.847534  11.115164  12.376213   9.596042  11.083788\n  9.942597  10.353807   7.9000416 11.317352   9.653303   9.353369\n  9.2388115 11.104185   8.752354   9.827117   9.358454  10.483421\n  8.599225  11.012524   7.692713  10.243949  12.162517  11.455049\n 11.256011   9.951341   7.072032   9.8480835  9.366089   9.485337\n 10.291279  11.211511   8.632984  13.905581   9.741953   9.777749\n 11.324947  10.768757  10.841803  10.1514015 10.01207   12.419735\n  9.474983  10.656851  10.7283945 10.428786  10.668459   9.90934\n 11.790775   7.43147   10.5687275 11.705707   8.268538  10.459284\n 11.379669   9.638754   9.450024  10.033522   9.209647  10.378477\n 11.443923   8.31646    8.638652   9.449912  11.586214   8.416512\n 11.493013  11.482861  10.305709   8.792089   9.464731  11.03896\n 10.638243   7.3666267 10.909758   9.017443   9.840024  11.21036\n  7.3592334  9.805452   9.422962   9.602631   8.602209  10.716257\n  9.8666525  8.079383  10.509623  11.312139  11.854692  10.744121\n 11.665302  11.895231   8.017393   8.37538    9.577429   9.781065\n  9.375851   9.7151985 10.261825  10.916061  10.654322  10.385223\n  9.325506  10.666911  10.636646   6.958338   8.641932  11.416778\n  9.942672   8.952709  10.736375   9.350695  10.967937  10.302855\n 10.750653   9.987632  11.898019   7.800878  10.466528   8.751203\n  9.888031   7.473116  12.722194   9.396306  11.470519   7.2830663\n 13.536694  13.225745  10.725645   9.336943  10.002956   9.132998\n 10.275881   8.338698   7.485298   9.212291 ]", "policy_t": "[[-0.29760194  0.12901211 -0.94957334  0.75747335  0.5823419  -0.9024898 ]\n [-0.41772813  0.8053529   0.5177957  -0.07120866  0.5563159  -0.68031734]\n [ 0.43215108  0.5376724   0.3849206  -0.3148654  -0.692807    0.9605601 ]\n ...\n [-0.3790688  -0.8587339   0.44642067  0.9748914   0.01377583 -0.62109387]\n [ 0.08136809  0.6428406   0.9527737   0.89622486 -0.04665589 -0.841023  ]\n [ 0.5799266   0.18952441  0.86250424 -0.20079559 -0.33192825 -0.15524101]]", "td_error": "[0.89818907 0.28276777 1.4201164  0.26661396 1.1401591  0.2535653\n 1.4736786  0.2520001  1.1788135  0.8469033  0.19890785 0.49780846\n 1.1018915  0.6320715  0.38580918 1.8210778  0.28884077 0.40253735\n 0.12076259 0.36940813 0.59161377 1.0412674  0.13084221 0.8300352\n 0.99723244 0.2359991  0.84247494 0.94510126 1.5277152  0.44150233\n 0.45531845 0.2332046  0.685071   1.0145488  0.6102228  0.594903\n 0.32053614 0.42069054 0.23528671 0.9007883  1.5733724  0.49040508\n 0.59337354 1.2206938  0.8551154  0.1788764  1.0101604  0.56011343\n 0.24593782 0.6147263  0.90616417 0.24300575 0.48086357 0.47850657\n 2.2813935  0.4863019  0.16527557 0.42840052 1.22154    0.5639224\n 1.4000311  0.17082691 0.79485226 0.23449183 0.47753382 1.0411968\n 0.44168139 0.08448935 0.49209166 0.89503574 1.0112815  0.5251026\n 0.65013266 1.7982492  0.22707558 0.29421043 1.6441879  0.16882992\n 0.650548   0.21580982 0.74946165 0.67732763 0.1853056  0.10262156\n 0.7211957  0.9819932  0.37694836 0.19773388 0.08069634 0.22527313\n 1.6658053  0.21667767 0.20375633 0.0975256  0.21478367 1.7467637\n 1.0645375  0.44800568 0.5022397  1.2373219  0.39832115 0.5535903\n 0.48383307 0.7078047  0.92303896 0.51480675 0.4117837  2.2802296\n 1.4490266  0.94701624 2.2254033  1.5413966  1.1043129  1.5013044\n 0.6884961  0.4609444  1.1682777  0.3069048  0.37616873 0.39494228\n 0.79529047 1.4369488  1.0139103  0.9932189  0.5853529  1.3476\n 0.41994572 0.90901566 0.5716195  1.1593556  1.8555646  0.25119305\n 1.933908   0.58982563 0.8202934  1.4483175  0.64283514 0.71201277\n 0.32458925 1.0422401  1.2811253  0.45368624 1.4912453  0.5678835\n 1.7576528  0.4937744  0.6046746  0.29567719 0.60940194 1.2286019\n 0.54448795 0.57057905 0.21883178 1.5869403  1.0089877  0.6549592\n 1.3688059  0.18830538 0.0727973  0.43865108 1.1559892  1.121995\n 0.15308762 0.80562544 1.9318075  1.624861   0.32943344 0.37602997\n 0.24706745 0.4697132  0.6923518  0.9902077  0.4286449  0.54768276\n 2.1774726  1.3903751  0.7651839  1.6883998  0.44577646 1.4836657\n 0.09889793 0.18606591 0.63522243 0.3154614  1.6668053  1.318162\n 1.2655711  1.3513994  0.32590485 0.41044497 1.1553802  0.52134657\n 0.35194635 0.6885097  0.2414093  1.4263253  0.24177694 0.31386948\n 3.4803867  0.08008718 0.23064923 0.5666399  0.7419381  0.98765135\n 0.6211529  0.69381285 1.6453304  0.20276022 0.22010899 0.83294916\n 0.64772654 1.4559188  0.3955357  1.8078337  1.1673703  0.7951\n 0.4558835  0.30993176 0.33199525 0.80405474 0.93903494 1.1351111\n 0.27444768 1.5175323  1.2127366  1.8769567  0.94316006 0.410398\n 1.6440845  0.47882032 0.16885424 1.0209053  0.21101665 0.35400486\n 1.2797904  1.4550033  1.2674541  0.2876575  0.66596556 0.7458558\n 0.2803564  0.620049   0.7487421  0.40276003 1.399148   1.0840042\n 2.9293137  1.5326748  0.43295813 0.11591291 1.3101034  0.17066956\n 1.5767345  0.6919391  0.6924889  0.6239519 ]", "mean_td_error": 0.7868791222572327, "actor_loss": -11.46528434753418, "critic_loss": 0.48458361625671387, "alpha_loss": -6.038088798522949, "alpha_value": 0.5460283160209656, "target_entropy": -6, "mean_q": 9.331377029418945, "max_q": 14.87964153289795, "min_q": 4.899319648742676, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 12000, "episodes_total": 12, "training_iteration": 12, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_14-44-25", "timestamp": 1587048265, "time_this_iter_s": 86.10313606262207, "time_total_s": 200.85615372657776, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 200.85615372657776, "timesteps_since_restore": 12000, "iterations_since_restore": 12, "perf": {"cpu_util_percent": 92.19583333333334, "ram_util_percent": 11.198611111111113}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -177.03656413388163, "episode_reward_min": -442.6766196581752, "episode_reward_mean": -303.7877034047733, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-280.76831446794455, -257.49418035497234, -442.6766196581752, -306.58538870598477, -305.0018729768031, -268.4399207217176, -376.195792008098, -315.43501147963894, -308.24336954051677, -177.03656413388163], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26339271515782914, "mean_processing_ms": 0.1203247096570114, "mean_inference_ms": 1.174156354330592}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -200.0574765235254, "episode_reward_min": -418.600553095456, "episode_reward_mean": -331.4164610609616, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-200.0574765235254, -411.0826562385054, -418.600553095456, -413.76740122151716, -213.5742182258041], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.28144449392371834, "mean_processing_ms": 0.39409132321596185, "mean_inference_ms": 1.2859146477953711}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 6500, "num_steps_trained": 766464, "num_steps_sampled": 13000, "sample_time_ms": 3.191, "replay_time_ms": 23.596, "grad_time_ms": 54.55, "update_time_ms": 0.005, "opt_peak_throughput": 4692.985, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[10.814215 ]\n [ 6.535217 ]\n [11.141041 ]\n [11.382068 ]\n [12.746748 ]\n [12.58962  ]\n [14.926409 ]\n [15.867191 ]\n [13.170179 ]\n [14.035769 ]\n [12.648004 ]\n [11.509284 ]\n [12.07111  ]\n [ 8.6939125]\n [15.592149 ]\n [13.260578 ]\n [13.363025 ]\n [19.016396 ]\n [ 8.703893 ]\n [11.652633 ]\n [12.437612 ]\n [11.932373 ]\n [11.038513 ]\n [13.179396 ]\n [ 9.873129 ]\n [10.782242 ]\n [14.407958 ]\n [12.378751 ]\n [12.004252 ]\n [11.103593 ]\n [11.477132 ]\n [ 4.912813 ]\n [ 8.622696 ]\n [13.545727 ]\n [12.8987875]\n [12.396532 ]\n [12.289641 ]\n [12.294449 ]\n [11.807656 ]\n [ 9.831177 ]\n [13.755918 ]\n [12.4446335]\n [10.479652 ]\n [13.602954 ]\n [ 8.931948 ]\n [11.226399 ]\n [12.501461 ]\n [ 8.20534  ]\n [12.382588 ]\n [11.860777 ]\n [11.67938  ]\n [10.684626 ]\n [12.6264925]\n [10.370823 ]\n [12.308529 ]\n [13.120708 ]\n [10.06591  ]\n [12.173442 ]\n [13.825807 ]\n [12.745667 ]\n [12.179628 ]\n [14.48077  ]\n [11.002894 ]\n [14.607842 ]\n [12.80673  ]\n [12.356386 ]\n [14.109855 ]\n [13.546914 ]\n [12.755616 ]\n [13.749578 ]\n [14.037099 ]\n [10.866474 ]\n [14.5306   ]\n [10.384895 ]\n [13.2247715]\n [12.808763 ]\n [11.279412 ]\n [11.5744095]\n [12.227921 ]\n [13.656244 ]\n [16.0566   ]\n [13.946007 ]\n [ 9.785911 ]\n [14.419282 ]\n [14.822957 ]\n [13.069417 ]\n [ 8.070546 ]\n [10.638876 ]\n [13.480767 ]\n [14.045923 ]\n [14.789687 ]\n [ 9.865383 ]\n [13.062727 ]\n [ 9.387647 ]\n [10.476971 ]\n [11.664447 ]\n [ 9.494658 ]\n [12.112943 ]\n [10.651205 ]\n [16.53012  ]\n [10.803611 ]\n [ 7.930511 ]\n [15.558251 ]\n [11.886654 ]\n [10.930445 ]\n [10.508482 ]\n [14.887867 ]\n [11.503303 ]\n [14.101588 ]\n [11.637075 ]\n [11.551654 ]\n [14.975432 ]\n [13.445668 ]\n [ 9.434561 ]\n [12.068509 ]\n [10.917686 ]\n [12.626224 ]\n [12.186648 ]\n [ 9.181699 ]\n [11.21986  ]\n [10.677399 ]\n [13.295627 ]\n [10.0322   ]\n [12.896292 ]\n [13.246672 ]\n [ 9.797487 ]\n [15.447838 ]\n [12.171287 ]\n [11.946301 ]\n [11.912906 ]\n [ 8.931948 ]\n [10.510358 ]\n [10.913508 ]\n [ 9.437859 ]\n [ 9.641658 ]\n [11.399419 ]\n [15.239546 ]\n [13.021716 ]\n [12.626283 ]\n [10.6382475]\n [12.61837  ]\n [12.423524 ]\n [17.074068 ]\n [11.239192 ]\n [14.678371 ]\n [13.233759 ]\n [ 8.6329975]\n [11.236395 ]\n [12.608452 ]\n [12.019446 ]\n [ 9.709965 ]\n [11.897944 ]\n [12.405173 ]\n [11.782755 ]\n [11.799434 ]\n [ 9.657997 ]\n [11.252768 ]\n [ 7.1054792]\n [ 9.1574745]\n [13.093373 ]\n [13.446538 ]\n [12.875417 ]\n [11.702651 ]\n [11.707601 ]\n [ 5.5885477]\n [15.379147 ]\n [10.062213 ]\n [11.905308 ]\n [14.032556 ]\n [12.780949 ]\n [16.902346 ]\n [10.8563795]\n [13.021687 ]\n [ 9.552749 ]\n [15.090171 ]\n [11.7086735]\n [13.020609 ]\n [11.411607 ]\n [10.0322   ]\n [ 9.868811 ]\n [16.307016 ]\n [11.628311 ]\n [12.775509 ]\n [11.632464 ]\n [12.395367 ]\n [12.100772 ]\n [11.14979  ]\n [11.550279 ]\n [13.514765 ]\n [10.346223 ]\n [11.763947 ]\n [10.24481  ]\n [11.737077 ]\n [13.174547 ]\n [12.800302 ]\n [15.474526 ]\n [15.354527 ]\n [12.054235 ]\n [14.500828 ]\n [16.171247 ]\n [11.287308 ]\n [13.561234 ]\n [ 9.795906 ]\n [10.486289 ]\n [11.799164 ]\n [13.045272 ]\n [11.056187 ]\n [11.619984 ]\n [10.414806 ]\n [ 9.499826 ]\n [11.4626255]\n [ 9.332879 ]\n [12.885849 ]\n [12.275408 ]\n [14.256553 ]\n [10.750438 ]\n [11.0441065]\n [11.56053  ]\n [12.210956 ]\n [11.541564 ]\n [12.022473 ]\n [12.171287 ]\n [13.6398   ]\n [13.879582 ]\n [11.666812 ]\n [10.257765 ]\n [12.053362 ]\n [12.670487 ]\n [12.308647 ]\n [10.034232 ]\n [15.367184 ]\n [12.1353445]\n [18.545395 ]\n [11.15802  ]\n [13.036707 ]\n [11.192266 ]\n [12.281884 ]\n [11.675881 ]\n [10.805202 ]\n [13.130787 ]\n [10.684209 ]\n [12.776336 ]\n [11.770589 ]\n [12.951051 ]\n [10.355302 ]\n [11.537788 ]\n [13.929152 ]\n [11.048988 ]\n [13.726224 ]\n [14.928778 ]\n [14.588982 ]\n [12.7499895]\n [12.402722 ]\n [13.11693  ]\n [13.421636 ]\n [12.671564 ]]", "q_t_selected": "[10.814215   6.535217  11.141041  11.382068  12.746748  12.58962\n 14.926409  15.867191  13.170179  14.035769  12.648004  11.509284\n 12.07111    8.6939125 15.592149  13.260578  13.363025  19.016396\n  8.703893  11.652633  12.437612  11.932373  11.038513  13.179396\n  9.873129  10.782242  14.407958  12.378751  12.004252  11.103593\n 11.477132   4.912813   8.622696  13.545727  12.8987875 12.396532\n 12.289641  12.294449  11.807656   9.831177  13.755918  12.4446335\n 10.479652  13.602954   8.931948  11.226399  12.501461   8.20534\n 12.382588  11.860777  11.67938   10.684626  12.6264925 10.370823\n 12.308529  13.120708  10.06591   12.173442  13.825807  12.745667\n 12.179628  14.48077   11.002894  14.607842  12.80673   12.356386\n 14.109855  13.546914  12.755616  13.749578  14.037099  10.866474\n 14.5306    10.384895  13.2247715 12.808763  11.279412  11.5744095\n 12.227921  13.656244  16.0566    13.946007   9.785911  14.419282\n 14.822957  13.069417   8.070546  10.638876  13.480767  14.045923\n 14.789687   9.865383  13.062727   9.387647  10.476971  11.664447\n  9.494658  12.112943  10.651205  16.53012   10.803611   7.930511\n 15.558251  11.886654  10.930445  10.508482  14.887867  11.503303\n 14.101588  11.637075  11.551654  14.975432  13.445668   9.434561\n 12.068509  10.917686  12.626224  12.186648   9.181699  11.21986\n 10.677399  13.295627  10.0322    12.896292  13.246672   9.797487\n 15.447838  12.171287  11.946301  11.912906   8.931948  10.510358\n 10.913508   9.437859   9.641658  11.399419  15.239546  13.021716\n 12.626283  10.6382475 12.61837   12.423524  17.074068  11.239192\n 14.678371  13.233759   8.6329975 11.236395  12.608452  12.019446\n  9.709965  11.897944  12.405173  11.782755  11.799434   9.657997\n 11.252768   7.1054792  9.1574745 13.093373  13.446538  12.875417\n 11.702651  11.707601   5.5885477 15.379147  10.062213  11.905308\n 14.032556  12.780949  16.902346  10.8563795 13.021687   9.552749\n 15.090171  11.7086735 13.020609  11.411607  10.0322     9.868811\n 16.307016  11.628311  12.775509  11.632464  12.395367  12.100772\n 11.14979   11.550279  13.514765  10.346223  11.763947  10.24481\n 11.737077  13.174547  12.800302  15.474526  15.354527  12.054235\n 14.500828  16.171247  11.287308  13.561234   9.795906  10.486289\n 11.799164  13.045272  11.056187  11.619984  10.414806   9.499826\n 11.4626255  9.332879  12.885849  12.275408  14.256553  10.750438\n 11.0441065 11.56053   12.210956  11.541564  12.022473  12.171287\n 13.6398    13.879582  11.666812  10.257765  12.053362  12.670487\n 12.308647  10.034232  15.367184  12.1353445 18.545395  11.15802\n 13.036707  11.192266  12.281884  11.675881  10.805202  13.130787\n 10.684209  12.776336  11.770589  12.951051  10.355302  11.537788\n 13.929152  11.048988  13.726224  14.928778  14.588982  12.7499895\n 12.402722  13.11693   13.421636  12.671564 ]", "twin_q_t_selected": "[10.513672   6.5685897 11.165075  10.84412   12.596889  12.749728\n 14.6959    15.137952  12.213711  14.091142  12.426798  11.6756735\n 12.55652    8.533985  15.523256  13.147231  13.4401455 19.124813\n  8.288888  11.414366  11.820656  12.1677265 10.330318  12.594631\n 10.559886  10.947399  15.086978  12.313313  11.301754  11.401193\n 11.4240885  5.56669    8.370233  13.230026  13.492426  13.17368\n 11.718714  12.424132  11.744546  10.64119   13.301532  11.717139\n 10.454083  13.480161   9.088273  10.503485  11.860002   7.656859\n 13.161804  11.786215  11.52811   11.333947  12.449431  10.345148\n 12.68878   12.102098  10.034352  11.719676  13.313629  11.745333\n 11.519024  14.611244  11.392273  15.0404625 13.719431  12.211802\n 13.673042  13.239179  12.043195  13.296511  13.774374  10.508196\n 14.114201  10.288209  12.9274    12.3610935 11.116119  11.07665\n 12.226321  13.534936  16.255766  13.266415   9.220817  13.717048\n 14.97683   13.481382   7.844704  10.257001  13.871464  14.223324\n 14.573576   9.914115  11.94047    9.7984915 11.100854  11.62791\n  9.844686  12.429546  10.090314  16.591482   9.7705345  7.9445\n 15.684045  11.995633  11.709463  10.436138  14.741445  12.019069\n 14.13871   11.33986   11.833022  14.027412  12.43145    9.844328\n 12.302398  10.14359   12.667584  11.573424   8.639031  11.363813\n 10.618476  12.253409  10.167803  12.370924  14.154155   8.8322735\n 14.876483  11.474107  11.639702  11.744534   9.088273  10.859232\n 10.770607   9.545108   9.427199  11.230186  15.5659275 12.556882\n 12.629963  10.996892  12.823722  13.044876  16.314505  11.604699\n 13.234072  12.872281   9.365945  11.729263  12.356608  11.049247\n  9.919525  11.536281  12.146159  10.288032  12.623019  10.097544\n 11.0649395  6.466089   9.916372  12.093429  12.357705  12.395525\n 11.692981  11.325251   5.731889  15.726474   9.729275  10.853539\n 13.541544  12.770549  17.03069   10.766032  12.784667   9.03358\n 15.196167  10.900286  12.556118  11.420664  10.167803  10.110659\n 15.763977  10.860717  12.472493  11.009792  12.236994  11.771736\n 11.26578   12.226546  13.673561  10.831878  11.915031  10.687333\n 12.086219  12.909175  12.577469  14.334299  14.392212  11.340132\n 13.893044  15.753192  11.237239  13.090537   9.760846   9.599758\n 12.406868  13.035631  10.461598  10.496121   9.785884   8.944287\n 12.027254   9.275413  12.848441  12.505884  13.919778  10.093352\n 11.129297  11.417479  11.566496  11.045746  12.39894   11.474107\n 13.87857   13.260598  11.492646  10.8469    12.080139  11.760295\n 12.370405  10.962153  14.449247  12.25612   17.765707  10.974411\n 12.286731  10.714679  11.737759  11.05308   10.831983  12.783901\n 10.588341  12.783456  12.2579    12.879336   9.854265  11.544228\n 14.430961  10.530803  13.549506  13.631178  13.814853  12.550124\n 12.041549  13.050104  12.826423  12.650633 ]", "q_t_selected_target": "[11.491678   6.1517057 11.416869  12.469021  13.2672    12.888699\n 16.443354  16.105947  12.574103  13.908286  12.118971  11.463972\n 12.792121   8.799047  16.58652   12.831799  14.176838  19.998014\n  7.2782874 11.448333  11.951897  12.778433  11.754239  14.61624\n 10.213695  10.721398  14.279731  12.348199  10.433511  12.273297\n 10.582054   5.665671   7.351387  11.688468  13.417336  10.731267\n 11.345992  12.477311  10.731536  10.75178   11.611708  10.999134\n  9.832739  14.120107   9.086369  11.808324  12.901531   8.597666\n 11.50217   12.952121  12.336914   9.526815  12.4096365 11.272184\n 12.631394  11.498274  11.014559  10.687108  13.576091  11.5798435\n 10.708232  13.220656  12.196035  15.893922  13.192405  11.29432\n 13.75385   14.180785  11.041726  14.01069   14.066642  10.495932\n 15.106285  10.290811  12.785905  12.241227  12.088948  12.778489\n 12.697705  13.784866  16.841892  13.546694   8.707321  14.744569\n 13.642173  13.768632   7.2299247  9.3351    12.486045  14.582155\n 18.31906    9.842504  11.637384   9.983934  11.936871  13.306994\n 10.1975    12.326796   9.6751175 15.396506   9.051812   8.685287\n 14.521052  11.9838505 11.684779  10.013706  15.019077  12.163824\n 13.554188  11.91352   12.0048895 13.454939  12.193421  10.074025\n 12.018029  10.128216  12.889035  11.132836   8.993015  10.760041\n 11.346042  12.822934   9.823844  11.266909  13.725329   9.36655\n 16.060658  13.652372  11.492896  11.725719   8.1480875 11.526076\n 11.372898   9.597519   8.748287  12.541248  15.067195  13.276053\n 14.184804  11.930671  13.581115  12.76649   15.6669445 11.575105\n 13.964697  13.530308   9.05516   11.7574215 13.236035  11.79867\n 10.842646  13.156315  11.796448  11.356744  12.439346   8.925721\n 11.659678   8.4696     9.143477  12.917829  11.449387  11.671352\n 13.286521  11.623698   4.7988844 14.974749   8.998208  10.83293\n 12.79929   12.931793  18.21585   12.906792  13.695967  10.391998\n 13.6208315 10.4783325 13.882881  10.683114   9.671566  10.729413\n 14.007499  13.119762  12.403025  12.27334   13.290037  11.455201\n 10.738368  11.381689  12.937022  10.69705   12.486186  11.477856\n 13.277931  14.762573  14.086838  15.73674   16.323326  12.98149\n 14.54428   18.057625  12.006175  13.920237   9.656635   9.999376\n 11.848691  11.673371  10.643162  10.266729   9.267623  10.892778\n 10.97902   10.791907  12.042138  11.051832  15.318663  10.003736\n 12.881907  12.665929  11.36649   12.439458  13.211348  12.741086\n 15.507512  12.664437  11.097007  12.822196  11.371395  11.567606\n 11.203741  12.448601  16.090483  12.081264  17.519768  11.633258\n 13.090057  10.566139  12.060602  10.064865  10.381382  14.527963\n 11.533491  14.262543  10.519383  11.268196   9.64641   11.050646\n 15.407557   8.819597  15.634747  14.757128  14.784535  12.845529\n 13.534052  12.033319  13.618135  11.437122 ]", "q_tp1_best_masked": "[11.50878    8.39496   12.047683  12.681477  14.222202  12.648293\n 16.331211  16.002647  12.926144  13.73644   12.233465  13.028312\n 13.460491   9.733123  14.982548  12.675185  14.270243  19.097364\n  8.975213  12.0811615 11.769251  12.263196  12.485706  14.597182\n 10.832572  11.512413  14.635135  13.244049  10.97327   12.699126\n 11.421127   7.6247888  8.221453  11.606201  13.241927  10.321318\n 11.921206  13.558948  10.988486  11.242596  12.202567  12.499605\n 10.759759  14.037355  10.753568  12.661603  13.178664   9.7393465\n 12.25374   13.505707  13.649452  11.297305  12.812644  11.178751\n 12.426769  11.896492  12.657869  11.065001  12.899983  12.5617895\n 11.914361  13.332207  12.574747  16.30558   12.929417  11.602245\n 14.067646  13.744179  11.231887  13.93514   14.658009  10.936583\n 15.183278  12.269071  12.539701  12.757272  13.110985  12.668499\n 12.662155  14.955555  17.425917  13.806366   9.67532   14.230473\n 13.66693   14.512951   8.391032  10.076015  12.502364  14.619879\n 17.445797  10.343632  12.290656  12.24681   13.555931  12.660244\n 11.23509   12.043909  11.521081  14.738285   9.615919  11.36233\n 14.192917  12.670678  12.626682  10.446508  14.056782  12.389825\n 13.134794  13.424522  12.24113   12.598852  12.164133  10.050088\n 12.745234  10.796167  13.414652  12.717425  10.168975  10.9911375\n 12.657356  12.885263  12.183383  11.042414  14.044916  10.861061\n 17.03136   13.476792  12.185735  12.281864   9.805809  11.806817\n 11.997743  11.543452   9.903542  12.908812  13.990695  13.899414\n 13.034435  12.498312  13.304362  12.519962  15.086952  11.642767\n 13.998795  13.88654   10.14886   12.45932   13.204592  12.738888\n 11.798583  13.10712   11.845465  11.529524  13.03782   10.112778\n 12.508663  10.457577  11.075818  13.278727  11.292564  12.043103\n 13.771949  12.644075   7.2418985 14.411474  10.039833  11.813379\n 12.818269  13.543954  17.934412  13.167295  13.547722  12.345047\n 12.776129  10.456433  13.398816  11.199498  12.029567  12.81756\n 14.973323  13.441601  12.2485695 13.107698  13.777375  11.456992\n 11.398926  11.4404955 13.68953   10.789624  13.032438  11.981527\n 13.759433  14.433115  14.26513   14.816767  16.466616  13.152107\n 13.734576  18.230507  12.189099  13.896991   9.7278805 10.432474\n 12.880177  12.431448  11.709322  12.4405365  9.480516  12.234486\n 11.115882  12.398542  12.645643  10.975125  15.483715  10.95715\n 12.066501  12.898937  11.798582  12.692566  13.410672  12.556301\n 16.039085  12.660868  11.5549755 12.867674  12.103844  11.3135395\n 11.926619  12.632094  15.360089  12.767146  16.832777  12.6757345\n 12.532151  11.724748  12.045053   9.463903  11.168783  15.075567\n 13.052968  14.020985  11.625631  10.992863  11.704258  13.373533\n 13.350467   9.468132  15.366298  14.714862  13.877486  12.944377\n 13.659598  12.696737  13.980536  11.990533 ]", "policy_t": "[[-0.5399039  -0.93128115  0.82028794 -0.21196133  0.7635958  -0.49938834]\n [ 0.47149408  0.6425612  -0.8440544  -0.6086328  -0.86714923  0.41610384]\n [ 0.86553144 -0.08518702  0.49124503  0.06124318 -0.4545765  -0.10357255]\n ...\n [ 0.7040677  -0.31463546  0.72742975 -0.4511248   0.36524785 -0.7994814 ]\n [ 0.40514517  0.715901   -0.8767134  -0.17349648 -0.19136298 -0.81629926]\n [ 0.3796394  -0.29985857  0.2955122   0.8152809  -0.4653303  -0.73998344]]", "td_error": "[0.82773495 0.4001975  0.2638111  1.355927   0.5953822  0.21902466\n 1.6321993  0.6033759  0.4782343  0.15516949 0.41842985 0.12850666\n 0.47830582 0.18509865 1.0288167  0.37210608 0.7752528  0.9274101\n 1.2181029  0.11913347 0.30847788 0.72838306 1.0698233  1.7292261\n 0.34337854 0.14342213 0.4677372  0.03271914 1.2194924  1.0209045\n 0.868556   0.4259193  1.1450772  1.6994085  0.2968192  2.0538392\n 0.6581855  0.11802053 1.0445652  0.5155964  1.917017   1.0817523\n 0.63412905 0.5785494  0.07816267 0.9433818  0.7207999  0.66656613\n 1.2700267  1.1286249  0.7331691  1.482471   0.12832546 0.9141989\n 0.19012547 1.1131296  0.9644275  1.2594509  0.25608873 0.66565657\n 1.1410942  1.3253508  0.9984517  1.0697694  0.45635033 0.98977375\n 0.2184062  0.7877388  1.3576794  0.48764515 0.16090536 0.19140339\n 0.783885   0.04834318 0.29018068 0.3437009  0.8911824  1.4529595\n 0.4705844  0.18927622 0.685709   0.33979607 0.79604244 0.676404\n 1.2577205  0.49323225 0.7277005  1.1128383  1.1900706  0.4475317\n 3.6374278  0.0472455  0.86421394 0.39086533 1.1479583  1.6608162\n 0.5278282  0.15830183 0.695642   1.1642952  1.2352605  0.747782\n 1.1000957  0.05448961 0.3895092  0.45860386 0.20442152 0.40263844\n 0.56596136 0.42505217 0.3125515  1.0464835  0.7451377  0.4345808\n 0.1674242  0.40242243 0.24213123 0.7472     0.2713337  0.5317955\n 0.6981044  0.5211086  0.27615738 1.3666992  0.45374155 0.4826069\n 0.8984971  1.8296757  0.30010557 0.10300016 0.8620229  0.8412814\n 0.5308404  0.10603571 0.7861414  1.2264457  0.33554173 0.48675442\n 1.5566812  1.113101   0.8600688  0.3106761  1.0273418  0.18275356\n 0.72214985 0.47728777 0.36647367 0.2745924  0.75350523 0.4850998\n 1.0279007  1.4392023  0.47921848 0.74736166 0.41179276 0.95204926\n 0.5008249  1.6838157  0.39344597 0.49997234 1.452735   0.9641185\n 1.5887051  0.19117498 0.86133385 0.5780616  0.8975358  0.546494\n 0.98776007 0.15604448 1.2493334  2.0955858  0.79278994 1.098834\n 1.5223374  0.8261471  1.0945177  0.73302126 0.42843533 0.7396784\n 2.027998   1.8752484  0.22097635 0.95221186 0.9738569  0.48105288\n 0.4694171  0.5067234  0.65714073 0.24282742 0.64669704 1.0117841\n 1.3662834  1.7207122  1.3979526  0.83232737 1.4499564  1.2843065\n 0.34734392 2.095405   0.7439017  0.5943508  0.12174082 0.44326544\n 0.30385208 1.3670802  0.29729414 0.7913232  0.8327222  1.6707215\n 0.7659197  1.4877615  0.82500696 1.3388138  1.2304974  0.41815948\n 1.7952056  1.1769247  0.5222354  1.145803   1.0006409  0.9183893\n 1.7483273  0.905653   0.48272228 2.2698636  0.6953554  0.6477852\n 1.1357851  1.950408   1.1822672  0.11446857 0.6357832  0.56704235\n 0.42833853 0.3873334  0.27206278 1.2996154  0.43721008 1.5706186\n 0.8972163  1.482647   1.4948611  1.6469975  0.45837355 0.49036217\n 1.2275004  1.9702983  1.9968815  0.6487999  0.58261824 0.19547176\n 1.3119164  1.0501976  0.4941063  1.2239761 ]", "mean_td_error": 0.8140173554420471, "actor_loss": -13.552379608154297, "critic_loss": 0.49633586406707764, "alpha_loss": -9.047807693481445, "alpha_value": 0.4044562578201294, "target_entropy": -6, "mean_q": 12.115062713623047, "max_q": 19.016395568847656, "min_q": 4.912813186645508, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 13000, "episodes_total": 13, "training_iteration": 13, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_14-46-08", "timestamp": 1587048368, "time_this_iter_s": 87.35145092010498, "time_total_s": 288.20760464668274, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 288.20760464668274, "timesteps_since_restore": 13000, "iterations_since_restore": 13, "perf": {"cpu_util_percent": 92.29109589041096, "ram_util_percent": 11.200000000000001}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -116.76322447567159, "episode_reward_min": -246.63503494583503, "episode_reward_mean": -175.87387310439902, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-216.9097586451948, -246.63503494583503, -176.5554271338365, -169.2072895976553, -116.76322447567159, -157.19810923161532, -216.03285834915835, -125.1136485538642, -183.25942523788922, -151.06395487326978], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2633912843133535, "mean_processing_ms": 0.12024727353018916, "mean_inference_ms": 1.1743838975556464}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -194.36138843836824, "episode_reward_min": -418.600553095456, "episode_reward_mean": -288.07220750093416, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-194.36138843836824, -418.600553095456, -413.76740122151716, -213.5742182258041, -200.0574765235254], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.28839507300023143, "mean_processing_ms": 0.4019574901308217, "mean_inference_ms": 1.336801153591497}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 7000, "num_steps_trained": 1022464, "num_steps_sampled": 14000, "sample_time_ms": 2.292, "replay_time_ms": 14.552, "grad_time_ms": 37.018, "update_time_ms": 0.005, "opt_peak_throughput": 6915.646, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[15.1657715]\n [10.222781 ]\n [13.828489 ]\n [17.557508 ]\n [13.326859 ]\n [10.222204 ]\n [13.435275 ]\n [14.869973 ]\n [13.614086 ]\n [11.766128 ]\n [13.831872 ]\n [14.910767 ]\n [13.254208 ]\n [10.149159 ]\n [14.856738 ]\n [16.117392 ]\n [13.290119 ]\n [16.56748  ]\n [13.766315 ]\n [11.3078375]\n [13.673487 ]\n [14.328331 ]\n [13.435275 ]\n [12.0150175]\n [10.047468 ]\n [13.399735 ]\n [13.41787  ]\n [11.187707 ]\n [14.014764 ]\n [14.300038 ]\n [15.630251 ]\n [15.822072 ]\n [12.62164  ]\n [14.846689 ]\n [13.775651 ]\n [15.115735 ]\n [15.592129 ]\n [15.506436 ]\n [15.57649  ]\n [16.165611 ]\n [15.71686  ]\n [ 9.827143 ]\n [15.941123 ]\n [12.863666 ]\n [11.108892 ]\n [12.763419 ]\n [13.006585 ]\n [12.02232  ]\n [12.531422 ]\n [12.902798 ]\n [18.154474 ]\n [10.978656 ]\n [14.951765 ]\n [13.59904  ]\n [10.743044 ]\n [13.652675 ]\n [11.394083 ]\n [10.055647 ]\n [11.8569765]\n [18.201918 ]\n [13.7623   ]\n [13.315736 ]\n [10.41242  ]\n [12.442139 ]\n [17.672281 ]\n [10.602339 ]\n [19.301672 ]\n [13.190178 ]\n [15.934245 ]\n [11.906496 ]\n [11.072233 ]\n [16.26271  ]\n [12.403027 ]\n [11.701878 ]\n [10.75149  ]\n [14.197812 ]\n [14.293515 ]\n [12.060028 ]\n [14.751396 ]\n [ 8.544037 ]\n [11.948006 ]\n [ 8.880059 ]\n [15.475719 ]\n [14.490383 ]\n [13.720723 ]\n [13.678647 ]\n [16.671356 ]\n [14.069023 ]\n [11.989735 ]\n [11.512449 ]\n [ 5.788248 ]\n [13.212356 ]\n [12.989137 ]\n [ 9.068214 ]\n [12.722319 ]\n [16.464039 ]\n [ 8.7651005]\n [14.990641 ]\n [13.713154 ]\n [13.210179 ]\n [15.907828 ]\n [15.963796 ]\n [12.152448 ]\n [14.900991 ]\n [14.681725 ]\n [11.973158 ]\n [15.645206 ]\n [16.78511  ]\n [11.026073 ]\n [15.4751215]\n [13.363854 ]\n [16.239845 ]\n [10.871826 ]\n [19.136139 ]\n [12.375054 ]\n [13.317687 ]\n [13.339289 ]\n [18.74185  ]\n [15.182133 ]\n [12.794226 ]\n [13.143371 ]\n [11.462848 ]\n [13.731267 ]\n [13.242997 ]\n [13.5493965]\n [14.01435  ]\n [14.818561 ]\n [12.442192 ]\n [13.762187 ]\n [13.309325 ]\n [15.937656 ]\n [15.243026 ]\n [11.943106 ]\n [13.381215 ]\n [15.628558 ]\n [16.854681 ]\n [15.300036 ]\n [14.617861 ]\n [14.2145195]\n [12.176645 ]\n [12.383245 ]\n [13.782222 ]\n [11.801197 ]\n [17.607298 ]\n [ 9.263176 ]\n [14.615214 ]\n [12.210078 ]\n [16.79967  ]\n [12.007127 ]\n [12.503627 ]\n [13.145308 ]\n [10.102154 ]\n [13.599244 ]\n [17.477638 ]\n [15.676207 ]\n [13.047842 ]\n [17.530445 ]\n [14.660966 ]\n [ 9.079645 ]\n [18.066315 ]\n [15.628558 ]\n [12.907581 ]\n [15.571109 ]\n [11.954473 ]\n [13.32465  ]\n [16.21087  ]\n [15.478653 ]\n [14.505099 ]\n [18.78462  ]\n [14.3507   ]\n [17.444239 ]\n [14.138553 ]\n [10.5222   ]\n [11.89758  ]\n [ 7.95078  ]\n [12.138296 ]\n [14.714564 ]\n [ 9.37055  ]\n [13.875675 ]\n [14.402409 ]\n [13.690046 ]\n [16.12996  ]\n [14.191285 ]\n [13.537987 ]\n [15.654936 ]\n [14.377549 ]\n [12.477016 ]\n [12.154693 ]\n [13.311231 ]\n [14.667887 ]\n [11.805491 ]\n [11.083849 ]\n [11.096274 ]\n [14.555515 ]\n [11.779763 ]\n [15.354971 ]\n [16.556099 ]\n [13.189018 ]\n [13.081198 ]\n [14.145848 ]\n [12.114917 ]\n [13.931699 ]\n [13.914428 ]\n [16.658253 ]\n [13.320591 ]\n [11.033376 ]\n [ 8.148841 ]\n [15.956613 ]\n [10.513487 ]\n [19.814888 ]\n [11.906747 ]\n [ 7.571621 ]\n [12.014111 ]\n [ 8.488137 ]\n [16.415554 ]\n [13.860539 ]\n [15.653994 ]\n [14.204018 ]\n [15.0345335]\n [13.191093 ]\n [10.505278 ]\n [ 9.989289 ]\n [12.992263 ]\n [14.3669615]\n [13.507018 ]\n [19.896833 ]\n [16.767944 ]\n [12.589744 ]\n [11.840825 ]\n [14.669507 ]\n [15.676858 ]\n [13.042402 ]\n [13.284709 ]\n [14.430441 ]\n [12.17061  ]\n [15.942576 ]\n [15.122257 ]\n [14.374574 ]\n [13.892698 ]\n [16.12563  ]\n [11.810169 ]\n [15.96191  ]\n [14.270274 ]\n [11.674808 ]\n [15.618809 ]\n [12.973965 ]\n [11.187188 ]\n [13.337556 ]\n [12.768502 ]\n [10.365947 ]\n [14.344201 ]\n [14.686645 ]\n [13.934397 ]\n [11.437558 ]\n [14.854534 ]\n [17.391842 ]]", "q_t_selected": "[15.1657715 10.222781  13.828489  17.557508  13.326859  10.222204\n 13.435275  14.869973  13.614086  11.766128  13.831872  14.910767\n 13.254208  10.149159  14.856738  16.117392  13.290119  16.56748\n 13.766315  11.3078375 13.673487  14.328331  13.435275  12.0150175\n 10.047468  13.399735  13.41787   11.187707  14.014764  14.300038\n 15.630251  15.822072  12.62164   14.846689  13.775651  15.115735\n 15.592129  15.506436  15.57649   16.165611  15.71686    9.827143\n 15.941123  12.863666  11.108892  12.763419  13.006585  12.02232\n 12.531422  12.902798  18.154474  10.978656  14.951765  13.59904\n 10.743044  13.652675  11.394083  10.055647  11.8569765 18.201918\n 13.7623    13.315736  10.41242   12.442139  17.672281  10.602339\n 19.301672  13.190178  15.934245  11.906496  11.072233  16.26271\n 12.403027  11.701878  10.75149   14.197812  14.293515  12.060028\n 14.751396   8.544037  11.948006   8.880059  15.475719  14.490383\n 13.720723  13.678647  16.671356  14.069023  11.989735  11.512449\n  5.788248  13.212356  12.989137   9.068214  12.722319  16.464039\n  8.7651005 14.990641  13.713154  13.210179  15.907828  15.963796\n 12.152448  14.900991  14.681725  11.973158  15.645206  16.78511\n 11.026073  15.4751215 13.363854  16.239845  10.871826  19.136139\n 12.375054  13.317687  13.339289  18.74185   15.182133  12.794226\n 13.143371  11.462848  13.731267  13.242997  13.5493965 14.01435\n 14.818561  12.442192  13.762187  13.309325  15.937656  15.243026\n 11.943106  13.381215  15.628558  16.854681  15.300036  14.617861\n 14.2145195 12.176645  12.383245  13.782222  11.801197  17.607298\n  9.263176  14.615214  12.210078  16.79967   12.007127  12.503627\n 13.145308  10.102154  13.599244  17.477638  15.676207  13.047842\n 17.530445  14.660966   9.079645  18.066315  15.628558  12.907581\n 15.571109  11.954473  13.32465   16.21087   15.478653  14.505099\n 18.78462   14.3507    17.444239  14.138553  10.5222    11.89758\n  7.95078   12.138296  14.714564   9.37055   13.875675  14.402409\n 13.690046  16.12996   14.191285  13.537987  15.654936  14.377549\n 12.477016  12.154693  13.311231  14.667887  11.805491  11.083849\n 11.096274  14.555515  11.779763  15.354971  16.556099  13.189018\n 13.081198  14.145848  12.114917  13.931699  13.914428  16.658253\n 13.320591  11.033376   8.148841  15.956613  10.513487  19.814888\n 11.906747   7.571621  12.014111   8.488137  16.415554  13.860539\n 15.653994  14.204018  15.0345335 13.191093  10.505278   9.989289\n 12.992263  14.3669615 13.507018  19.896833  16.767944  12.589744\n 11.840825  14.669507  15.676858  13.042402  13.284709  14.430441\n 12.17061   15.942576  15.122257  14.374574  13.892698  16.12563\n 11.810169  15.96191   14.270274  11.674808  15.618809  12.973965\n 11.187188  13.337556  12.768502  10.365947  14.344201  14.686645\n 13.934397  11.437558  14.854534  17.391842 ]", "twin_q_t_selected": "[15.118261   9.96345   13.551957  16.73859   13.870374  10.534067\n 13.799606  14.124     13.353851  11.619203  13.797777  15.699857\n 13.700872  10.4935055 14.910685  15.484198  13.066973  16.177162\n 13.5057535 11.854094  14.4975    14.62545   13.799606  12.138044\n 10.171114  13.927055  13.168439  11.931233  13.948319  14.853562\n 15.388616  15.594018  13.179947  14.468913  14.282391  14.584381\n 16.10668   15.6363    14.998447  16.400017  16.211063  10.403778\n 16.986567  12.380862  10.135035  12.255829  12.92926   12.508511\n 11.705121  12.764216  18.3535    11.404069  15.269522  13.405631\n 11.057824  14.141693  11.477787   9.788153  11.96908   18.610811\n 13.304792  13.199524  10.814259  12.321252  17.180513  10.362478\n 19.94735   13.465186  15.193081  13.181086  11.610674  16.08724\n 12.130774  12.231086  10.992576  14.538152  14.034943  10.862329\n 15.238113   8.869516  11.1161175  8.912595  14.9537115 15.399258\n 14.731298  14.14969   16.674051  14.681845  12.218614  11.072774\n  5.715175  12.705346  13.705548   9.036858  13.167938  15.16952\n  8.62358   15.718627  14.112067  12.906171  15.4307    15.554999\n 12.538814  14.70289   13.366793  11.93733   15.183098  15.713052\n 10.904941  15.421934  13.608017  15.773437   9.707642  19.7515\n 12.477693  13.761029  13.188634  18.198149  15.394423  12.327404\n 12.810992  11.263393  14.541067  13.2380295 14.020313  13.816678\n 15.024321  12.578289  13.354133  13.686255  14.98255   16.019655\n 11.393905  13.826164  15.858822  17.523739  15.626872  15.658371\n 13.390949  12.409805  12.063912  13.8284025 11.452059  17.93992\n  8.589823  14.8584795 13.227696  16.324785  12.080662  12.8376\n 13.313406  10.786164  12.023413  19.057598  15.414164  13.2184\n 17.263689  13.8206005  9.099255  17.429132  15.858822  11.753587\n 15.518513  11.837397  13.065962  16.103472  16.25388   14.384199\n 17.754602  14.255866  17.161734  14.528221  11.026461  11.398087\n  8.131462  12.891259  15.185125   9.186631  14.158085  14.612452\n 14.1805935 16.819244  13.937011  13.018869  14.77323   13.563395\n 12.106676  11.081651  12.776035  13.394701  11.64605   10.89159\n 11.258363  14.493665  11.233971  15.006755  17.214027  12.274821\n 13.637888  13.734196  12.191034  13.790454  13.3296585 16.497717\n 13.170495  10.786935   8.046765  15.641569  10.941443  20.445427\n 11.448476   6.9783745 12.102745   9.191918  17.119669  13.704771\n 17.031506  14.071983  14.819793  12.738584  10.748023   9.928683\n 13.1504135 13.893537  13.72129   20.00902   17.202524  12.869623\n 11.955811  14.07138   15.352709  13.404189  13.350198  14.262792\n 12.755016  15.717566  15.168018  13.715031  14.098079  16.342796\n 12.53921   15.399173  14.118156  11.593528  15.069829  12.85109\n 10.683831  13.0892515 13.043889   9.868335  14.082651  14.49268\n 13.604696  11.628875  14.777826  17.646841 ]", "q_t_selected_target": "[16.668947  12.451287  13.345509  16.682768  14.095363  11.0326805\n 16.451061  14.989687  14.611508  10.802309  12.388041  16.06511\n 13.188285   9.289585  15.361653  13.7720375 14.551013  16.548128\n 12.079627  10.800834  14.336523  13.794832  16.746895  12.558779\n  9.84356   12.822154  13.690857  14.167425  13.504367  14.145261\n 15.124348  16.627491  13.633823  13.550899  12.947538  15.5899515\n 15.043631  14.642548  14.077055  14.628964  14.423415   9.790696\n 15.7103405 10.993261  10.42681   14.32583   13.7173605 11.764424\n 12.231379  11.916736  17.284594  12.230411  14.517662  13.972257\n 11.438906  12.221661  11.495416  11.266447  12.867946  17.900227\n 15.26524   14.544793  10.555363  13.9374485 18.143623  10.936284\n 20.744768  14.287636  17.185083  11.954397  12.015227  17.160652\n 12.973797  11.873727   9.721684  15.76723   14.523138  11.913175\n 15.637773   6.4384556 11.180293   8.418947  15.3620615 14.265485\n 14.368021  14.907805  16.749903  13.245963  13.041411  11.308867\n  5.2443075 13.652731  14.046814   9.62181   13.217152  15.737872\n  8.996281  15.209058  14.2468815 12.949088  17.597073  14.899202\n 11.336151  15.908979  14.63824   12.350985  16.686953  18.044472\n 10.162067  16.643448  13.157438  16.35629   11.277292  20.20599\n 12.813283  15.28239   12.27038   17.463486  15.00579   13.628969\n 12.506649  12.686704  13.288683  13.80445   14.688727  13.713644\n 14.205933  13.367919  12.833159  13.190832  16.223522  15.668469\n 10.2256365 13.011039  17.00574   18.452238  14.703747  13.904938\n 11.844082  12.926298  13.25505   14.842446  12.49079   16.97666\n  9.628078  14.127275  12.569478  14.9180565 11.542165  11.216389\n 12.572697  11.30394   13.054866  17.652393  16.630695  12.259659\n 14.764795  13.632703   8.027357  15.595524  17.28199   11.846304\n 13.969173  13.241603  13.215712  15.950769  16.196363  14.927009\n 18.419014  13.486652  16.803928  15.237363  11.935036  13.113759\n  8.631219  11.34769   14.432773   8.279219  13.518068  15.321312\n 12.644194  15.0730915 14.62697   13.466656  16.015848  13.755788\n 13.184243  10.831672  13.26327   13.875753  13.244829   9.00738\n 11.952862  15.576597  12.419983  13.465055  16.915346  14.088211\n 12.635115  13.634072  11.70345   12.237374  13.356711  18.369122\n 12.64369   10.055359   9.565004  16.835928  10.865197  21.372227\n 11.489684   9.05155   11.815574   9.981975  18.253637  12.529782\n 13.663078  14.805511  14.962633  13.313771   9.936282   8.917815\n 11.422229  13.603463  14.821095  22.66861   14.650441  13.640265\n 12.238033  14.653721  14.657927  12.785139  11.621518  13.193557\n 12.282669  15.522174  15.587649  12.954579  13.0302305 17.425503\n 12.074865  14.301017  14.416262  11.741626  15.222652  12.509082\n 10.441156  12.672785  12.128296  10.252817  14.216564  14.856602\n 12.88719   12.436513  15.73874   16.652327 ]", "q_tp1_best_masked": "[17.620897  14.286186  13.766022  15.995454  13.731251  13.415951\n 16.478256  14.962457  15.784349  11.1514225 12.597876  17.187033\n 13.95741   10.34158   16.11939   13.681901  15.383701  16.951725\n 12.808182  10.985967  14.895881  13.032947  16.777079  13.427707\n 10.757209  12.7633295 14.667593  15.324136  13.895972  15.192993\n 16.13933   16.23118   14.707982  13.588568  13.695908  16.822966\n 15.287005  15.429972  14.763752  14.642807  14.361919  10.263317\n 15.760297  11.8666725 11.0048065 14.8217    14.859761  12.9931\n 12.828962  12.337858  17.0918    13.795387  15.496338  16.019375\n 12.352742  12.798458  12.199642  13.005457  14.10331   18.494942\n 15.853281  13.916069  11.759437  15.302723  18.514107  12.109192\n 19.93292   15.017435  16.418823  12.789181  13.152615  16.74755\n 13.214156  12.585637  10.814889  16.273876  14.4519005 12.36123\n 16.004414   8.153545  12.139008  10.327245  15.078869  13.913487\n 14.268836  15.370847  17.287115  12.76339   13.686117  13.154665\n  6.750105  14.214488  15.460857  11.63611   14.489326  14.9144535\n 11.831568  15.586509  14.757272  14.272852  17.619078  15.008768\n 12.797583  14.602823  14.804017  14.04526   17.137264  17.819864\n 12.323561  16.594765  14.155603  15.880798  11.988266  18.727581\n 13.492403  15.527972  13.747377  17.262878  15.450272  15.007547\n 13.19648   12.916671  13.102279  14.370926  15.561023  14.819834\n 13.9299965 14.232142  12.966474  13.907971  15.931881  15.876581\n 11.591762  13.634682  15.989303  17.774923  14.442344  13.409824\n 12.335585  14.127579  14.737338  14.825263  14.141155  16.964901\n 11.363803  14.273759  12.838959  14.608703  12.224332  12.829228\n 13.1224785 13.369805  14.323018  17.156466  16.483421  14.832112\n 14.839085  13.68473    8.855528  15.586563  16.268343  12.028552\n 13.316523  13.497589  12.987747  15.029814  15.974351  16.035477\n 17.738892  13.444147  16.01488   14.8238945 12.702791  13.738214\n 11.43132   11.930895  14.55437    9.869046  14.628107  15.155197\n 13.274736  15.255851  15.492179  14.169045  15.997243  13.823399\n 12.863859  11.164153  13.739061  14.116167  13.660565   9.865398\n 13.3131075 15.577712  13.57658   12.737624  16.08009   14.770637\n 12.283702  14.324401  12.334526  13.136066  13.706262  18.305643\n 13.403938  11.097253  11.072957  17.388325  12.872536  20.734285\n 12.913397  11.045404  11.947792  10.853297  17.01015   12.425635\n 13.73264   14.538621  15.053453  14.009237  10.853458   9.754579\n 12.024917  13.846912  14.856566  21.609577  14.207011  14.69873\n 14.320203  14.547024  14.70624   13.453336  11.844266  13.188404\n 12.568296  16.423847  15.178459  12.984072  12.495773  17.01991\n 13.54713   14.503878  14.230146  12.138915  15.363221  12.381363\n 11.945437  12.95207   12.8241    11.892809  14.265565  15.318393\n 14.422981  13.252616  16.396694  16.30677  ]", "policy_t": "[[-0.5333968   0.4080131   0.57820344 -0.0806275  -0.13265455 -0.4545204 ]\n [-0.71136373 -0.23337686 -0.09260798  0.4425316  -0.65535396  0.24729836]\n [-0.27426815 -0.3040582   0.96149445  0.12268341 -0.3546338  -0.6165285 ]\n ...\n [-0.877877   -0.7801548   0.49319398 -0.05786037  0.6617527   0.57683945]\n [ 0.90202785  0.89006424  0.8166125  -0.68243897  0.4539789  -0.78515697]\n [ 0.15555751  0.05982959  0.69014263  0.8490236  -0.73246634 -0.8815781 ]]", "td_error": "[1.5269308  2.3581715  0.34471464 0.4652815  0.49674606 0.65454483\n 2.8336205  0.49270058 1.1275396  0.89035606 1.426784   0.7597976\n 0.28925514 1.0317473  0.477942   2.028757   1.372467   0.19515896\n 1.5564075  0.7801318  0.41200686 0.68205833 3.1294541  0.48224783\n 0.26573086 0.84124136 0.3977027  2.607955   0.47717476 0.43153954\n 0.38508558 0.919446   0.73302984 1.1069026  1.0814824  0.73989344\n 0.80577374 0.9288206  1.2104139  1.6538496  1.5405464  0.32476425\n 0.7535043  1.6290026  0.48692894 1.8162065  0.7494378  0.50099087\n 0.4131503  0.9167714  0.96939373 1.0390482  0.59298134 0.4699211\n 0.5384717  1.6755233  0.05948067 1.3445473  0.95491743 0.50613785\n 1.7316937  1.2871633  0.20091915 1.5557532  0.717226   0.45387554\n 1.1202574  0.9599538  1.6214204  0.63729477 0.67377377 0.98567677\n 0.7068968  0.2646041  1.1503482  1.3992481  0.35890913 0.5988498\n 0.64301777 2.268321   0.4159441  0.4773798  0.26100397 0.6793356\n 0.50528765 0.9936371  0.07719898 1.1294708  0.93723726 0.21983767\n 0.5074041  0.6938801  0.6994715  0.56927395 0.2720232  0.64725924\n 0.30194044 0.36399317 0.33427095 0.15200424 1.9278083  0.86019516\n 1.0094795  1.1070385  0.65746593 0.3957405  1.2728004  1.7953906\n 0.8034396  1.1949201  0.3284974  0.3496499  0.98755836 0.7621708\n 0.38690948 1.7430315  0.9935813  1.0065136  0.28248787 1.0681543\n 0.47053242 1.3235831  0.8474841  0.5639367  0.9038725  0.20186996\n 0.715508   0.8576784  0.7250004  0.3069582  0.76341915 0.38831472\n 1.4428687  0.5926509  1.2620492  1.2630281  0.75970745 1.2331781\n 1.9586525  0.63307285 1.0314708  1.0371342  0.86416245 0.7969494\n 0.7015791  0.60957146 0.5088091  1.6441708  0.5017295  1.4542246\n 0.6566601  0.8597808  0.7879157  0.78997993 1.0855103  0.8734622\n 2.6322718  0.6080804  1.0620928  2.1521997  1.5383     0.5769973\n 1.5756373  1.3456683  0.12934399 0.20640182 0.3876133  0.4823594\n 0.5150089  0.81663084 0.49905777 0.90397596 1.1607056  1.4659257\n 0.5900979  1.167088   0.5170722  0.999372   0.49881172 0.8138819\n 1.2911263  1.4015102  0.56282234 0.25955868 0.80176544 0.4070773\n 0.8923969  0.7865     0.26759768 0.63659286 1.5190582  1.98034\n 0.7755432  1.0520072  0.913116   1.7158074  0.32896423 1.3562913\n 0.7244282  0.3059497  0.44952536 1.623702   0.29238462 1.7911367\n 0.6018529  0.8547964  1.4672012  1.0368371  0.21397829 1.2420692\n 0.22913551 1.7765522  0.24285412 1.1419468  1.4860258  1.252873\n 2.6796713  0.667511   0.10737038 0.34893274 0.6903682  1.0411711\n 1.6491094  0.52678585 1.2069416  2.715683   2.334793   0.91058207\n 0.33971548 0.29906368 0.8568568  0.4381566  1.6959352  1.1530595\n 0.29220295 0.3078971  0.44251156 1.0902228  0.965158   1.1912899\n 0.36452055 1.3795247  0.22204638 0.10745811 0.27448988 0.40344572\n 0.4943533  0.5406189  0.77789974 0.248806   0.13077497 0.26693964\n 0.88235664 0.9032965  0.92255974 0.8670149 ]", "mean_td_error": 0.8854217529296875, "actor_loss": -14.771780014038086, "critic_loss": 0.5758240222930908, "alpha_loss": -11.958795547485352, "alpha_value": 0.3000197112560272, "target_entropy": -6, "mean_q": 13.618417739868164, "max_q": 19.896833419799805, "min_q": 5.788248062133789, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 14000, "episodes_total": 14, "training_iteration": 14, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_14-47-50", "timestamp": 1587048470, "time_this_iter_s": 86.17354798316956, "time_total_s": 374.3811526298523, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 374.3811526298523, "timesteps_since_restore": 14000, "iterations_since_restore": 14, "perf": {"cpu_util_percent": 92.24275862068966, "ram_util_percent": 11.299999999999999}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -134.71189280035543, "episode_reward_min": -336.3379905895369, "episode_reward_mean": -232.3518363262959, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-218.8506585442819, -336.3379905895369, -146.64398204480173, -207.54386149670867, -297.0036292607728, -201.57316553381884, -288.5776229736743, -193.3277536045946, -298.9478064144136, -134.71189280035543], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26372646749193757, "mean_processing_ms": 0.12039847376040033, "mean_inference_ms": 1.1770165822074092}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -56.48484841707745, "episode_reward_min": -413.76740122151716, "episode_reward_mean": -215.64906656525847, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-56.48484841707745, -413.76740122151716, -213.5742182258041, -200.0574765235254, -194.36138843836824], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.29633402947990584, "mean_processing_ms": 0.4113381756390503, "mean_inference_ms": 1.396117664479655}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 7500, "num_steps_trained": 1278464, "num_steps_sampled": 15000, "sample_time_ms": 2.994, "replay_time_ms": 22.79, "grad_time_ms": 56.205, "update_time_ms": 0.005, "opt_peak_throughput": 4554.746, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[13.096859 ]\n [15.801646 ]\n [ 9.384131 ]\n [12.823623 ]\n [16.69509  ]\n [16.140392 ]\n [22.860462 ]\n [14.667649 ]\n [ 9.743905 ]\n [14.166824 ]\n [14.85858  ]\n [14.464742 ]\n [11.440219 ]\n [17.634666 ]\n [15.139527 ]\n [14.598737 ]\n [21.830915 ]\n [13.579899 ]\n [13.634473 ]\n [10.509632 ]\n [12.997236 ]\n [15.814074 ]\n [15.2728405]\n [ 9.009259 ]\n [17.795954 ]\n [13.67796  ]\n [12.5541315]\n [15.672861 ]\n [12.871222 ]\n [15.647501 ]\n [10.235053 ]\n [11.571439 ]\n [13.3573265]\n [13.307271 ]\n [13.974393 ]\n [14.116882 ]\n [14.814437 ]\n [15.201109 ]\n [13.761523 ]\n [14.315172 ]\n [15.6974945]\n [12.261369 ]\n [13.679086 ]\n [17.950655 ]\n [16.283716 ]\n [13.68839  ]\n [15.4860735]\n [ 7.9725776]\n [13.787258 ]\n [15.725436 ]\n [14.990147 ]\n [13.61726  ]\n [16.152626 ]\n [15.646748 ]\n [ 7.47006  ]\n [13.730464 ]\n [14.951426 ]\n [16.046757 ]\n [17.36997  ]\n [12.262789 ]\n [12.133118 ]\n [11.750282 ]\n [14.25125  ]\n [11.681909 ]\n [14.09248  ]\n [17.846083 ]\n [13.04551  ]\n [16.33262  ]\n [14.007297 ]\n [18.77803  ]\n [16.302929 ]\n [14.84351  ]\n [14.460813 ]\n [14.906279 ]\n [10.430772 ]\n [16.359798 ]\n [14.368516 ]\n [14.765001 ]\n [13.104138 ]\n [13.6231575]\n [13.762351 ]\n [18.615591 ]\n [11.970421 ]\n [14.026916 ]\n [21.88901  ]\n [15.652735 ]\n [13.112937 ]\n [13.271621 ]\n [10.006487 ]\n [11.750282 ]\n [15.587291 ]\n [13.222169 ]\n [12.902902 ]\n [10.767321 ]\n [14.188213 ]\n [13.754002 ]\n [19.081247 ]\n [16.79017  ]\n [15.333461 ]\n [13.362886 ]\n [16.729824 ]\n [15.616856 ]\n [16.166464 ]\n [17.950655 ]\n [14.927228 ]\n [14.286476 ]\n [15.194204 ]\n [10.911189 ]\n [16.995287 ]\n [16.009396 ]\n [11.367827 ]\n [15.00169  ]\n [20.912035 ]\n [14.990764 ]\n [13.122143 ]\n [14.620397 ]\n [13.427551 ]\n [17.265976 ]\n [14.244871 ]\n [15.149617 ]\n [16.453527 ]\n [16.095037 ]\n [16.997175 ]\n [14.855516 ]\n [14.033677 ]\n [15.823491 ]\n [11.581106 ]\n [12.959894 ]\n [15.648548 ]\n [16.31057  ]\n [14.420791 ]\n [18.897865 ]\n [16.650345 ]\n [16.331814 ]\n [14.301006 ]\n [12.554021 ]\n [15.670746 ]\n [15.574149 ]\n [18.465534 ]\n [13.41734  ]\n [ 6.95612  ]\n [ 7.557734 ]\n [17.129242 ]\n [15.634645 ]\n [17.503084 ]\n [12.63144  ]\n [15.663201 ]\n [17.198914 ]\n [13.978819 ]\n [10.3690195]\n [10.12931  ]\n [16.433609 ]\n [14.348688 ]\n [18.20754  ]\n [17.003754 ]\n [ 8.219258 ]\n [14.909843 ]\n [16.630007 ]\n [13.807951 ]\n [15.7794285]\n [23.01512  ]\n [12.324272 ]\n [14.393859 ]\n [12.444306 ]\n [12.565346 ]\n [10.855778 ]\n [15.412628 ]\n [11.767697 ]\n [15.125214 ]\n [17.14133  ]\n [14.679352 ]\n [14.670614 ]\n [14.728149 ]\n [17.446457 ]\n [15.740805 ]\n [12.310244 ]\n [10.414418 ]\n [11.613078 ]\n [14.152342 ]\n [18.764019 ]\n [11.437337 ]\n [16.132725 ]\n [14.798579 ]\n [19.131355 ]\n [18.916302 ]\n [14.965836 ]\n [16.574785 ]\n [16.358736 ]\n [ 8.887051 ]\n [13.212665 ]\n [15.131906 ]\n [14.871444 ]\n [14.474406 ]\n [13.584053 ]\n [15.398583 ]\n [17.378782 ]\n [14.865639 ]\n [10.462427 ]\n [ 9.714903 ]\n [12.60981  ]\n [13.999101 ]\n [15.515892 ]\n [11.920548 ]\n [ 9.55772  ]\n [ 8.959768 ]\n [ 8.660939 ]\n [14.407172 ]\n [13.762756 ]\n [11.850933 ]\n [ 9.969598 ]\n [14.242742 ]\n [13.650032 ]\n [14.665509 ]\n [20.155159 ]\n [15.361309 ]\n [12.777841 ]\n [11.027698 ]\n [14.447729 ]\n [12.692681 ]\n [17.129242 ]\n [21.967402 ]\n [14.288147 ]\n [13.899762 ]\n [12.184284 ]\n [11.448373 ]\n [13.156236 ]\n [15.598738 ]\n [ 8.312034 ]\n [14.90647  ]\n [12.444306 ]\n [13.986689 ]\n [17.96534  ]\n [14.229121 ]\n [15.28494  ]\n [12.793283 ]\n [14.792611 ]\n [20.149109 ]\n [12.556628 ]\n [12.468453 ]\n [14.889914 ]\n [11.082001 ]\n [14.2245035]\n [15.604666 ]\n [19.231424 ]\n [13.886299 ]\n [16.967081 ]\n [12.38942  ]\n [20.006376 ]\n [14.8136015]\n [14.209133 ]\n [11.239926 ]\n [12.64713  ]\n [17.048931 ]\n [16.096869 ]\n [14.615558 ]\n [15.983175 ]]", "q_t_selected": "[13.096859  15.801646   9.384131  12.823623  16.69509   16.140392\n 22.860462  14.667649   9.743905  14.166824  14.85858   14.464742\n 11.440219  17.634666  15.139527  14.598737  21.830915  13.579899\n 13.634473  10.509632  12.997236  15.814074  15.2728405  9.009259\n 17.795954  13.67796   12.5541315 15.672861  12.871222  15.647501\n 10.235053  11.571439  13.3573265 13.307271  13.974393  14.116882\n 14.814437  15.201109  13.761523  14.315172  15.6974945 12.261369\n 13.679086  17.950655  16.283716  13.68839   15.4860735  7.9725776\n 13.787258  15.725436  14.990147  13.61726   16.152626  15.646748\n  7.47006   13.730464  14.951426  16.046757  17.36997   12.262789\n 12.133118  11.750282  14.25125   11.681909  14.09248   17.846083\n 13.04551   16.33262   14.007297  18.77803   16.302929  14.84351\n 14.460813  14.906279  10.430772  16.359798  14.368516  14.765001\n 13.104138  13.6231575 13.762351  18.615591  11.970421  14.026916\n 21.88901   15.652735  13.112937  13.271621  10.006487  11.750282\n 15.587291  13.222169  12.902902  10.767321  14.188213  13.754002\n 19.081247  16.79017   15.333461  13.362886  16.729824  15.616856\n 16.166464  17.950655  14.927228  14.286476  15.194204  10.911189\n 16.995287  16.009396  11.367827  15.00169   20.912035  14.990764\n 13.122143  14.620397  13.427551  17.265976  14.244871  15.149617\n 16.453527  16.095037  16.997175  14.855516  14.033677  15.823491\n 11.581106  12.959894  15.648548  16.31057   14.420791  18.897865\n 16.650345  16.331814  14.301006  12.554021  15.670746  15.574149\n 18.465534  13.41734    6.95612    7.557734  17.129242  15.634645\n 17.503084  12.63144   15.663201  17.198914  13.978819  10.3690195\n 10.12931   16.433609  14.348688  18.20754   17.003754   8.219258\n 14.909843  16.630007  13.807951  15.7794285 23.01512   12.324272\n 14.393859  12.444306  12.565346  10.855778  15.412628  11.767697\n 15.125214  17.14133   14.679352  14.670614  14.728149  17.446457\n 15.740805  12.310244  10.414418  11.613078  14.152342  18.764019\n 11.437337  16.132725  14.798579  19.131355  18.916302  14.965836\n 16.574785  16.358736   8.887051  13.212665  15.131906  14.871444\n 14.474406  13.584053  15.398583  17.378782  14.865639  10.462427\n  9.714903  12.60981   13.999101  15.515892  11.920548   9.55772\n  8.959768   8.660939  14.407172  13.762756  11.850933   9.969598\n 14.242742  13.650032  14.665509  20.155159  15.361309  12.777841\n 11.027698  14.447729  12.692681  17.129242  21.967402  14.288147\n 13.899762  12.184284  11.448373  13.156236  15.598738   8.312034\n 14.90647   12.444306  13.986689  17.96534   14.229121  15.28494\n 12.793283  14.792611  20.149109  12.556628  12.468453  14.889914\n 11.082001  14.2245035 15.604666  19.231424  13.886299  16.967081\n 12.38942   20.006376  14.8136015 14.209133  11.239926  12.64713\n 17.048931  16.096869  14.615558  15.983175 ]", "twin_q_t_selected": "[12.99323   15.721801   9.92175   13.2164    16.42189   15.999256\n 22.972649  15.031431  11.022071  13.723498  15.302024  14.795778\n 12.478922  17.78669   16.22804   14.903773  21.45452   14.271044\n 13.742999   9.648507  12.262697  15.460255  14.720962   9.031066\n 17.283783  13.433564  12.558904  14.971843  12.715131  15.207442\n  9.961382  11.474097  14.220612  13.459899  14.027729  14.350386\n 14.860353  15.472236  13.468119  14.337674  16.393557  13.088538\n 13.757018  17.796995  14.99831   14.163862  15.079912   8.210856\n 14.025325  15.216966  16.065311  14.754662  16.57813   16.209085\n  7.693981  13.939345  14.80865   15.776869  16.985672  11.3874035\n 13.018525  11.982152  14.421528  12.101549  14.795497  16.983524\n 14.344247  17.009056  14.639262  19.175371  16.017555  14.75751\n 15.077007  15.167296   9.921584  16.56205   13.754508  15.034365\n 13.345     14.354982  13.951542  19.702448  11.589887  13.749642\n 22.123913  15.595052  12.138542  13.311391  10.207933  11.982152\n 14.792208  13.687479  13.095072  10.8280735 14.060497  13.771318\n 18.344828  16.77523   15.405647  13.004378  16.93666   14.931826\n 15.907828  17.796995  14.882075  14.119597  14.535169  10.412542\n 17.197098  15.598826  11.931695  15.084016  20.63738   14.8518505\n 13.988146  15.2584305 13.805955  16.837532  14.461083  15.012804\n 16.387629  14.873729  17.683397  14.058998  14.889795  16.045744\n 11.086064  12.850275  16.556377  15.753083  15.380113  19.23897\n 15.972488  15.917577  14.74709   13.267104  16.157013  15.684007\n 18.228653  13.717981   7.163257   7.9925785 16.449709  15.122961\n 17.54867   13.44029   15.849032  17.172264  14.158412   9.840487\n 12.001873  15.934922  13.831017  18.67908   16.000202   8.754235\n 14.900311  16.12751   13.183231  15.821576  22.567184  12.078419\n 14.704109  11.948804  12.224547  11.671976  16.543564  11.87942\n 15.77766   16.90187   14.67882   14.85311   14.533392  17.630033\n 15.686874  12.909548  10.337769  11.224051  14.057421  19.207407\n 12.566758  16.490917  15.186     18.94402   18.875992  14.592935\n 16.559805  16.938944  10.854864  13.262764  15.688098  14.353775\n 14.0853195 14.126136  14.811857  17.494389  14.977247  10.8745365\n  9.4073925 12.9115095 14.310008  15.029121  12.185162   8.552725\n  8.608024   8.191754  14.634869  13.730432  11.507637   9.911912\n 14.785946  14.534522  14.436042  19.853155  15.591916  13.441794\n 11.385888  15.126648  13.143444  16.449709  21.812576  14.6224375\n 14.1097355 11.212284  12.097678  13.546048  15.384426   8.985499\n 14.982219  11.948804  14.498543  17.875946  13.61309   15.672577\n 12.917638  15.190743  19.703886  12.466165  12.110785  15.089328\n 10.364439  13.651055  16.189465  19.393993  14.08616   17.827581\n 12.583545  20.75057   14.895998  13.549845  12.1076355 11.080792\n 18.270813  16.451134  13.568696  15.514799 ]", "q_t_selected_target": "[11.700024  14.615525   8.2303    12.803594  15.39016   17.218803\n 22.335384  14.763513  10.441512  14.387998  14.56989   14.608024\n  9.514338  17.403265  14.41682   13.825063  23.62824   14.5567665\n 11.513483   9.708633  13.165893  15.754847  14.352183   8.185488\n 18.007868  14.676763  12.26182   14.6128235 13.970544  14.044273\n  9.958863   9.777583  15.652216  14.183521  13.302831  13.075265\n 15.23973   16.001947  13.941101  14.918643  16.471327  12.526354\n 14.429052  15.68748   15.840309  13.356805  14.427123   7.2607183\n 14.41341   14.600884  15.64955   12.581362  17.33796   16.041983\n  6.2893276 11.940803  13.768484  16.544935  17.734755  10.883476\n 12.143148  10.464488  13.500918  12.700385  14.848685  15.517399\n 13.164696  16.698208  14.5116415 17.458363  14.559293  15.011225\n 14.640861  14.456941   9.220628  16.262594  13.976064  13.640762\n 11.913254  10.765651  14.227002  18.566347  10.58569   13.692457\n 23.088696  16.494923  11.722091  12.657953  10.351997  10.422444\n 16.366247  12.935626  12.338489   9.397926  13.476082  14.782123\n 18.342726  15.455428  14.852384  13.460416  15.795289  13.716745\n 15.848549  15.93666   14.053531  13.823161  13.701036   9.865188\n 16.663614  16.368637  11.651396  15.75146   19.76743   14.200611\n 12.644245  14.525242  13.45356   17.676786  13.4755    15.608015\n 16.986     15.258812  17.594368  14.572594  13.716055  16.11376\n 10.623919  12.93044   15.272465  16.982937  14.683311  17.458048\n 15.25248   15.928164  13.702152  13.124204  15.9908285 13.82564\n 18.440014  13.791731   6.7960515  6.645461  17.000475  14.660571\n 18.554781  13.654776  14.163405  16.010674  15.187039  10.520716\n 10.926525  15.329278  14.694518  20.40819   14.973123   5.4590406\n 16.083109  14.256798  13.741669  15.722096  23.297226  12.415544\n 14.426046  13.348515  13.705132  11.573313  15.181703  10.667305\n 14.486771  17.98257   14.178692  15.428832  14.941536  16.379477\n 14.512412  13.613558   9.67816   11.785195  14.625825  19.563585\n 12.6744995 16.314093  14.289448  19.24628   18.960056  15.19666\n 17.1798    15.506854   9.866436  12.990076  14.657402  16.348133\n 14.115898  14.912605  16.369045  17.881535  15.143953   9.736012\n  9.124348  12.715659  15.456837  14.390853  12.313395   9.651028\n  7.269809  10.1405735 14.277536  14.694272   9.339283  10.901227\n 13.723554  13.83513   12.549486  21.647198  15.285511  14.09692\n  8.78364   15.292867  13.725838  16.97494   22.299644  13.644981\n 14.884109  10.955681  10.996392  14.4606285 13.862857   8.283591\n 16.582     13.331421  13.56992   18.676603  12.979374  16.558722\n 11.818804  13.871135  19.945036  12.0268135 11.961959  13.600433\n 10.236672  14.518987  15.28715   19.98195   13.089461  17.243832\n 11.555514  19.245884  14.86028   11.8509445 11.343531  12.754116\n 15.2193775 14.786912  13.541346  14.905978 ]", "q_tp1_best_masked": "[12.105525  15.124307   9.404197  13.841734  15.167537  18.77569\n 21.203075  16.361181  13.602008  14.637     16.483006  16.216825\n 11.475222  16.503557  14.930567  14.829971  22.411182  14.399101\n 12.713651  11.442605  14.111678  15.9503565 15.3706875  9.73745\n 17.501923  15.168117  13.017605  15.72793   15.465946  14.6028385\n 11.58233   11.29455   15.791889  13.885622  13.355145  13.361961\n 15.784765  16.089418  13.0263    15.891127  16.82431   12.561305\n 15.326931  15.346754  15.851344  14.084231  14.816168   9.417882\n 14.937095  14.6988    15.988197  14.440937  18.63486   17.202656\n  8.930285  13.408342  14.506932  16.759975  18.388996  11.734651\n 12.91711   11.144132  13.9148035 13.376312  14.992963  15.72429\n 13.824813  16.753382  15.138694  16.744263  14.551825  15.425662\n 15.088034  14.256369  10.632227  16.550097  13.986902  14.292293\n 13.3448305 12.151285  14.835063  18.126074  12.283722  13.768869\n 21.896465  16.936836  12.346813  13.835811  12.185342  11.101663\n 17.376947  13.304887  13.384854  11.0965395 14.111491  15.66063\n 17.174995  16.243061  14.974097  14.104757  16.692247  13.423465\n 15.681362  15.598451  13.701022  15.203338  14.30231   11.677712\n 16.669777  16.505377  13.014265  16.210043  17.741867  15.002003\n 13.602782  14.907079  14.096971  17.230356  12.698731  16.004436\n 16.511995  14.607315  18.112217  15.791635  13.924374  15.829088\n 11.612146  14.569859  15.841355  17.670162  16.270699  16.82065\n 15.489948  15.9264    13.585541  13.400962  17.446249  14.370481\n 19.636803  15.3262615  8.309789   8.247887  17.241825  15.600275\n 18.399723  14.8030615 14.169921  16.755802  15.886066  11.432537\n 12.571135  15.348872  14.452966  19.314587  14.831424   6.4568586\n 16.211046  15.143803  14.45571   15.607193  22.345371  13.280098\n 14.635581  14.514491  14.732679  13.002266  15.789033  10.8925905\n 14.701242  18.196587  14.9945755 15.452923  16.212637  16.396805\n 14.586708  14.384055  11.679623  12.723569  15.942377  18.897495\n 13.344105  16.52758   14.442043  18.888165  18.70191   15.632315\n 16.197412  14.970906  12.8426285 13.33437   15.372484  16.372807\n 14.998309  15.236201  18.20483   17.534271  16.925966  11.020861\n 10.64373   12.7355    16.72413   14.623401  13.391408  11.952815\n  9.093216  11.65434   13.830023  15.524714  11.243797  11.875069\n 14.11987   15.243062  12.797818  20.717178  15.420109  15.108774\n 10.441329  14.7727    15.057612  17.21603   21.990526  14.1628\n 15.319893  12.588113  13.41433   14.702636  14.376221   9.757001\n 17.254425  14.497225  13.9449835 19.233301  12.880177  16.290302\n 12.915706  14.398454  19.181658  13.2347975 13.755036  14.038949\n 11.220848  14.565495  14.770754  19.62913   13.359237  17.315945\n 13.108327  17.945354  14.229219  12.359641  12.551701  14.482824\n 15.519978  15.138362  14.918493  15.166197 ]", "policy_t": "[[ 0.955848   -0.48701537 -0.09849137 -0.67640305  0.82216203 -0.4922856 ]\n [-0.81121856  0.90740144  0.9642277   0.29143095  0.5153873  -0.45087743]\n [ 0.33756876 -0.1003958  -0.6493717  -0.7532474   0.9567158   0.7884278 ]\n ...\n [ 0.9016148   0.58854103 -0.5400287   0.20285249 -0.6550726   0.72032785]\n [-0.02863288 -0.34930968 -0.5538197   0.14235389  0.9203937   0.22232234]\n [-0.55730265  0.32103848 -0.9252233   0.37061036 -0.04646581  0.25779843]]", "td_error": "[1.3450208  1.1461983  1.4226408  0.21641779 1.1683302  1.1489792\n 0.58117104 0.18189096 0.6390829  0.44283628 0.51041174 0.16551828\n 2.4452329  0.3074131  1.2669644  0.9261923  1.9855232  0.6312952\n 2.175253   0.4305625  0.53592587 0.17690945 0.6447177  0.83467484\n 0.46799946 1.1210003  0.29469776 0.70952845 1.1773677  1.3831983\n 0.13935423 1.7451849  1.8632469  0.7999363  0.69823027 1.1583691\n 0.4023347  0.6652751  0.32628012 0.5922198  0.42580128 0.4135847\n 0.71100044 2.186345   0.64270306 0.56932116 0.85586977 0.83099866\n 0.5071187  0.8703165  0.5375824  1.604599   0.9725809  0.28116894\n 1.2926929  1.8941021  1.1115537  0.63312244 0.5569334  0.9416199\n 0.44270372 1.4017291  0.8354707  0.8086562  0.40469694 1.8974047\n 0.6493683  0.33821774 0.31598282 1.5183382  1.6009493  0.21071482\n 0.30809736 0.57984686 0.9555502  0.19832993 0.30700397 1.2589207\n 1.3113155  3.2234192  0.37005568 0.59267235 1.1944642  0.19582176\n 1.0822344  0.8710294  0.90364885 0.63355255 0.24478722 1.4437728\n 1.1764979  0.51919794 0.66049814 1.3997707  0.64827347 1.0194626\n 0.37031174 1.3272724  0.5171704  0.27678347 1.0379534  1.5575953\n 0.1885972  1.9371653  0.85112095 0.37987566 1.16365    0.79667807\n 0.4325781  0.5645261  0.28193378 0.7086072  1.0072784  0.720696\n 0.91089916 0.4141717  0.18920183 0.6250324  0.87747717 0.52680445\n 0.56542206 0.61065435 0.34311104 0.39825916 0.7456813  0.17914248\n 0.7096667  0.05480957 0.829998   0.95111036 0.479661   1.6103697\n 1.0589371  0.20711851 0.8218961  0.35654163 0.24313354 1.8034382\n 0.11844063 0.22407007 0.26363707 1.1296952  0.3397665  0.71823215\n 1.028904   0.6189103  1.5927114  1.1749153  1.1184239  0.4159627\n 0.9362817  0.8549876  0.60466576 1.96488    1.5288553  3.0277061\n 1.1780314  2.1219606  0.3123598  0.07840586 0.50607395 0.21419811\n 0.15512514 1.1519594  1.310185   0.40809917 0.7963934  1.1562538\n 0.96466637 0.9609699  0.50039387 0.6669698  0.31076527 1.1587687\n 1.2014275  1.0036621  0.6979337  0.36663055 0.52094364 0.5778723\n 0.672452   0.17909622 0.70284176 0.20859337 0.06390953 0.41727495\n 0.61250496 1.1419859  0.98390675 0.24763823 0.7525997  1.7355237\n 0.19454336 1.0575109  1.2638249  0.44494915 0.22251034 0.93246937\n 0.4368     0.15084982 1.3022823  0.8816538  0.26053953 0.59580517\n 1.5140872  1.7142267  0.24348402 0.9476781  2.340002   0.9604721\n 0.7907901  0.442245   2.0012894  1.6430407  0.19110155 0.9871025\n 2.423153   0.5056782  0.807775   0.3397665  0.40965557 0.81031084\n 0.8793597  0.7426033  0.77663326 1.1094866  1.628725   0.36517525\n 1.6376562  1.1348658  0.6726961  0.75596046 0.94173145 1.0799632\n 1.0366569  1.1205425  0.22261143 0.4845829  0.32766008 1.3891873\n 0.48654747 0.5812073  0.6099148  0.6692419  0.8967681  0.43025017\n 0.9309678  1.1325893  0.04119825 2.0285444  0.43385458 0.89015484\n 2.4404945  1.4870892  0.55078125 0.843009  ]", "mean_td_error": 0.8403942584991455, "actor_loss": -15.368440628051758, "critic_loss": 0.534282922744751, "alpha_loss": -14.261063575744629, "alpha_value": 0.22324177622795105, "target_entropy": -6, "mean_q": 14.424063682556152, "max_q": 23.015119552612305, "min_q": 6.956120014190674, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 15000, "episodes_total": 15, "training_iteration": 15, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_14-49-33", "timestamp": 1587048573, "time_this_iter_s": 86.10295987129211, "time_total_s": 460.4841125011444, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 460.4841125011444, "timesteps_since_restore": 15000, "iterations_since_restore": 15, "perf": {"cpu_util_percent": 92.25724137931034, "ram_util_percent": 11.299999999999999}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -90.33941733906497, "episode_reward_min": -253.8691242502863, "episode_reward_mean": -197.58472571188798, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-192.12014543995622, -157.91264566449183, -242.86445697377198, -221.9289442023864, -239.66511311687768, -198.1078845078753, -253.8691242502863, -90.33941733906497, -133.12370260608463, -245.91582301808472], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2634215674875034, "mean_processing_ms": 0.12011312994800982, "mean_inference_ms": 1.1737465794086888}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -56.48484841707745, "episode_reward_min": -220.91228762964403, "episode_reward_mean": -177.07804384688384, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-220.91228762964403, -213.5742182258041, -200.0574765235254, -194.36138843836824, -56.48484841707745], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.3034672267391866, "mean_processing_ms": 0.4195080262603962, "mean_inference_ms": 1.4459875349176208}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 8000, "num_steps_trained": 1534464, "num_steps_sampled": 16000, "sample_time_ms": 2.985, "replay_time_ms": 20.832, "grad_time_ms": 57.864, "update_time_ms": 0.005, "opt_peak_throughput": 4424.132, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[16.356745 ]\n [15.3237095]\n [14.304459 ]\n [10.246355 ]\n [13.588667 ]\n [14.620359 ]\n [17.752363 ]\n [13.904019 ]\n [11.597382 ]\n [15.799087 ]\n [10.366158 ]\n [19.539665 ]\n [18.149662 ]\n [13.079149 ]\n [14.147118 ]\n [ 9.238508 ]\n [15.650451 ]\n [12.345778 ]\n [16.0523   ]\n [13.030964 ]\n [17.378716 ]\n [19.328777 ]\n [15.150155 ]\n [19.031958 ]\n [16.150225 ]\n [17.586021 ]\n [14.841913 ]\n [16.346344 ]\n [15.938532 ]\n [ 7.0846434]\n [10.80579  ]\n [15.451792 ]\n [19.651058 ]\n [15.27137  ]\n [20.169231 ]\n [13.141767 ]\n [16.78214  ]\n [17.781225 ]\n [14.080586 ]\n [15.11332  ]\n [15.749163 ]\n [14.141905 ]\n [16.292158 ]\n [16.73947  ]\n [13.033565 ]\n [13.700251 ]\n [16.575611 ]\n [ 8.592604 ]\n [21.567299 ]\n [11.656136 ]\n [24.085604 ]\n [13.10084  ]\n [15.223464 ]\n [13.57996  ]\n [21.04424  ]\n [14.360536 ]\n [13.238564 ]\n [14.348597 ]\n [ 6.8195477]\n [16.714357 ]\n [13.438246 ]\n [13.490963 ]\n [16.52389  ]\n [18.953104 ]\n [10.418812 ]\n [14.01239  ]\n [15.551601 ]\n [16.331553 ]\n [11.782188 ]\n [17.7432   ]\n [11.419907 ]\n [12.869212 ]\n [16.816296 ]\n [22.18056  ]\n [13.761436 ]\n [14.01009  ]\n [ 9.806859 ]\n [15.857252 ]\n [15.233466 ]\n [14.406906 ]\n [15.401153 ]\n [13.975659 ]\n [15.675077 ]\n [13.918324 ]\n [11.946463 ]\n [12.545955 ]\n [17.529234 ]\n [12.934405 ]\n [12.833232 ]\n [15.5154705]\n [16.688272 ]\n [15.942369 ]\n [17.127424 ]\n [17.405807 ]\n [ 9.967167 ]\n [11.153131 ]\n [16.696157 ]\n [18.68382  ]\n [16.778137 ]\n [16.807016 ]\n [15.352862 ]\n [18.309996 ]\n [16.49597  ]\n [11.861127 ]\n [22.371075 ]\n [12.715906 ]\n [15.6307125]\n [13.308146 ]\n [13.533214 ]\n [13.97933  ]\n [14.543126 ]\n [18.30433  ]\n [15.419934 ]\n [14.897022 ]\n [17.535261 ]\n [15.799273 ]\n [16.589727 ]\n [18.660152 ]\n [12.120033 ]\n [17.022955 ]\n [16.403309 ]\n [16.30666  ]\n [16.579403 ]\n [12.241897 ]\n [12.963132 ]\n [14.128008 ]\n [12.788522 ]\n [13.257464 ]\n [14.193667 ]\n [14.503722 ]\n [ 6.52774  ]\n [14.365608 ]\n [15.158313 ]\n [15.677931 ]\n [15.339976 ]\n [14.8869095]\n [17.034775 ]\n [18.886467 ]\n [17.371456 ]\n [19.235085 ]\n [15.864772 ]\n [16.52271  ]\n [ 7.826096 ]\n [15.446707 ]\n [12.976635 ]\n [12.470342 ]\n [13.504501 ]\n [21.158422 ]\n [19.481928 ]\n [14.464958 ]\n [18.689196 ]\n [17.086664 ]\n [13.726393 ]\n [15.304079 ]\n [11.08413  ]\n [13.761087 ]\n [14.828028 ]\n [15.371039 ]\n [16.490788 ]\n [10.656013 ]\n [10.45309  ]\n [ 7.2233086]\n [14.785429 ]\n [15.426215 ]\n [19.492191 ]\n [17.086664 ]\n [14.85886  ]\n [21.747429 ]\n [16.745245 ]\n [ 7.4142265]\n [15.696114 ]\n [11.958686 ]\n [12.151129 ]\n [15.436055 ]\n [21.930231 ]\n [14.09049  ]\n [10.919114 ]\n [17.447714 ]\n [18.012506 ]\n [16.0523   ]\n [19.074392 ]\n [11.810111 ]\n [11.655943 ]\n [19.707245 ]\n [16.074724 ]\n [13.028504 ]\n [14.034069 ]\n [15.194787 ]\n [15.306547 ]\n [10.861692 ]\n [16.910172 ]\n [15.148046 ]\n [16.13391  ]\n [12.61786  ]\n [ 3.6321514]\n [15.377144 ]\n [15.955441 ]\n [18.699972 ]\n [17.96946  ]\n [14.574456 ]\n [15.834503 ]\n [17.162876 ]\n [16.88849  ]\n [11.556665 ]\n [11.9736805]\n [15.966412 ]\n [14.800227 ]\n [13.017525 ]\n [18.35541  ]\n [17.06752  ]\n [26.33951  ]\n [14.529284 ]\n [15.059717 ]\n [14.088218 ]\n [11.567842 ]\n [18.110933 ]\n [10.698233 ]\n [17.005857 ]\n [17.767591 ]\n [16.491777 ]\n [15.00717  ]\n [15.107434 ]\n [14.760982 ]\n [13.535226 ]\n [15.163439 ]\n [20.760832 ]\n [18.619654 ]\n [ 8.28673  ]\n [13.310775 ]\n [12.528671 ]\n [14.985364 ]\n [16.524738 ]\n [19.418007 ]\n [12.9853735]\n [15.026824 ]\n [14.188272 ]\n [16.177908 ]\n [15.660451 ]\n [17.173094 ]\n [20.05263  ]\n [16.412945 ]\n [17.519423 ]\n [14.199505 ]\n [14.83567  ]\n [14.268288 ]\n [17.078447 ]\n [23.508812 ]\n [15.334479 ]\n [15.878894 ]\n [16.85684  ]\n [15.47673  ]\n [19.145296 ]\n [16.722057 ]\n [20.15866  ]\n [16.45229  ]\n [22.413477 ]]", "q_t_selected": "[16.356745  15.3237095 14.304459  10.246355  13.588667  14.620359\n 17.752363  13.904019  11.597382  15.799087  10.366158  19.539665\n 18.149662  13.079149  14.147118   9.238508  15.650451  12.345778\n 16.0523    13.030964  17.378716  19.328777  15.150155  19.031958\n 16.150225  17.586021  14.841913  16.346344  15.938532   7.0846434\n 10.80579   15.451792  19.651058  15.27137   20.169231  13.141767\n 16.78214   17.781225  14.080586  15.11332   15.749163  14.141905\n 16.292158  16.73947   13.033565  13.700251  16.575611   8.592604\n 21.567299  11.656136  24.085604  13.10084   15.223464  13.57996\n 21.04424   14.360536  13.238564  14.348597   6.8195477 16.714357\n 13.438246  13.490963  16.52389   18.953104  10.418812  14.01239\n 15.551601  16.331553  11.782188  17.7432    11.419907  12.869212\n 16.816296  22.18056   13.761436  14.01009    9.806859  15.857252\n 15.233466  14.406906  15.401153  13.975659  15.675077  13.918324\n 11.946463  12.545955  17.529234  12.934405  12.833232  15.5154705\n 16.688272  15.942369  17.127424  17.405807   9.967167  11.153131\n 16.696157  18.68382   16.778137  16.807016  15.352862  18.309996\n 16.49597   11.861127  22.371075  12.715906  15.6307125 13.308146\n 13.533214  13.97933   14.543126  18.30433   15.419934  14.897022\n 17.535261  15.799273  16.589727  18.660152  12.120033  17.022955\n 16.403309  16.30666   16.579403  12.241897  12.963132  14.128008\n 12.788522  13.257464  14.193667  14.503722   6.52774   14.365608\n 15.158313  15.677931  15.339976  14.8869095 17.034775  18.886467\n 17.371456  19.235085  15.864772  16.52271    7.826096  15.446707\n 12.976635  12.470342  13.504501  21.158422  19.481928  14.464958\n 18.689196  17.086664  13.726393  15.304079  11.08413   13.761087\n 14.828028  15.371039  16.490788  10.656013  10.45309    7.2233086\n 14.785429  15.426215  19.492191  17.086664  14.85886   21.747429\n 16.745245   7.4142265 15.696114  11.958686  12.151129  15.436055\n 21.930231  14.09049   10.919114  17.447714  18.012506  16.0523\n 19.074392  11.810111  11.655943  19.707245  16.074724  13.028504\n 14.034069  15.194787  15.306547  10.861692  16.910172  15.148046\n 16.13391   12.61786    3.6321514 15.377144  15.955441  18.699972\n 17.96946   14.574456  15.834503  17.162876  16.88849   11.556665\n 11.9736805 15.966412  14.800227  13.017525  18.35541   17.06752\n 26.33951   14.529284  15.059717  14.088218  11.567842  18.110933\n 10.698233  17.005857  17.767591  16.491777  15.00717   15.107434\n 14.760982  13.535226  15.163439  20.760832  18.619654   8.28673\n 13.310775  12.528671  14.985364  16.524738  19.418007  12.9853735\n 15.026824  14.188272  16.177908  15.660451  17.173094  20.05263\n 16.412945  17.519423  14.199505  14.83567   14.268288  17.078447\n 23.508812  15.334479  15.878894  16.85684   15.47673   19.145296\n 16.722057  20.15866   16.45229   22.413477 ]", "twin_q_t_selected": "[15.897186  14.488044  14.345862  10.643096  13.798412  14.704438\n 16.63094   14.265562  11.521296  14.580762  10.787732  20.437252\n 17.881699  14.472456  13.115123   8.656168  15.295905  12.181575\n 15.370803  11.9495125 18.217218  18.71083   15.645848  19.762161\n 16.439728  15.822627  14.223028  16.778376  15.897515   7.09874\n 10.433355  16.225887  19.687235  14.633658  20.258276  12.9711485\n 16.285791  17.862137  14.139168  14.774422  15.901823  14.657041\n 16.056004  17.050968  13.037285  13.562462  16.612328   8.615395\n 20.997988  12.8801155 23.001108  13.438935  14.859288  12.626785\n 19.739767  14.185254  13.3192    14.062415   7.2446265 16.081718\n 14.048361  13.858045  16.35996   18.86229   10.629326  14.380942\n 16.236397  16.616213  12.139484  17.731268  11.979051  12.918361\n 16.53944   23.33854   13.09324   14.3670635  8.802294  15.8324375\n 15.126892  13.196486  15.54802   13.84849   15.934133  13.842277\n 12.936942  12.141169  17.810806  12.8993635 13.610974  15.967331\n 16.75086   17.241852  18.032745  18.17424   10.67252   10.756511\n 17.343723  18.32872   16.426067  17.02294   16.037859  18.030737\n 16.837774  12.465415  22.58663   12.581222  15.248185  12.489459\n 13.037533  13.822961  15.11647   17.160349  15.032346  15.855655\n 17.868177  15.547642  16.966312  17.903214  12.434853  16.451994\n 15.9778595 15.9518385 17.30647   12.134645  12.872479  15.178899\n 13.217164  13.667572  13.987354  13.915185   5.835457  13.940721\n 15.538939  16.019247  14.726361  14.368219  16.48001   18.607718\n 16.277662  18.919691  16.983965  16.056604   8.315854  14.959508\n 12.407791  13.972834  13.254641  21.169203  20.643047  14.649013\n 18.719622  16.723291  13.730123  15.605833  10.65852   13.739517\n 14.993964  14.918931  17.156017   9.97538    9.499209   6.8056197\n 15.464862  15.410805  19.793186  16.723291  14.869411  21.148043\n 16.789747   7.2463794 16.404377  10.653437  12.275724  15.945008\n 21.337389  13.092901  12.142233  16.877584  17.634256  15.370803\n 19.74679   11.951852  11.285169  19.891405  16.355267  13.217812\n 14.675327  15.2752075 14.926973  11.440685  17.925165  15.085328\n 16.587954  13.229952   3.1173198 15.328633  15.463557  18.706387\n 18.449177  14.145293  15.6756315 16.589342  17.76062   12.201694\n 13.164979  16.387913  15.163269  12.181722  19.427807  17.194355\n 24.193655  13.583859  15.927634  12.75672   11.446839  17.83943\n 11.058666  16.66037   18.118095  16.11261   15.450097  14.317497\n 14.445258  12.719175  15.670247  21.968164  18.948374   8.534188\n 13.358122  13.809789  15.107744  16.73006   19.327854  12.972964\n 15.438077  13.014175  15.2165165 16.669022  16.879995  19.253174\n 15.469633  17.210218  13.543283  14.568162  13.672855  17.588589\n 22.908596  15.1386385 16.096489  17.232668  15.176084  19.2085\n 16.04756   20.15694   16.803041  22.47923  ]", "q_t_selected_target": "[16.498217  14.035801  14.230918   9.404263  14.480063  13.492047\n 17.477253  15.005755  12.654293  14.657232  11.404109  18.85925\n 16.599451  13.67849   14.945847   9.24011   15.62512   13.46514\n 14.88759   12.269136  17.174068  20.303978  16.069073  18.48498\n 14.433899  15.235731  14.103885  18.829205  15.3350935  7.826821\n 12.372751  14.792287  20.241646  15.428442  19.024548  13.45391\n 16.401644  15.334931  15.374116  13.410614  14.355835  16.10404\n 16.741968  15.265799  11.88814   14.494016  14.581173   7.6963115\n 19.628029  14.233858  23.834682  15.146387  15.785061  14.061063\n 21.032732  15.64239   13.7545805 13.641724   7.9177623 17.295113\n 14.873582  12.204932  14.492661  18.060827  11.480598  15.315404\n 15.282338  14.991147  14.483767  16.972397  11.649572  13.173048\n 14.738539  22.23229   14.717262  12.453163   7.2577286 16.660961\n 16.32465   13.145275  16.997084  14.099242  16.96995   13.649992\n 12.9983225 11.20129   15.567703  14.942549  12.619512  15.261914\n 16.5596    16.156055  16.11336   17.188778   8.938923   9.923679\n 17.391178  19.504026  16.692535  16.296879  15.166663  18.850021\n 17.548483  13.040442  22.706278  12.035488  16.264278  13.829392\n 14.187089  15.004261  14.225588  17.539032  15.096648  16.365053\n 17.457808  16.101364  17.702969  16.30145   13.114772  17.421118\n 15.491386  16.367609  16.055964  11.548191  13.735263  13.793576\n 12.927273  14.192433  13.201245  13.094355   5.411886  14.273636\n 14.74257   17.395658  13.568892  13.409751  15.707065  19.988058\n 18.489239  20.450153  14.773551  15.652788   8.029097  14.431511\n 11.415237  15.119156  12.811712  22.369606  19.48695   13.945208\n 17.757603  17.70821   13.974527  13.174053  11.60816   13.903126\n 14.694314  14.648607  18.106236   9.923453   6.8721347  7.068089\n 14.890669  15.307418  19.433077  17.671158  14.727736  22.671947\n 17.62207    8.937295  15.814795  11.878202  12.437168  16.68286\n 21.918577  13.790404  10.973869  19.480488  15.932233  15.022555\n 18.53898    9.883455  13.488303  20.752205  15.320322  13.8416195\n 12.499765  15.709792  14.136155  10.9947195 17.434267  14.652492\n 15.042773  15.362338   1.2772386 16.57054   15.5483265 18.002954\n 17.563208  14.229785  15.324318  15.947101  16.922895  11.150172\n 12.553823  13.855858  14.175215  12.692572  18.375563  16.2603\n 23.594942  14.018621  16.559183  15.181672   8.246601  16.963276\n 12.143842  18.246094  18.750605  15.549614  14.708123  15.077766\n 14.145592  13.75814   14.747808  20.111217  19.665539   7.457403\n 14.289439  12.139081  13.523556  17.024279  19.29442   14.590526\n 15.173205  13.13951   14.866007  16.158875  18.482874  20.826105\n 14.221019  17.738575  14.921715  15.761202  14.495206  18.056849\n 23.92591   15.273827  16.845594  15.653519  15.25877   18.944471\n 13.742268  20.063583  16.559828  24.964642 ]", "q_tp1_best_masked": "[15.820292  13.9816885 14.886268  11.916835  15.161927  14.511939\n 17.74248   15.1922455 13.747681  14.830288  11.91995   18.886007\n 16.126667  14.072492  16.502802  12.305934  16.219406  14.023943\n 15.70978   13.693132  17.794928  20.930542  17.449364  18.193546\n 14.534458  14.851296  13.639051  18.736338  15.395392   9.143695\n 13.398145  14.729125  20.202759  15.247695  19.081997  14.3882\n 15.8788595 15.817876  15.544254  14.142289  14.861074  16.2198\n 17.39695   15.12566   13.2470255 15.800082  15.000153   9.243333\n 18.141079  14.928663  22.06694   16.224892  16.14251   14.436764\n 21.515297  17.008692  14.476341  14.18393    9.611256  17.985493\n 16.089607  12.95251   14.399284  18.185282  12.15903   16.07483\n 15.878887  15.474484  14.429615  16.281961  13.064614  14.6155615\n 15.137817  21.008137  16.517591  12.637403   8.835857  16.321472\n 16.458641  12.999221  17.197912  14.760246  17.42486   14.19482\n 15.916776  11.826568  16.041847  15.08457   13.253768  14.195363\n 16.291525  15.602589  16.672205  16.384085  11.22067   10.978706\n 17.238783  18.777714  17.161053  16.780697  14.920046  19.357687\n 17.888823  13.884416  22.07561   14.131749  18.444118  14.451101\n 15.862513  14.972611  14.536816  18.785812  15.215764  15.559737\n 16.731026  16.78863   17.752825  16.336916  14.218282  17.828852\n 15.394641  17.019264  15.46736   12.294447  14.518157  14.315789\n 14.341556  14.983207  14.757163  13.034052   8.146558  14.926191\n 14.296885  17.420881  13.817625  13.890075  15.521731  19.17562\n 17.924223  21.027431  16.159304  16.20734    9.923626  13.998831\n 13.450362  16.24939   13.506703  23.404036  19.75452   15.143003\n 18.65898   17.738113  15.126917  12.749921  13.145448  14.7938175\n 15.047461  14.697365  18.208633  11.398402   8.617472   7.985613\n 14.880475  15.8120575 20.383121  17.700686  14.849814  22.621801\n 16.88999   10.6988125 15.779862  13.227283  14.991236  18.14353\n 20.580133  15.779299  12.964483  20.65712   16.34756   15.8461075\n 18.169235  11.112316  15.098351  20.091574  15.781608  14.289015\n 13.400338  15.533711  14.45357   12.033131  17.57322   15.937795\n 14.766551  16.173088   4.6289644 17.619822  15.56241   18.512619\n 17.587976  14.581684  16.011366  16.175537  16.111547  13.569663\n 12.969609  14.9565935 14.6960945 13.4928875 17.584536  16.328493\n 22.71362   14.886047  16.651644  14.39581   11.142202  17.452726\n 13.228287  18.838001  18.314583  15.076698  13.965199  15.586132\n 15.417607  14.861452  15.0947695 19.877728  19.261263   8.650701\n 15.002156  12.320601  13.812088  17.017939  19.107874  15.628138\n 15.38795   15.090395  15.870997  17.243683  19.51914   20.561937\n 14.441117  17.563757  15.495152  15.982419  14.970346  18.243017\n 23.604027  16.089518  18.953218  16.12065   15.42415   18.691477\n 14.395742  19.830845  16.652933  24.333109 ]", "policy_t": "[[ 0.26855397  0.63404155 -0.5917769   0.62794125  0.57854724  0.89222   ]\n [ 0.794104   -0.6162888   0.73059225  0.56535363  0.7945436   0.62016606]\n [-0.65121526  0.33929253  0.78401136  0.50721264 -0.6632748   0.4889953 ]\n ...\n [ 0.80262804  0.92605686 -0.5069666   0.8672826   0.9074781   0.8091084 ]\n [ 0.2760427   0.8663964   0.36931586  0.720876    0.41615832  0.8888087 ]\n [ 0.6905112   0.37742472 -0.9575013  -0.95807946  0.1832155   0.23757422]]", "td_error": "[0.3712511  0.8700757  0.09424257 1.040463   0.7865238  1.1703515\n 0.56071186 0.9209647  1.0949545  0.60916233 0.8271642  1.1292095\n 1.4162292  0.69665337 1.3147264  0.2927723  0.1772728  1.2014637\n 0.8239608  0.5407257  0.6238985  1.284174   0.67107105 0.9120798\n 1.8610773  1.4685931  0.428586   2.2668447  0.5829301  0.7351291\n 1.7531786  1.0465527  0.5724993  0.47592783 1.1892061  0.39745235\n 0.24817467 2.4867496  1.2642388  1.533257   1.4696579  1.7045674\n 0.5678873  1.6294203  1.147285   0.86265945 2.0127964  0.90768766\n 1.6546144  1.9657326  0.5422478  1.8764997  0.74368477 0.95769024\n 0.65223694 1.3694954  0.47569895 0.5637822  0.8856752  0.8970747\n 1.1302786  1.4695716  1.9492636  0.84686947 0.9565296  1.1187377\n 0.61166096 1.4827356  2.5229301  0.76483727 0.279572   0.2792616\n 1.9393291  0.57899    1.2899241  1.7354136  2.0468478  0.81611633\n 1.1444707  0.6564212  1.5224972  0.18716764 1.1653447  0.23030853\n 0.5566201  1.1422715  2.1023169  2.0256643  0.6025915  0.47948647\n 0.15996552 0.6497412  1.4667253  0.6012459  1.3809204  1.0311413\n 0.37123775 0.997756   0.17603493 0.6180992  0.5286975  0.6796551\n 0.8816109  0.8771715  0.22742558 0.61307573 0.8248296  0.9305897\n 0.90171576 1.1031156  0.6042104  0.57199097 0.19379425 0.9887147\n 0.24391174 0.427907   0.9249487  1.9802322  0.8373289  0.68364334\n 0.69919777 0.23835993 0.8869734  0.64008    0.8174572  0.8598771\n 0.21432114 0.72991514 0.88926554 1.115099   0.7697122  0.21244383\n 0.6060562  1.5470695  1.4642773  1.2178135  1.0503273  1.2409658\n 1.6646795  1.3727655  1.6508174  0.63686943 0.244879   0.77159643\n 1.2769756  1.8975682  0.5678587  1.2057934  0.58055973 0.6117778\n 0.94680595 0.8032322  0.2462697  2.2809029  0.736835   0.15282345\n 0.21668196 0.49637794 1.2828331  0.3922429  3.1040149  0.20884442\n 0.33971643 0.11109209 0.2096119  0.76618004 0.13639927 1.2242117\n 0.8545742  1.606992   0.3541317  0.6526246  0.22374153 0.9923277\n 0.29642105 0.49879456 0.6115594  2.3178387  1.8911486  0.68899584\n 0.87161064 1.9975262  2.0177474  0.9528799  0.89467335 0.7184615\n 1.8549328  0.47479486 0.9806051  0.28949642 0.50749683 0.46419525\n 1.3181581  2.4384322  2.097497   1.2176509  0.24594212 0.7002249\n 0.64611053 0.21458149 0.43074942 0.9290085  0.43606472 0.72900724\n 0.59564924 2.3213043  0.80653334 0.41790152 0.5361986  0.8706379\n 1.6716404  0.47271204 1.0655074  1.7592034  3.2607393  1.0119057\n 1.2653923  1.4129801  0.8077612  0.7525797  0.5205102  0.3949685\n 0.4575281  0.630939   0.6690345  1.2532806  0.88152504 0.95305586\n 0.95499086 1.030149   1.5229983  0.3968792  0.07851028 1.6113567\n 0.20562649 0.58704853 0.83120537 0.50428534 1.4563293  1.1732035\n 1.7202702  0.3737545  1.0503206  1.0592856  0.52463436 0.7233305\n 0.71720695 0.09792042 0.857903   1.3912354  0.15032339 0.23242664\n 2.642541   0.0942173  0.17537594 2.5182877 ]", "mean_td_error": 0.9451675415039062, "actor_loss": -15.888824462890625, "critic_loss": 0.6653160452842712, "alpha_loss": -16.05435562133789, "alpha_value": 0.1669720560312271, "target_entropy": -6, "mean_q": 15.228126525878906, "max_q": 26.339509963989258, "min_q": 3.6321513652801514, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 16000, "episodes_total": 16, "training_iteration": 16, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_14-51-14", "timestamp": 1587048674, "time_this_iter_s": 86.22034764289856, "time_total_s": 546.704460144043, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 546.704460144043, "timesteps_since_restore": 16000, "iterations_since_restore": 16, "perf": {"cpu_util_percent": 92.15384615384616, "ram_util_percent": 11.299999999999997}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -160.09652358044056, "episode_reward_min": -255.76840451887534, "episode_reward_mean": -210.33551728855727, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-182.6650687959388, -248.35699008922933, -248.8857484607318, -186.02373525600072, -173.12516805337748, -160.09652358044056, -255.76840451887534, -222.95778364247928, -218.29449950583293, -207.1812509826668], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26380872652829146, "mean_processing_ms": 0.12009627413707524, "mean_inference_ms": 1.1764802790940692}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -56.48484841707745, "episode_reward_min": -229.43520271527103, "episode_reward_mean": -180.25024074477724, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-229.43520271527103, -200.0574765235254, -194.36138843836824, -56.48484841707745, -220.91228762964403], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.3096081379913497, "mean_processing_ms": 0.42626010135294123, "mean_inference_ms": 1.4870268023904223}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 8500, "num_steps_trained": 1790464, "num_steps_sampled": 17000, "sample_time_ms": 2.844, "replay_time_ms": 20.843, "grad_time_ms": 50.997, "update_time_ms": 0.005, "opt_peak_throughput": 5019.904, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[12.025241 ]\n [18.280224 ]\n [15.272815 ]\n [22.809664 ]\n [15.17166  ]\n [14.242865 ]\n [10.641097 ]\n [17.732199 ]\n [12.510467 ]\n [16.928028 ]\n [13.759539 ]\n [10.541992 ]\n [15.078646 ]\n [14.881937 ]\n [15.544421 ]\n [13.63605  ]\n [15.013704 ]\n [14.150354 ]\n [21.925625 ]\n [10.031741 ]\n [13.838294 ]\n [13.030319 ]\n [ 6.0211077]\n [15.509834 ]\n [13.636419 ]\n [15.192576 ]\n [20.235556 ]\n [12.442052 ]\n [11.9647875]\n [19.033592 ]\n [ 9.692037 ]\n [15.534887 ]\n [15.679194 ]\n [18.031748 ]\n [ 7.2884607]\n [11.54439  ]\n [13.717145 ]\n [17.040194 ]\n [20.345764 ]\n [11.878102 ]\n [16.713358 ]\n [ 6.066406 ]\n [18.632837 ]\n [15.188288 ]\n [17.771158 ]\n [20.302128 ]\n [19.106194 ]\n [20.401518 ]\n [14.244466 ]\n [13.452    ]\n [13.012844 ]\n [19.141449 ]\n [10.177329 ]\n [15.407359 ]\n [22.131538 ]\n [20.834831 ]\n [15.357592 ]\n [14.981268 ]\n [12.195104 ]\n [18.406286 ]\n [14.3569145]\n [14.722422 ]\n [13.836355 ]\n [14.009843 ]\n [15.969605 ]\n [14.370583 ]\n [17.896185 ]\n [17.972889 ]\n [16.14187  ]\n [17.670061 ]\n [15.459398 ]\n [13.456108 ]\n [15.178663 ]\n [16.43033  ]\n [15.788789 ]\n [ 6.2974944]\n [19.294233 ]\n [15.462642 ]\n [12.831154 ]\n [18.598661 ]\n [14.378448 ]\n [20.993315 ]\n [13.973717 ]\n [19.688765 ]\n [13.872027 ]\n [16.85679  ]\n [13.720181 ]\n [21.728231 ]\n [15.112334 ]\n [15.592196 ]\n [ 8.961225 ]\n [20.333256 ]\n [13.127431 ]\n [14.609009 ]\n [14.386375 ]\n [16.291851 ]\n [13.516685 ]\n [11.876505 ]\n [14.999318 ]\n [17.553558 ]\n [15.43197  ]\n [16.350315 ]\n [21.060833 ]\n [11.896053 ]\n [18.286005 ]\n [17.528101 ]\n [14.99778  ]\n [13.832403 ]\n [16.456934 ]\n [18.067732 ]\n [13.521539 ]\n [13.561882 ]\n [12.4131365]\n [13.862933 ]\n [14.949336 ]\n [13.288529 ]\n [14.753315 ]\n [13.766951 ]\n [19.791666 ]\n [13.1482   ]\n [15.03308  ]\n [14.441609 ]\n [14.406033 ]\n [16.035112 ]\n [14.958657 ]\n [ 6.757048 ]\n [17.282356 ]\n [12.18837  ]\n [15.300478 ]\n [ 8.024962 ]\n [13.694565 ]\n [14.011468 ]\n [22.81991  ]\n [17.70239  ]\n [ 9.628412 ]\n [13.806075 ]\n [15.691471 ]\n [15.579489 ]\n [15.6340885]\n [13.379805 ]\n [14.399142 ]\n [ 9.903995 ]\n [13.962301 ]\n [13.187886 ]\n [15.741885 ]\n [22.039518 ]\n [15.40776  ]\n [17.695478 ]\n [14.975582 ]\n [14.370791 ]\n [15.228083 ]\n [17.155272 ]\n [17.468664 ]\n [10.091101 ]\n [16.842373 ]\n [14.857907 ]\n [15.16936  ]\n [12.994112 ]\n [24.698822 ]\n [15.919396 ]\n [17.115675 ]\n [19.980562 ]\n [ 5.8576403]\n [14.85554  ]\n [15.710568 ]\n [20.321123 ]\n [12.7480545]\n [17.419863 ]\n [14.623294 ]\n [12.629896 ]\n [15.403606 ]\n [12.169746 ]\n [ 9.292588 ]\n [ 9.5558   ]\n [13.233257 ]\n [11.520964 ]\n [14.441858 ]\n [14.756402 ]\n [15.425508 ]\n [24.223272 ]\n [15.535882 ]\n [14.444899 ]\n [16.737343 ]\n [15.721028 ]\n [13.583922 ]\n [16.462147 ]\n [14.261011 ]\n [14.866579 ]\n [17.528439 ]\n [16.957436 ]\n [15.974723 ]\n [14.687478 ]\n [15.676236 ]\n [19.618916 ]\n [14.814237 ]\n [15.413686 ]\n [18.677994 ]\n [14.687378 ]\n [18.93163  ]\n [14.045956 ]\n [15.512857 ]\n [12.28421  ]\n [16.42271  ]\n [13.814184 ]\n [13.706108 ]\n [15.786066 ]\n [16.173569 ]\n [12.86335  ]\n [14.485206 ]\n [14.556792 ]\n [13.150841 ]\n [16.668642 ]\n [14.418774 ]\n [15.296839 ]\n [15.928109 ]\n [14.6580515]\n [15.723583 ]\n [10.125673 ]\n [16.647078 ]\n [12.1231785]\n [13.465293 ]\n [20.292822 ]\n [17.096148 ]\n [13.364356 ]\n [13.914458 ]\n [15.124706 ]\n [11.630305 ]\n [16.216505 ]\n [15.609124 ]\n [12.790046 ]\n [15.804347 ]\n [18.044914 ]\n [15.9138365]\n [25.674105 ]\n [19.888954 ]\n [17.612963 ]\n [ 7.5042486]\n [14.150992 ]\n [14.164153 ]\n [15.780636 ]\n [13.114561 ]\n [15.521826 ]\n [12.897545 ]\n [19.118029 ]\n [14.643714 ]\n [21.367863 ]\n [17.931221 ]\n [24.416613 ]\n [19.961266 ]\n [15.848761 ]\n [16.115074 ]\n [13.95697  ]\n [13.565198 ]\n [19.404957 ]\n [15.364653 ]\n [19.296886 ]]", "q_t_selected": "[12.025241  18.280224  15.272815  22.809664  15.17166   14.242865\n 10.641097  17.732199  12.510467  16.928028  13.759539  10.541992\n 15.078646  14.881937  15.544421  13.63605   15.013704  14.150354\n 21.925625  10.031741  13.838294  13.030319   6.0211077 15.509834\n 13.636419  15.192576  20.235556  12.442052  11.9647875 19.033592\n  9.692037  15.534887  15.679194  18.031748   7.2884607 11.54439\n 13.717145  17.040194  20.345764  11.878102  16.713358   6.066406\n 18.632837  15.188288  17.771158  20.302128  19.106194  20.401518\n 14.244466  13.452     13.012844  19.141449  10.177329  15.407359\n 22.131538  20.834831  15.357592  14.981268  12.195104  18.406286\n 14.3569145 14.722422  13.836355  14.009843  15.969605  14.370583\n 17.896185  17.972889  16.14187   17.670061  15.459398  13.456108\n 15.178663  16.43033   15.788789   6.2974944 19.294233  15.462642\n 12.831154  18.598661  14.378448  20.993315  13.973717  19.688765\n 13.872027  16.85679   13.720181  21.728231  15.112334  15.592196\n  8.961225  20.333256  13.127431  14.609009  14.386375  16.291851\n 13.516685  11.876505  14.999318  17.553558  15.43197   16.350315\n 21.060833  11.896053  18.286005  17.528101  14.99778   13.832403\n 16.456934  18.067732  13.521539  13.561882  12.4131365 13.862933\n 14.949336  13.288529  14.753315  13.766951  19.791666  13.1482\n 15.03308   14.441609  14.406033  16.035112  14.958657   6.757048\n 17.282356  12.18837   15.300478   8.024962  13.694565  14.011468\n 22.81991   17.70239    9.628412  13.806075  15.691471  15.579489\n 15.6340885 13.379805  14.399142   9.903995  13.962301  13.187886\n 15.741885  22.039518  15.40776   17.695478  14.975582  14.370791\n 15.228083  17.155272  17.468664  10.091101  16.842373  14.857907\n 15.16936   12.994112  24.698822  15.919396  17.115675  19.980562\n  5.8576403 14.85554   15.710568  20.321123  12.7480545 17.419863\n 14.623294  12.629896  15.403606  12.169746   9.292588   9.5558\n 13.233257  11.520964  14.441858  14.756402  15.425508  24.223272\n 15.535882  14.444899  16.737343  15.721028  13.583922  16.462147\n 14.261011  14.866579  17.528439  16.957436  15.974723  14.687478\n 15.676236  19.618916  14.814237  15.413686  18.677994  14.687378\n 18.93163   14.045956  15.512857  12.28421   16.42271   13.814184\n 13.706108  15.786066  16.173569  12.86335   14.485206  14.556792\n 13.150841  16.668642  14.418774  15.296839  15.928109  14.6580515\n 15.723583  10.125673  16.647078  12.1231785 13.465293  20.292822\n 17.096148  13.364356  13.914458  15.124706  11.630305  16.216505\n 15.609124  12.790046  15.804347  18.044914  15.9138365 25.674105\n 19.888954  17.612963   7.5042486 14.150992  14.164153  15.780636\n 13.114561  15.521826  12.897545  19.118029  14.643714  21.367863\n 17.931221  24.416613  19.961266  15.848761  16.115074  13.95697\n 13.565198  19.404957  15.364653  19.296886 ]", "twin_q_t_selected": "[12.869052  18.674528  15.369375  22.785402  14.807176  14.765965\n 11.464865  18.46919   12.76191   16.715164  14.35134   10.884772\n 15.888586  14.378696  15.6837    13.612279  14.152236  14.385122\n 21.415422  11.194375  14.528893  13.195512   5.738125  15.408593\n 13.237403  15.478105  19.749977  13.913168  13.72803   20.103659\n 10.876576  14.493779  16.503736  18.651217   7.872188  11.83793\n 13.332981  17.441042  20.546333  11.459576  17.144058   6.370776\n 19.207918  14.921059  18.45479   19.546951  20.368572  20.658525\n 14.92214   13.289202  12.550826  20.290934   9.072313  15.801835\n 22.478441  19.853312  15.149952  13.622182  11.522396  19.82364\n 13.819293  15.526436  12.727465  15.319997  16.288822  15.224622\n 16.279722  17.614952  15.855798  18.21298   15.937142  13.858612\n 14.7596855 15.380749  16.153763   5.723667  18.446095  14.7414665\n 12.608335  18.624235  14.614225  20.97863   13.7259865 19.457655\n 14.757759  17.498552  13.779448  20.67838   14.795966  14.89305\n  9.24528   20.051403  12.579244  13.370107  14.889918  14.931986\n 13.468631  10.387929  15.709403  16.961061  16.193327  15.590047\n 19.88141   11.720231  18.012579  18.06929   15.003391  14.171605\n 15.698909  17.479153  12.743857  13.495449  13.457017  14.464395\n 14.38477   13.035377  14.627802  13.64613   20.48468   12.972142\n 14.969555  14.228918  14.603828  16.144659  14.662038   6.522154\n 17.243032  11.13551   15.675272   8.113367  13.860428  12.9797535\n 22.267788  17.74004    9.919916  13.646076  15.527714  15.79041\n 15.6262    13.636828  15.190498   8.890074  14.9642725 12.847224\n 15.602518  21.53746   16.18681   18.065897  14.748514  14.896203\n 15.417324  16.766546  17.277311  10.468213  17.216719  14.89514\n 15.850796  13.035941  23.747936  15.740076  16.797138  18.043549\n  4.8202767 13.777447  16.180235  19.573769  13.554649  17.4538\n 14.317258  12.08032   15.894155  12.651119   9.555482   9.390905\n 13.273634  11.647829  14.342049  14.511415  16.046846  24.298265\n 15.583531  14.215765  16.064661  15.853057  13.790226  16.725895\n 15.217574  14.660817  18.536438  16.218075  17.642054  15.076157\n 15.66087   17.975288  15.02278   14.869329  18.671692  14.323241\n 18.020502  13.968737  14.576545  13.398457  16.269941  13.52111\n 13.113505  15.274361  15.784071  12.181763  14.295541  13.878133\n 13.759509  16.106823  13.947101  14.889429  15.914805  14.341852\n 16.474712   9.839437  16.844147  11.597499  12.906145  20.058153\n 17.63643   13.137479  14.234807  15.561069  12.657579  14.85388\n 16.104677  13.734515  16.233837  17.23478   15.477145  25.664309\n 20.397657  16.65136    6.9085636 14.650162  14.105719  16.271423\n 13.16368   14.315526  13.553341  18.914524  13.703005  20.425856\n 17.820509  24.012201  19.51137   15.44869   16.28529   14.225683\n 13.374237  19.793592  16.272337  19.07026  ]", "q_t_selected_target": "[13.482748  17.144203  14.237112  24.093489  12.163308  14.740333\n 10.831131  20.674248  11.452015  16.427425  13.828051  10.527313\n 14.089116  12.673359  17.238415  12.360283  15.459269  14.949079\n 23.37041   11.152834  15.781553  14.0463295  7.252328  16.746265\n 12.660473  15.954427  19.948267  13.972682  12.433662  20.556643\n  9.198989  15.133869  16.810005  18.429731   6.1873207 11.868104\n 13.753099  15.954396  19.781118   9.758785  17.58364    7.5666704\n 19.511583  14.984954  18.006401  22.006817  18.471006  19.671623\n 14.716594  12.50939   13.601572  18.033918  10.846052  15.119722\n 23.434542  21.163475  16.171135  15.106155  12.232133  18.747746\n 14.9386635 15.457612  13.036173  14.980961  16.838568  14.462724\n 16.319035  18.471804  14.759656  17.316332  15.311696  13.760271\n 14.561069  16.051037  16.942856   4.0948772 17.525232  15.995196\n 12.628011  17.094265  14.371488  21.549368  13.276139  18.974031\n 15.06104   17.470915  13.314014  20.740833  16.377134  15.345321\n  7.039895  19.660227  14.55071   14.8936205 15.486597  17.048119\n 14.672287  11.510352  13.669922  17.342003  14.752694  14.697489\n 22.591717  13.632189  16.966263  17.030443  14.160171  14.632938\n 17.241018  18.754503  14.173958  12.917267  12.181553  13.580413\n 15.110235  11.955936  16.378454  14.342343  21.332418  12.058832\n 15.515653  15.727632  14.420008  16.115366  14.94982    5.392499\n 17.82454   10.568391  16.339643   8.113879  12.793424  14.733976\n 22.968872  17.527924   9.036218  13.5522995 15.406583  15.304816\n 13.447468  12.501527  14.409798  10.133963  13.80559   13.102667\n 15.611471  21.834919  14.98255   17.149199  12.9018135 14.261435\n 13.873057  14.196738  16.22303    9.257822  18.35994   15.571797\n 16.26839   13.725018  25.416433  14.601163  18.292152  18.422352\n  3.2263258 14.552315  15.897055  18.519665  13.792147  16.420698\n 13.9179325 12.024577  14.607182  11.5195265 10.005414   9.456434\n 13.215524  13.241559  15.444903  16.413187  13.360081  26.12636\n 15.37019   14.72418   17.924065  14.270816  14.198122  16.595026\n 14.296523  13.887057  20.084846  17.62319   18.063496  15.276845\n 15.382023  19.382671  14.925357  15.1146145 19.602098  15.250569\n 19.666395  14.763997  14.521822  13.303637  17.211073  13.14909\n 13.315323  12.67302   16.321976  12.770769  14.036928  14.997585\n 12.424315  17.505907  15.147581  14.473742  16.967512  13.853931\n 16.253763   8.752691  17.813969  11.637514  13.435578  20.160912\n 17.771843  14.107514  13.705868  14.803551  12.758253  16.772713\n 16.20272   12.102825  15.308259  17.512497  15.698406  27.019905\n 18.14471   18.117195   5.5187187 16.092295  15.225991  16.147614\n 13.512185  15.507115  14.139499  22.64823   12.697707  21.667877\n 19.384432  26.28333   18.83078   15.264475  15.4491    14.400073\n 12.049472  20.783598  17.577024  21.64899  ]", "q_tp1_best_masked": "[13.981988  17.324741  14.283955  23.940931  12.213112  14.780409\n 12.426555  19.917364  12.806496  15.8747015 14.108767  12.452487\n 14.842825  13.901378  18.455545  12.445192  16.316368  15.148592\n 24.367445  12.564467  15.5107    15.228234   9.511575  17.149328\n 13.421388  15.925762  20.358913  14.532867  12.708821  21.161394\n 10.540911  15.258749  17.590942  18.507156   7.1411967 13.004106\n 14.885105  17.337181  19.536575  10.902837  17.143372  10.079295\n 18.907442  15.543684  18.53611   21.46652   18.690817  19.807117\n 14.942038  13.388966  13.953531  17.517946  12.37846   15.781367\n 23.419596  19.94938   16.522621  15.095054  13.029579  19.629362\n 16.026255  15.899189  12.873489  15.659424  16.075207  15.974841\n 15.812708  17.764307  14.571659  17.043526  16.207651  14.274702\n 14.7990885 18.042202  16.447004   6.1090302 16.783669  16.917362\n 13.601043  16.46084   14.866851  21.343966  13.285271  18.267653\n 14.97915   17.381601  13.914086  19.984936  17.627157  14.539446\n  8.855803  19.215254  16.19576   15.297698  15.459958  17.90612\n 14.842377  12.812152  13.8459215 17.17416   15.0316    15.549885\n 21.775146  14.303655  16.987646  16.768429  14.6383295 15.0619335\n 17.506031  18.761505  14.776204  14.575394  12.791888  13.667107\n 16.162788  13.055396  17.547735  14.863522  21.262602  12.582374\n 15.619215  16.531237  15.423371  17.43582   15.68373    8.024398\n 18.619188  12.731821  17.750753  10.242864  13.203117  15.247517\n 22.9851    18.862404   9.801901  13.402966  16.586231  15.881082\n 14.671328  13.938437  13.8272705 11.410462  14.351203  15.10933\n 15.673222  21.307724  15.330907  16.605112  13.997039  14.49581\n 15.257767  13.524922  16.54833   11.607562  18.213575  16.052818\n 16.369743  13.85346   23.823568  14.982739  19.065475  19.900314\n  5.3337517 14.629176  16.99925   17.624659  14.5788765 16.902227\n 14.41181   12.7779665 15.628769  12.13239   11.5627    11.280733\n 13.572801  14.34635   16.196772  16.761835  13.164882  25.11445\n 15.81117   16.01163   17.594023  14.599548  14.491965  16.418074\n 14.864507  13.890661  19.503664  18.086637  19.154274  16.409163\n 16.379026  21.042887  15.978481  16.351505  18.92462   14.907399\n 20.839495  14.861004  15.705972  13.886326  17.822327  13.822343\n 15.095545  13.301336  16.459005  14.6687155 14.2272215 16.146334\n 12.69851   18.082977  15.629312  15.028691  17.3754    13.417214\n 17.907928  10.160726  17.315708  12.958065  13.931654  19.98068\n 18.340815  14.936039  14.83753   15.455502  13.176104  17.315132\n 16.470333  13.289576  15.313956  18.34038   15.236173  26.272112\n 18.037395  17.914354   7.5501842 16.63279   15.069796  16.766745\n 14.12797   16.4666    14.256558  23.931376  13.51028   22.640495\n 20.943748  25.66512   18.886776  16.315353  16.437183  15.268008\n 13.482424  20.651054  18.116064  21.59735  ]", "policy_t": "[[-0.06156021 -0.905405    0.45817947  0.9907656   0.9615874  -0.38293153]\n [ 0.89471734  0.17708623  0.69564843  0.95246553 -0.5515667   0.9492153 ]\n [ 0.61743593 -0.33095467  0.57221746  0.44071507 -0.5853976   0.5073664 ]\n ...\n [ 0.21521997  0.7523298  -0.36496764  0.6877979   0.65855265 -0.34169477]\n [ 0.9638499   0.7642515   0.9016032  -0.63668877 -0.18512541  0.7577934 ]\n [-0.31197143  0.30803895 -0.07341874  0.33373284 -0.4308082   0.66106486]]", "td_error": "[1.0356016  1.3331728  1.083983   1.2959557  2.82611    0.26155043\n 0.41188383 2.573553   1.1841736  0.39417076 0.29590082 0.18606901\n 1.3944998  1.9569578  1.6243544  1.2638817  0.8762984  0.6813402\n 1.6998873  0.58131695 1.5979595  0.933414   1.3727117  1.2870517\n 0.77643824 0.61908627 0.24278927 0.7950721  0.88162136 0.9880171\n 1.0853176  0.52055407 0.7185397  0.30973434 1.3930037  0.17694426\n 0.2280364  1.2862215  0.66493034 1.9100537  0.65493107 1.3480794\n 0.5912056  0.13361454 0.34181595 2.0822773  1.2663765  0.85839844\n 0.33883715 0.8612108  0.81973696 1.6822729  1.221231   0.48487473\n 1.1295519  0.81940365 0.91736317 0.8044305  0.37338305 0.7086773\n 0.8505597  0.4020071  0.55444527 0.655077   0.7093539  0.4270196\n 0.80823135 0.67788315 1.2391782  0.6251888  0.38657427 0.20125198\n 0.40810585 0.52479076 0.97158    1.9157035  1.3449316  0.8931422\n 0.11140966 1.5171833  0.12484884 0.5633955  0.57371235 0.5991783\n 0.7461467  0.3208809  0.43580008 0.5249262  1.4229841  0.34957314\n 2.0633574  0.5321026  1.6973724  0.90406275 0.8484502  1.4362001\n 1.1796293  0.74428797 1.6844387  0.29624844 1.0599542  1.2726922\n 2.120595   1.8240466  1.1830292  0.7682524  0.840415   0.63093424\n 1.1630969  0.981061   1.0412598  0.6113987  0.7535238  0.583251\n 0.443182   1.2060165  1.6878958  0.6358032  1.1942453  1.001339\n 0.51433516 1.3923678  0.09889793 0.05477333 0.14830971 1.247102\n 0.5618448  1.0935493  0.8517685  0.04471445 0.9840727  1.2383657\n 0.42502308 0.19329166 0.7379465  0.17377615 0.2030096  0.38013315\n 2.1826763  1.0067897  0.39567804 0.73692846 0.6576972  0.170331\n 0.06968355 0.251029   0.81473494 0.7314892  1.9602346  0.37206268\n 1.449646   2.7641706  1.1499577  1.0218349  1.3303938  0.6952739\n 0.7583127  0.709991   1.1930542  1.2285733  1.3357458  0.9685068\n 2.1126328  0.53904676 0.23483324 1.4277811  0.64079475 1.0161333\n 0.55234337 0.33053112 1.0416989  0.89090633 0.58137894 0.08244753\n 0.03792191 1.6571627  1.0529499  1.7792788  2.3760962  1.865591\n 0.18951702 0.39384842 1.5230627  1.5162268  0.51104784 0.13187408\n 0.4782815  0.8766408  2.0524082  1.0354347  1.2551074  0.39502764\n 0.28653002 0.8218136  0.10427189 0.27217817 0.92725563 0.74525976\n 1.1903296  0.7566509  0.5228791  0.5571232  0.86474705 0.5185571\n 0.29630136 2.857193   0.34315586 0.3407936  0.35344505 0.78012276\n 1.0308595  1.1181746  0.96464396 0.6193924  1.0460548  0.6460204\n 0.37556458 1.2298636  1.0683565  0.2628398  0.27957392 0.11733437\n 0.40555382 0.85659695 0.36876488 0.5393367  0.61431074 1.2375202\n 0.34581995 1.1594553  0.7108331  0.40506744 0.21834564 1.3506985\n 1.9985952  0.985034   1.6876874  1.6917176  1.0910554  0.24539375\n 0.37306452 0.6031499  0.9140558  3.6319532  1.4756522  0.771018\n 1.5085669  2.068924   0.9055376  0.38425064 0.7510824  0.30874634\n 1.4202456  1.1843233  1.7585297  2.4654179 ]", "mean_td_error": 0.9065183401107788, "actor_loss": -15.939992904663086, "critic_loss": 0.6295626163482666, "alpha_loss": -17.49505615234375, "alpha_value": 0.12568536400794983, "target_entropy": -6, "mean_q": 15.29277515411377, "max_q": 25.674104690551758, "min_q": 5.857640266418457, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 17000, "episodes_total": 17, "training_iteration": 17, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_14-52-57", "timestamp": 1587048777, "time_this_iter_s": 86.29307627677917, "time_total_s": 632.9975364208221, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 632.9975364208221, "timesteps_since_restore": 17000, "iterations_since_restore": 17, "perf": {"cpu_util_percent": 92.27808219178083, "ram_util_percent": 11.400000000000002}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -133.39316163305776, "episode_reward_min": -253.05802642599147, "episode_reward_mean": -181.32341176444686, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-253.05802642599147, -133.39316163305776, -167.90116094931736, -178.32733384843002, -182.14815629951562, -180.00798746459972, -162.04327362655897, -145.4620027979926, -213.46030883232947, -197.43270576667564], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26373045140759066, "mean_processing_ms": 0.12002519800779665, "mean_inference_ms": 1.177116988425405}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -56.48484841707745, "episode_reward_min": -234.58962540688248, "episode_reward_mean": -187.15667052144863, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-234.58962540688248, -194.36138843836824, -56.48484841707745, -220.91228762964403, -229.43520271527103], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.3147161141571709, "mean_processing_ms": 0.43236321741810696, "mean_inference_ms": 1.5237090756281242}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 9000, "num_steps_trained": 2046464, "num_steps_sampled": 18000, "sample_time_ms": 3.472, "replay_time_ms": 17.377, "grad_time_ms": 43.705, "update_time_ms": 0.005, "opt_peak_throughput": 5857.478, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[18.91523  ]\n [11.518908 ]\n [20.72578  ]\n [ 8.800333 ]\n [17.638626 ]\n [16.060078 ]\n [10.567142 ]\n [11.7523575]\n [13.667706 ]\n [17.899633 ]\n [17.879326 ]\n [11.661019 ]\n [20.8389   ]\n [10.684121 ]\n [14.3254385]\n [14.271773 ]\n [14.9675665]\n [12.738864 ]\n [13.376936 ]\n [11.970033 ]\n [14.171381 ]\n [16.437366 ]\n [15.090554 ]\n [17.639576 ]\n [13.772964 ]\n [ 6.130318 ]\n [15.163232 ]\n [10.614299 ]\n [13.660316 ]\n [ 7.31401  ]\n [10.979634 ]\n [16.311657 ]\n [13.177638 ]\n [ 5.88349  ]\n [13.593191 ]\n [20.39139  ]\n [14.778878 ]\n [16.503977 ]\n [12.423561 ]\n [ 7.863564 ]\n [21.587671 ]\n [14.964037 ]\n [16.8314   ]\n [20.76945  ]\n [24.514065 ]\n [10.34936  ]\n [23.18702  ]\n [19.147367 ]\n [14.016772 ]\n [21.476532 ]\n [ 7.2615595]\n [11.058799 ]\n [31.020964 ]\n [16.23611  ]\n [13.905385 ]\n [ 9.60993  ]\n [12.67494  ]\n [16.952566 ]\n [16.86066  ]\n [11.587188 ]\n [17.51721  ]\n [15.845271 ]\n [19.761934 ]\n [13.820602 ]\n [16.289999 ]\n [19.841818 ]\n [14.395104 ]\n [16.86066  ]\n [13.144071 ]\n [15.638598 ]\n [14.111516 ]\n [17.756453 ]\n [11.705418 ]\n [17.101141 ]\n [12.011369 ]\n [12.769687 ]\n [14.623808 ]\n [15.792221 ]\n [11.877239 ]\n [23.85146  ]\n [15.93067  ]\n [15.779883 ]\n [20.27729  ]\n [16.185137 ]\n [20.933891 ]\n [ 9.262236 ]\n [14.883235 ]\n [14.8146105]\n [14.280406 ]\n [14.354914 ]\n [17.778172 ]\n [14.84651  ]\n [15.55694  ]\n [18.304121 ]\n [19.234224 ]\n [11.548341 ]\n [ 9.477181 ]\n [25.866995 ]\n [17.933561 ]\n [11.870719 ]\n [14.225767 ]\n [12.503976 ]\n [17.00631  ]\n [17.059513 ]\n [13.925582 ]\n [17.943453 ]\n [15.159254 ]\n [20.70157  ]\n [11.506375 ]\n [14.778336 ]\n [16.805218 ]\n [11.715951 ]\n [ 9.722245 ]\n [18.86959  ]\n [18.926575 ]\n [14.125808 ]\n [15.90211  ]\n [15.263012 ]\n [11.999586 ]\n [18.634632 ]\n [12.452075 ]\n [14.184781 ]\n [15.115558 ]\n [21.546816 ]\n [13.576416 ]\n [10.375071 ]\n [11.168134 ]\n [17.732546 ]\n [19.862026 ]\n [13.337362 ]\n [19.424753 ]\n [16.946499 ]\n [13.774639 ]\n [ 8.564101 ]\n [13.275091 ]\n [16.091827 ]\n [13.932277 ]\n [17.793823 ]\n [14.526972 ]\n [13.032417 ]\n [12.67075  ]\n [ 7.0268345]\n [14.163249 ]\n [19.076761 ]\n [14.005426 ]\n [13.498901 ]\n [11.655393 ]\n [15.944191 ]\n [13.229571 ]\n [20.27729  ]\n [16.163054 ]\n [16.757795 ]\n [ 9.602374 ]\n [20.3193   ]\n [16.638483 ]\n [14.59681  ]\n [14.084821 ]\n [17.170782 ]\n [17.960539 ]\n [12.711564 ]\n [14.952311 ]\n [18.01521  ]\n [15.966411 ]\n [20.8389   ]\n [15.259678 ]\n [13.8237705]\n [14.333143 ]\n [12.832529 ]\n [24.535471 ]\n [10.751188 ]\n [16.89065  ]\n [14.633219 ]\n [19.955456 ]\n [14.40199  ]\n [12.797815 ]\n [15.759609 ]\n [17.793812 ]\n [16.106451 ]\n [10.983669 ]\n [14.359024 ]\n [21.84325  ]\n [13.149492 ]\n [17.577465 ]\n [18.017834 ]\n [15.680898 ]\n [11.802284 ]\n [ 9.9584875]\n [ 9.651805 ]\n [11.989419 ]\n [ 6.776738 ]\n [16.835634 ]\n [13.634115 ]\n [14.878356 ]\n [15.629748 ]\n [17.64675  ]\n [21.644562 ]\n [13.249019 ]\n [15.836017 ]\n [15.415327 ]\n [17.97386  ]\n [ 9.820865 ]\n [12.197696 ]\n [13.123875 ]\n [20.176678 ]\n [14.263891 ]\n [12.832782 ]\n [24.342108 ]\n [24.695026 ]\n [12.07659  ]\n [20.708208 ]\n [18.227144 ]\n [11.726587 ]\n [16.203676 ]\n [10.732927 ]\n [17.30476  ]\n [12.388524 ]\n [17.147568 ]\n [17.04348  ]\n [14.67918  ]\n [17.583172 ]\n [13.882557 ]\n [19.611551 ]\n [18.53196  ]\n [22.885483 ]\n [15.498044 ]\n [14.588558 ]\n [11.555989 ]\n [14.389057 ]\n [19.229616 ]\n [17.46784  ]\n [14.516552 ]\n [17.720182 ]\n [14.476694 ]\n [15.655783 ]\n [14.690884 ]\n [12.926238 ]\n [12.668782 ]\n [10.45456  ]\n [ 6.7765527]\n [21.700062 ]\n [26.96648  ]\n [ 8.996106 ]\n [14.450146 ]\n [ 4.992077 ]\n [15.844775 ]\n [12.3771715]\n [13.488307 ]\n [15.05448  ]\n [15.908441 ]\n [15.299943 ]\n [14.495638 ]\n [13.783541 ]\n [21.007935 ]\n [18.629616 ]\n [10.014495 ]\n [19.104694 ]]", "q_t_selected": "[18.91523   11.518908  20.72578    8.800333  17.638626  16.060078\n 10.567142  11.7523575 13.667706  17.899633  17.879326  11.661019\n 20.8389    10.684121  14.3254385 14.271773  14.9675665 12.738864\n 13.376936  11.970033  14.171381  16.437366  15.090554  17.639576\n 13.772964   6.130318  15.163232  10.614299  13.660316   7.31401\n 10.979634  16.311657  13.177638   5.88349   13.593191  20.39139\n 14.778878  16.503977  12.423561   7.863564  21.587671  14.964037\n 16.8314    20.76945   24.514065  10.34936   23.18702   19.147367\n 14.016772  21.476532   7.2615595 11.058799  31.020964  16.23611\n 13.905385   9.60993   12.67494   16.952566  16.86066   11.587188\n 17.51721   15.845271  19.761934  13.820602  16.289999  19.841818\n 14.395104  16.86066   13.144071  15.638598  14.111516  17.756453\n 11.705418  17.101141  12.011369  12.769687  14.623808  15.792221\n 11.877239  23.85146   15.93067   15.779883  20.27729   16.185137\n 20.933891   9.262236  14.883235  14.8146105 14.280406  14.354914\n 17.778172  14.84651   15.55694   18.304121  19.234224  11.548341\n  9.477181  25.866995  17.933561  11.870719  14.225767  12.503976\n 17.00631   17.059513  13.925582  17.943453  15.159254  20.70157\n 11.506375  14.778336  16.805218  11.715951   9.722245  18.86959\n 18.926575  14.125808  15.90211   15.263012  11.999586  18.634632\n 12.452075  14.184781  15.115558  21.546816  13.576416  10.375071\n 11.168134  17.732546  19.862026  13.337362  19.424753  16.946499\n 13.774639   8.564101  13.275091  16.091827  13.932277  17.793823\n 14.526972  13.032417  12.67075    7.0268345 14.163249  19.076761\n 14.005426  13.498901  11.655393  15.944191  13.229571  20.27729\n 16.163054  16.757795   9.602374  20.3193    16.638483  14.59681\n 14.084821  17.170782  17.960539  12.711564  14.952311  18.01521\n 15.966411  20.8389    15.259678  13.8237705 14.333143  12.832529\n 24.535471  10.751188  16.89065   14.633219  19.955456  14.40199\n 12.797815  15.759609  17.793812  16.106451  10.983669  14.359024\n 21.84325   13.149492  17.577465  18.017834  15.680898  11.802284\n  9.9584875  9.651805  11.989419   6.776738  16.835634  13.634115\n 14.878356  15.629748  17.64675   21.644562  13.249019  15.836017\n 15.415327  17.97386    9.820865  12.197696  13.123875  20.176678\n 14.263891  12.832782  24.342108  24.695026  12.07659   20.708208\n 18.227144  11.726587  16.203676  10.732927  17.30476   12.388524\n 17.147568  17.04348   14.67918   17.583172  13.882557  19.611551\n 18.53196   22.885483  15.498044  14.588558  11.555989  14.389057\n 19.229616  17.46784   14.516552  17.720182  14.476694  15.655783\n 14.690884  12.926238  12.668782  10.45456    6.7765527 21.700062\n 26.96648    8.996106  14.450146   4.992077  15.844775  12.3771715\n 13.488307  15.05448   15.908441  15.299943  14.495638  13.783541\n 21.007935  18.629616  10.014495  19.104694 ]", "twin_q_t_selected": "[19.435917  11.542229  21.858456   9.634984  18.083433  17.350569\n 11.602967  12.921598  14.173215  18.606287  16.97968   11.52375\n 20.147923  10.171884  14.328148  14.405039  14.966868  11.992574\n 13.765296  11.343548  12.909467  16.393127  14.993939  17.65232\n 14.2917385  7.976788  15.293411   9.787749  14.221149   7.3759317\n 10.968513  17.42038   13.334357   6.22699   14.391929  20.305187\n 15.198445  16.467743  11.995476   9.33836   19.939476  14.621739\n 16.647268  20.033401  24.85212   11.426162  23.821215  19.487083\n 13.876001  21.461973   8.363085  11.515557  29.989265  16.117449\n 13.008559  10.627066  12.778558  16.327421  17.509903  11.173233\n 18.123327  15.271576  20.012339  14.19828   15.765603  21.339102\n 14.558829  17.509903  13.0272    15.765629  15.231563  17.782318\n 12.4011545 16.842566  12.472884  11.669433  15.547541  15.232852\n 11.782014  23.49186   16.048367  15.280693  18.979364  16.738281\n 21.185768   9.362001  14.40119   14.329283  13.898317  14.24961\n 17.338663  15.107722  14.393736  18.010973  19.26336   11.53287\n  9.3784485 26.27849   17.668291  11.720912  13.384353  11.401131\n 17.10272   16.536785  13.54936   19.176311  14.793695  20.437355\n 12.170391  16.485691  17.210342  12.4688225  9.524718  18.59228\n 18.774313  12.662763  16.79664   14.16041   11.199532  18.226559\n 12.28428   13.819046  16.220453  21.779999  12.89772    9.453284\n 11.93899   18.134111  20.302263  13.856054  19.247187  16.638546\n 13.394358   8.151382  12.745685  16.898933  13.661106  18.137526\n 14.678644  13.622082  13.752365   5.92659   14.957736  19.608654\n 14.956907  13.056316  13.120995  16.44408   13.087894  18.979364\n 16.029251  16.81399    9.632679  20.512949  17.87159   12.645165\n 13.962958  16.293493  18.147701  13.488844  16.510601  18.41272\n 16.61429   20.147923  14.430559  14.532287  14.775216  12.567074\n 24.841015  11.307435  18.032583  14.062022  20.531733  14.9904585\n 12.97505   15.69019   18.379328  14.478355  11.341773  15.429308\n 23.191097  12.806422  16.889378  17.442701  14.72306   12.760134\n 10.59543    9.098592  12.474839   7.7549086 17.43467   14.3239565\n 15.277104  14.74457   17.470774  21.346308  13.2223    16.13116\n 14.911117  17.290016   9.891003  12.212423  13.841218  19.977076\n 14.110849  13.33147   23.993444  26.218195  11.711891  19.679184\n 17.389055  13.463663  16.3613    11.006393  17.521362  11.055665\n 17.224266  17.81799   15.109594  18.208624  15.116939  20.097559\n 18.699305  21.480806  16.071718  14.030043  11.917038  15.20642\n 18.515303  17.363745  14.6053095 17.074131  15.068964  15.259292\n 14.62282   12.615896  12.652361  10.9646845  6.334135  21.982227\n 25.859318   8.846958  13.961292   5.499593  16.326216  11.2115345\n 12.434444  15.167385  15.092756  15.139655  15.330762  13.698399\n 20.665325  18.975292   9.184674  19.09468  ]", "q_t_selected_target": "[18.81903   10.81797   19.143764   8.92694   16.768301  17.393877\n 10.652301  11.720858  13.614467  17.694927  18.106594  10.493562\n 22.429266   9.704531  15.05997   11.457317  15.796167  11.336747\n 14.623863  10.159255  14.902271  16.279556  14.36896   19.322023\n 16.270002   6.776335  16.131575  10.640495  13.950454   8.8683605\n 11.36105   16.712751  14.386005   4.977832  13.054466  22.536251\n 13.866315  19.45085   11.791363   8.623267  19.651066  15.540575\n 18.764206  18.748377  24.3782    11.119474  23.379692  18.83038\n 12.734941  22.788322   8.222464  11.565315  30.514019  16.97765\n 13.5068    11.667215  13.826228  15.020495  18.237032  11.455563\n 18.877064  16.362736  19.350725  14.520362  15.169275  20.430803\n 15.395887  18.309534  11.263896  17.109035  14.50143   19.683275\n 12.545955  16.401121  13.82556   10.575659  12.560474  15.442608\n 10.693794  23.853659  17.687674  14.414831  21.341639  16.341805\n 21.558577   8.724302  16.091196  17.150143  12.622439  15.614866\n 17.92716   12.542181  15.319251  18.508419  18.865177  13.350176\n 11.314135  27.135525  18.626589   9.551431  14.137005  11.48471\n 17.912735  17.315983  14.877348  20.950884  16.824219  20.901672\n 13.185403  16.322168  17.026918  11.783792   7.31187   18.094934\n 18.116392  13.934183  15.586215  14.489845  10.686847  17.06312\n 10.450856  14.826487  13.63511   18.35268   12.324862   9.201637\n 12.404413  17.526554  18.584993  12.489775  18.450676  16.968119\n 12.51354    8.13515   14.7457485 16.067318  12.481278  19.151922\n 13.799379  14.804696  14.367052   5.5428286 14.818214  20.302378\n 13.583805  12.711988   8.782085  15.278872  13.786795  21.473072\n 15.053394  16.557487  11.304526  21.548477  18.095274  14.372601\n 14.203303  16.279097  19.139542  13.589899  14.184049  19.402163\n 14.374521  21.574253  14.211132  14.74627   15.851215  11.246415\n 24.06704   11.672954  16.602982  12.711095  18.879835  16.424513\n 12.173898  14.675993  16.887447  13.892944  10.714176  15.4132185\n 23.86599   12.718247  15.832602  17.618528  15.053505  12.921594\n 12.226009   9.930693  12.262016   8.491985  18.658834  15.496846\n 14.600336  17.318123  18.716066  24.072535  13.014632  17.581755\n 15.709829  15.858811  10.700265  10.725034  14.740234  20.04097\n 12.886578  14.827233  23.846441  26.454565  11.245536  19.496405\n 18.155651  13.982376  16.598007  11.819154  19.39122   12.786175\n 18.545437  17.962038  14.736758  18.087803  15.027672  18.735167\n 20.37361   23.811386  16.663752  14.035332  11.094963  14.723024\n 20.028215  16.71434   13.190385  16.721563  14.628545  13.684789\n 13.598799  13.2587805 15.375627   9.811522   5.3302183 23.06468\n 28.079523   7.7012105 15.128908   4.904208  15.504219  10.065935\n 12.510021  14.668483  15.383589  13.453546  16.266838  14.957265\n 19.79829   19.861645   6.5912194 20.557392 ]", "q_tp1_best_masked": "[19.269674  11.909046  19.77238   10.203616  16.893475  18.045853\n 12.240757  12.847558  14.48118   17.33421   18.280905  11.554909\n 23.788649  11.383073  15.643463  13.261214  16.243753  11.533916\n 16.051018  11.52471   15.354655  16.831202  14.957212  20.488968\n 15.488819  10.405774  16.476824  11.780995  14.989513  11.561295\n 12.463399  17.520071  15.569017   8.557449  13.319913  21.963112\n 14.601411  18.907255  13.089939  10.112197  20.218504  15.989356\n 20.419382  18.404232  23.5809    11.752301  23.266298  18.044374\n 13.805446  24.029034  10.381672  12.694847  29.464655  19.243238\n 13.994523  12.556971  15.238703  14.995568  18.68212   13.561236\n 18.739677  16.181633  19.610123  15.301433  15.593498  19.631056\n 15.613414  18.755354  12.376431  18.078205  14.529814  19.837818\n 13.01826   16.222143  14.803378  12.154139  14.146078  15.220515\n 12.408212  22.635641  18.114025  15.702337  20.682213  16.809086\n 21.105679   9.700142  16.362919  17.436155  13.001717  15.964532\n 17.743542  13.004379  14.7388115 18.591093  18.347326  15.134937\n 12.057335  25.621458  18.839636  12.001633  14.856272  12.5432\n 17.93026   17.614157  15.853814  20.764187  17.026009  21.424694\n 14.406052  17.13773   16.866325  12.963541   8.3750725 18.03486\n 18.565311  14.514516  15.36692   15.356492  11.463903  17.481539\n 11.86349   16.452757  14.408225  18.389572  13.037788  10.320802\n 12.961251  17.463242  18.482124  13.320517  19.022017  16.53192\n 13.707079   9.860465  15.433941  16.175627  12.417768  19.30778\n 14.376772  15.37658   15.2332735  7.8246965 15.5137005 19.979967\n 14.314867  13.2139     9.917986  15.592487  14.9141865 20.814974\n 15.769394  15.914511  13.32818   20.466393  17.864119  15.198897\n 14.443174  17.29836   19.688116  13.927704  14.084029  20.003672\n 14.932209  22.925     14.462842  15.312394  16.280767  12.792451\n 23.689705  13.818782  17.770603  13.380958  18.911846  17.393103\n 12.57047   15.338725  16.466614  14.303701  11.874615  15.903484\n 24.811352  14.049005  16.113596  17.738522  15.706033  13.764366\n 13.230143  11.101933  12.922915  10.228415  20.624636  16.767946\n 14.6532135 17.005402  18.94957   23.25739   13.823409  18.116837\n 16.83635   16.65052   10.633708  11.181591  15.120715  20.301973\n 13.2989645 15.659379  23.750355  25.43565   12.092031  21.181335\n 19.011084  15.628334  18.102726  12.112295  20.015188  13.680927\n 18.47334   18.167604  14.607184  17.94878   16.30645   18.380339\n 19.831842  25.59204   15.987926  14.093731  12.585403  14.961177\n 19.840416  16.766794  13.722027  16.642593  13.736173  13.56761\n 14.932213  13.770099  16.379065  10.566113   6.8359227 22.604155\n 27.232529   9.383031  15.256743   6.455892  16.304558  12.10874\n 13.045339  15.121956  16.40649   14.260116  17.031199  15.674472\n 19.503443  20.079216   7.497197  20.110441 ]", "policy_t": "[[-0.73908895  0.2060368   0.8243482   0.59476435  0.62202036 -0.91120493]\n [ 0.40115106  0.92833054 -0.389117    0.72623444 -0.04031789 -0.2235434 ]\n [ 0.42096782  0.48907626  0.53238153  0.99767816 -0.7171282  -0.40201998]\n ...\n [-0.7194939  -0.6492136   0.64592695  0.9918835   0.82124746  0.50587773]\n [-0.43587577  0.16296351 -0.17707294 -0.9451339  -0.27401686 -0.03258491]\n [-0.7209024  -0.19401109 -0.16854465  0.51799214  0.94405234 -0.7037304 ]]", "td_error": "[0.3565426  0.71259785 2.1483536  0.4173255  1.0927286  0.6885538\n 0.51791286 0.61612034 0.30599356 0.558033   0.6770916  1.0988231\n 1.9358549  0.72347164 0.7331767  2.8810887  0.8289499  1.0289717\n 1.0527472  1.4975352  1.3618474  0.13569069 0.67328644 1.676075\n 2.2376513  0.92323494 0.9032531  0.43947124 0.2804165  1.5233896\n 0.38697624 0.55436134 1.1300077  1.0774083  0.93809366 2.1879625\n 1.1223469  2.9649897  0.41815567 0.7373979  1.1125078  0.74768686\n 2.0248718  1.6530485  0.3048916  0.53840065 0.31709766 0.48684597\n 1.2114463  1.3190699  0.55076265 0.2781372  0.5158491  0.80086994\n 0.4484129  1.5487175  1.0994792  1.6194983  1.0517502  0.20697737\n 1.0567951  0.8043122  0.5364113  0.5109205  0.85852575 0.74864197\n 0.9189205  1.1242523  1.8217392  1.4069219  0.5600233  1.9138899\n 0.49266863 0.5707321  1.5834332  1.6439009  2.5252     0.27968454\n 1.1358323  0.1819992  1.6981554  1.115457   1.7133112  0.27657223\n 0.49874687 0.58781624 1.4489837  2.578196   1.4669223  1.3126044\n 0.36874294 2.434935   0.5816021  0.35087204 0.38361454 1.8095703\n 1.8863196  1.0627823  0.8256626  2.2443848  0.42070723 0.5514226\n 0.8582201  0.5178337  1.1398768  2.3910017  1.847744   0.3322096\n 1.3470197  0.85367775 0.20256233 0.37643576 2.3116117  0.63600063\n 0.7340517  0.73152256 0.7631602  0.551301   0.9127121  1.3674755\n 1.9173212  0.82457304 2.0328956  3.3107271  0.91220665 0.71254015\n 0.85085154 0.40677452 1.4971514  1.1069336  0.88529396 0.17559624\n 1.0709581  0.22259188 1.7353606  0.42806244 1.315413   1.1862478\n 0.80342865 1.4774466  1.1554947  0.93388367 0.3972435  0.95967007\n 0.89736176 0.5656204  3.6061082  0.9152641  0.6280618  1.8447447\n 1.042758   0.228405   1.6869998  1.1323528  0.8402376  0.97582245\n 0.1794138  0.45304108 1.0854216  0.48969507 1.5474072  1.1881981\n 1.9158292  1.080842   0.6339865  0.5682416  1.2970357  1.4533863\n 0.6212034  0.64364195 0.85863495 1.6365256  1.363759   1.7282887\n 0.7125349  1.0489068  1.1991224  1.3994589  0.44854498 0.53514194\n 1.3488159  0.25970984 1.4008198  0.28756618 0.47891903 0.6403847\n 1.9490504  0.5554943  0.24271011 1.226162   1.5236826  1.5178103\n 0.4773941  2.1309638  1.1573048  2.5770998  0.2210269  1.598166\n 0.5466075  1.7731266  0.84433126 1.4800258  1.257688   0.09980106\n 1.3007927  1.7451077  0.32133484 0.99795437 0.6487045  0.6972914\n 0.4190445  1.3872509  0.31551933 0.9494934  1.978159   1.0640802\n 1.35952    0.5313034  0.2152071  0.31272602 0.61719084 1.1193886\n 1.7579775  1.6282415  0.8788705  0.27925777 0.64155054 0.4086814\n 1.155756   0.70145226 1.3705459  0.6755934  0.29613495 1.7727485\n 1.058053   0.48771334 2.715055   0.89810085 1.2251256  1.2235355\n 1.6666241  1.2203217  0.92318916 0.34162664 0.5812764  1.7284179\n 0.5269313  0.44244957 0.40784216 1.7662535  1.3536382  1.2162952\n 1.0383396  1.0591908  3.0083652  1.4577045 ]", "mean_td_error": 1.0461136102676392, "actor_loss": -15.885704040527344, "critic_loss": 0.8012708425521851, "alpha_loss": -17.892793655395508, "alpha_value": 0.09523332118988037, "target_entropy": -6, "mean_q": 15.242757797241211, "max_q": 31.020963668823242, "min_q": 4.992076873779297, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 18000, "episodes_total": 18, "training_iteration": 18, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_14-54-39", "timestamp": 1587048879, "time_this_iter_s": 86.53010201454163, "time_total_s": 719.5276384353638, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 719.5276384353638, "timesteps_since_restore": 18000, "iterations_since_restore": 18, "perf": {"cpu_util_percent": 92.36689655172414, "ram_util_percent": 11.400000000000004}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -212.1690002199358, "episode_reward_min": -270.96755284880965, "episode_reward_mean": -242.23338343211645, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-237.41154194330798, -249.56255905676758, -229.00393963909784, -220.73107887349903, -266.61779161247654, -270.96755284880965, -216.6669229236508, -251.72420605559864, -212.1690002199358, -267.47924114802083], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26372267784928927, "mean_processing_ms": 0.12004044786276998, "mean_inference_ms": 1.1766820743847803}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -56.48484841707745, "episode_reward_min": -234.58962540688248, "episode_reward_mean": -167.68637352938748, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-97.0099034780624, -56.48484841707745, -220.91228762964403, -229.43520271527103, -234.58962540688248], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.3195387485511304, "mean_processing_ms": 0.43772967007142094, "mean_inference_ms": 1.5553170063318527}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 9500, "num_steps_trained": 2302464, "num_steps_sampled": 19000, "sample_time_ms": 2.601, "replay_time_ms": 16.997, "grad_time_ms": 44.121, "update_time_ms": 0.005, "opt_peak_throughput": 5802.169, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[ 9.387317 ]\n [20.20474  ]\n [20.356024 ]\n [16.413363 ]\n [12.516909 ]\n [17.733484 ]\n [16.585005 ]\n [14.0594845]\n [20.572767 ]\n [23.064327 ]\n [20.098404 ]\n [12.122419 ]\n [ 9.752367 ]\n [10.3911915]\n [18.656683 ]\n [14.623194 ]\n [13.800198 ]\n [14.033511 ]\n [23.31595  ]\n [19.059439 ]\n [11.703257 ]\n [17.607885 ]\n [15.455904 ]\n [16.884815 ]\n [11.616956 ]\n [16.587341 ]\n [15.497904 ]\n [14.414376 ]\n [18.236753 ]\n [13.505598 ]\n [22.928526 ]\n [21.264635 ]\n [13.375231 ]\n [13.912056 ]\n [17.515413 ]\n [14.38507  ]\n [ 9.954255 ]\n [18.429655 ]\n [15.728221 ]\n [23.684822 ]\n [17.597013 ]\n [12.17025  ]\n [15.296366 ]\n [23.546652 ]\n [13.351367 ]\n [ 8.610969 ]\n [28.544573 ]\n [13.295227 ]\n [12.954495 ]\n [15.127438 ]\n [19.91977  ]\n [15.491515 ]\n [14.70513  ]\n [14.623403 ]\n [12.864167 ]\n [14.477485 ]\n [ 8.84412  ]\n [13.818561 ]\n [13.60108  ]\n [ 9.194163 ]\n [14.215466 ]\n [15.134017 ]\n [14.118239 ]\n [15.124303 ]\n [22.400747 ]\n [13.86347  ]\n [ 7.4650292]\n [18.197514 ]\n [18.220133 ]\n [17.38876  ]\n [17.18222  ]\n [13.029995 ]\n [14.40778  ]\n [13.968743 ]\n [13.308115 ]\n [15.517138 ]\n [17.720856 ]\n [15.318056 ]\n [16.916328 ]\n [14.279175 ]\n [16.597115 ]\n [14.049831 ]\n [11.987039 ]\n [13.423492 ]\n [14.78923  ]\n [17.290968 ]\n [15.911653 ]\n [10.88566  ]\n [16.730446 ]\n [20.774988 ]\n [11.145261 ]\n [18.293064 ]\n [16.47329  ]\n [ 8.087891 ]\n [17.814272 ]\n [15.155796 ]\n [15.399416 ]\n [ 8.14368  ]\n [10.814063 ]\n [16.286697 ]\n [21.628838 ]\n [12.613698 ]\n [12.197554 ]\n [12.4025545]\n [16.31988  ]\n [14.451982 ]\n [15.094362 ]\n [13.358163 ]\n [22.16068  ]\n [18.734632 ]\n [15.445639 ]\n [11.565262 ]\n [22.037064 ]\n [20.511204 ]\n [ 5.7794275]\n [10.605121 ]\n [14.509174 ]\n [11.359357 ]\n [12.658765 ]\n [16.359753 ]\n [12.448185 ]\n [13.369072 ]\n [17.08903  ]\n [ 9.867097 ]\n [12.008421 ]\n [14.615797 ]\n [15.722441 ]\n [16.688341 ]\n [14.459908 ]\n [12.2194   ]\n [12.647371 ]\n [13.650045 ]\n [23.798313 ]\n [13.9767065]\n [13.506715 ]\n [22.928177 ]\n [23.596037 ]\n [20.118904 ]\n [ 6.648076 ]\n [ 8.9257765]\n [16.87686  ]\n [17.450846 ]\n [14.316891 ]\n [13.336664 ]\n [16.586708 ]\n [14.606606 ]\n [ 7.392865 ]\n [13.2394085]\n [20.057337 ]\n [ 7.804968 ]\n [13.100854 ]\n [12.545908 ]\n [21.229376 ]\n [13.953065 ]\n [14.313805 ]\n [ 5.3085613]\n [ 4.6402583]\n [ 2.1690085]\n [13.478298 ]\n [14.770473 ]\n [11.889696 ]\n [22.27612  ]\n [19.242462 ]\n [14.349451 ]\n [12.180567 ]\n [13.097759 ]\n [13.86384  ]\n [11.966194 ]\n [16.893438 ]\n [15.290952 ]\n [ 5.4382586]\n [13.663077 ]\n [10.327307 ]\n [15.870774 ]\n [20.328207 ]\n [20.698929 ]\n [21.10954  ]\n [11.992314 ]\n [12.78057  ]\n [10.567063 ]\n [14.791575 ]\n [17.071709 ]\n [14.132997 ]\n [13.668277 ]\n [12.146953 ]\n [16.072454 ]\n [14.242017 ]\n [12.093658 ]\n [14.206347 ]\n [14.9776   ]\n [14.343346 ]\n [13.829623 ]\n [13.563718 ]\n [14.820592 ]\n [11.477916 ]\n [15.763833 ]\n [13.212322 ]\n [20.312971 ]\n [13.813792 ]\n [20.210709 ]\n [13.839177 ]\n [13.743625 ]\n [14.54663  ]\n [13.0396   ]\n [14.534333 ]\n [20.728508 ]\n [13.13643  ]\n [18.909815 ]\n [14.188998 ]\n [13.86547  ]\n [ 9.713137 ]\n [15.315326 ]\n [16.513554 ]\n [13.342418 ]\n [12.9502   ]\n [13.034444 ]\n [15.213452 ]\n [ 8.985742 ]\n [18.843971 ]\n [16.843473 ]\n [10.374832 ]\n [11.566053 ]\n [22.211668 ]\n [12.917565 ]\n [11.893803 ]\n [15.124164 ]\n [11.677786 ]\n [17.17677  ]\n [15.567277 ]\n [18.775375 ]\n [13.095493 ]\n [14.206347 ]\n [11.447265 ]\n [13.361379 ]\n [14.288635 ]\n [-4.2198453]\n [13.007319 ]\n [16.969728 ]\n [ 5.716442 ]\n [13.523108 ]\n [15.495174 ]\n [ 9.4334955]\n [14.99701  ]\n [12.10106  ]\n [13.982998 ]\n [24.23011  ]\n [25.81332  ]\n [14.988689 ]\n [22.602316 ]\n [13.892418 ]\n [20.481073 ]\n [ 9.648447 ]\n [16.645262 ]\n [14.02982  ]\n [13.671816 ]\n [10.761285 ]]", "q_t_selected": "[ 9.387317  20.20474   20.356024  16.413363  12.516909  17.733484\n 16.585005  14.0594845 20.572767  23.064327  20.098404  12.122419\n  9.752367  10.3911915 18.656683  14.623194  13.800198  14.033511\n 23.31595   19.059439  11.703257  17.607885  15.455904  16.884815\n 11.616956  16.587341  15.497904  14.414376  18.236753  13.505598\n 22.928526  21.264635  13.375231  13.912056  17.515413  14.38507\n  9.954255  18.429655  15.728221  23.684822  17.597013  12.17025\n 15.296366  23.546652  13.351367   8.610969  28.544573  13.295227\n 12.954495  15.127438  19.91977   15.491515  14.70513   14.623403\n 12.864167  14.477485   8.84412   13.818561  13.60108    9.194163\n 14.215466  15.134017  14.118239  15.124303  22.400747  13.86347\n  7.4650292 18.197514  18.220133  17.38876   17.18222   13.029995\n 14.40778   13.968743  13.308115  15.517138  17.720856  15.318056\n 16.916328  14.279175  16.597115  14.049831  11.987039  13.423492\n 14.78923   17.290968  15.911653  10.88566   16.730446  20.774988\n 11.145261  18.293064  16.47329    8.087891  17.814272  15.155796\n 15.399416   8.14368   10.814063  16.286697  21.628838  12.613698\n 12.197554  12.4025545 16.31988   14.451982  15.094362  13.358163\n 22.16068   18.734632  15.445639  11.565262  22.037064  20.511204\n  5.7794275 10.605121  14.509174  11.359357  12.658765  16.359753\n 12.448185  13.369072  17.08903    9.867097  12.008421  14.615797\n 15.722441  16.688341  14.459908  12.2194    12.647371  13.650045\n 23.798313  13.9767065 13.506715  22.928177  23.596037  20.118904\n  6.648076   8.9257765 16.87686   17.450846  14.316891  13.336664\n 16.586708  14.606606   7.392865  13.2394085 20.057337   7.804968\n 13.100854  12.545908  21.229376  13.953065  14.313805   5.3085613\n  4.6402583  2.1690085 13.478298  14.770473  11.889696  22.27612\n 19.242462  14.349451  12.180567  13.097759  13.86384   11.966194\n 16.893438  15.290952   5.4382586 13.663077  10.327307  15.870774\n 20.328207  20.698929  21.10954   11.992314  12.78057   10.567063\n 14.791575  17.071709  14.132997  13.668277  12.146953  16.072454\n 14.242017  12.093658  14.206347  14.9776    14.343346  13.829623\n 13.563718  14.820592  11.477916  15.763833  13.212322  20.312971\n 13.813792  20.210709  13.839177  13.743625  14.54663   13.0396\n 14.534333  20.728508  13.13643   18.909815  14.188998  13.86547\n  9.713137  15.315326  16.513554  13.342418  12.9502    13.034444\n 15.213452   8.985742  18.843971  16.843473  10.374832  11.566053\n 22.211668  12.917565  11.893803  15.124164  11.677786  17.17677\n 15.567277  18.775375  13.095493  14.206347  11.447265  13.361379\n 14.288635  -4.2198453 13.007319  16.969728   5.716442  13.523108\n 15.495174   9.4334955 14.99701   12.10106   13.982998  24.23011\n 25.81332   14.988689  22.602316  13.892418  20.481073   9.648447\n 16.645262  14.02982   13.671816  10.761285 ]", "twin_q_t_selected": "[ 8.496977  21.035755  20.3001    16.33806   12.19036   18.4703\n 17.279432  14.314487  21.221104  22.541111  20.105583  12.509752\n 11.136227  10.039511  17.766216  15.859828  14.147586  13.508244\n 22.920275  19.6953    11.536649  16.315714  15.018235  17.964346\n 11.672458  17.520899  15.6835985 13.596271  18.740412  14.9748535\n 22.18418   20.675634  13.712773  12.839383  16.984234  14.216276\n  9.282428  17.931696  16.318933  23.477156  17.039314  12.460065\n 14.842161  23.567797  13.825877   8.236745  29.873535  13.41711\n 12.882114  15.857254  19.217562  14.728399  14.957644  14.812757\n 12.18835   14.69141    9.170693  13.534798  13.644917  10.25803\n 13.642994  15.1002445 13.882396  14.4329405 22.278803  13.58631\n  7.6528897 19.362638  17.414822  17.47717   17.619133  13.978522\n 14.368787  14.169654  13.841138  14.749395  18.345633  14.867017\n 16.543327  14.324648  17.4032    14.654218  12.159607  14.08236\n 14.630944  17.477365  15.602718  10.876057  17.235489  20.404478\n 10.995289  18.179377  17.439543   8.083093  18.338614  14.461954\n 16.097998   8.403214   9.73513   15.731114  21.850267  13.042621\n 11.668998  12.885081  16.83403   15.742733  16.165941  12.653089\n 22.52683   20.565805  14.012377  11.73615   23.4755    20.112076\n  6.428829  10.545068  15.434233  11.316376  12.706729  16.808903\n 11.582538  13.299352  16.624453  10.435811  12.065765  14.033316\n 15.557547  17.63606   13.6695175 12.478655  13.361418  14.189239\n 23.216705  13.170621  14.522176  22.928335  22.737083  21.16068\n  7.0309553  9.023282  17.697672  17.551819  14.438796  14.53224\n 16.255482  13.437023   8.330866  13.309536  22.076668   7.484155\n 13.195373  11.462577  21.491615  14.646761  14.193982   4.78569\n  4.4033246  1.4044901 13.334323  13.888383  12.5585985 21.52777\n 19.054865  15.115514  12.0731325 13.690275  13.423763  12.13355\n 17.156845  16.908358   4.952707  13.972947  10.576237  15.131366\n 19.912191  19.972534  20.796335  12.278847  13.564376  11.247905\n 14.703488  17.47857   13.74951   14.241186  12.322089  15.768628\n 14.575494  11.556546  14.185612  15.256032  13.875421  13.922404\n 13.648719  15.198365  12.893883  15.391276  13.538753  20.552818\n 13.954438  22.188583  13.641144  13.380194  15.234752  12.936283\n 15.09589   22.791489  13.948656  18.47105   14.733627  14.413737\n  9.373573  15.181823  17.075815  13.756343  12.887007  12.886754\n 16.40286    8.485514  18.400595  17.464872  11.032995  11.7684555\n 21.466063  11.807539  10.881269  15.391483  13.210329  17.765795\n 16.335869  19.070814  13.097801  14.185612  11.257188  13.0153675\n 13.907403  -3.0946984 13.875136  15.862191   5.918029  12.824915\n 15.053883   8.32454   14.501989  12.193266  14.147389  25.626759\n 25.496355  15.366039  21.61948   13.427878  19.561594   9.9614105\n 16.620916  14.353568  14.168397  11.26644  ]", "q_t_selected_target": "[ 9.744852  21.13144   19.314034  18.36754   12.782178  19.103294\n 17.254665  14.248945  21.341042  24.105501  19.87534   10.755018\n 11.928838  10.012728  17.796783  15.448821  13.893489  16.794685\n 22.001549  19.318697  12.431396  16.849976  15.354282  16.384968\n 11.583241  17.051249  17.267889  14.554604  17.334644  14.898897\n 24.117264  19.747635  14.729819  14.689851  16.169903  13.445492\n 10.858526  20.225048  17.50848   24.52141   17.628809  13.604449\n 16.762787  23.650255  14.479328   7.280263  30.807974  12.202282\n 12.425399  14.3120775 19.712831  14.651869  16.152176  14.866389\n 12.040432  14.800724   8.178001  12.589815  14.455968  12.288905\n 14.105417  15.497656  14.4361515 14.457486  20.96838   14.007723\n  9.12693   19.303406  17.471657  18.090351  19.060347  14.770848\n 14.990756  14.346287  13.761288  15.330755  18.189823  14.997592\n 16.308744  13.466415  16.09671   16.715279  13.673125  12.754716\n 13.448323  16.942072  17.580072  11.67888   16.212677  20.089888\n 10.83836   22.163166  17.367336   7.44501   18.985859  15.473472\n 14.506286   6.1546373 10.4544    16.154106  22.424168  13.8210335\n 11.420141  12.879461  16.856586  17.300253  15.813556  13.394967\n 22.652143  21.857687  15.506333  10.699694  24.454721  19.470955\n  5.942306  12.434637  14.085704  12.225478  13.962381  17.952892\n 12.886351  14.014618  17.274982  11.15676   13.100509  12.923932\n 17.389925  14.837663  15.150921  13.452861  12.791382  13.015104\n 22.883392  12.897137  13.719728  22.667137  24.208817  22.64821\n  5.2725153 10.669783  19.332941  18.45184   13.661883  14.586967\n 15.586438  14.289422   7.4368143 13.614531  22.744822   7.3890705\n 14.0356865 12.471831  20.524906  15.422702  14.903297   2.9509032\n  5.5134807 -0.7803778 12.677503  13.793292   9.477373  24.553854\n 20.75941   15.016842  13.847573  13.216104  15.214331  14.078084\n 14.460476  14.935577   7.5695977 14.586974  11.1932745 15.843487\n 19.441914  20.02029   20.115625  14.081972  12.937018  14.385839\n 16.27184   17.863037  14.787579  12.717008  12.984709  15.059325\n 15.295304  13.041616  15.033173  15.216723  13.528511  15.938083\n 14.249069  13.616805  11.681381  16.740326  14.514053  22.13557\n 15.261471  22.411892  14.110785  13.821612  13.111363  11.086944\n 14.460277  22.26349   14.735549  20.06564   15.865533  13.621657\n  8.557261  15.749167  18.120407  11.338436  13.578149  14.20126\n 16.931826   8.113137  19.034094  18.31926   11.975461  11.031498\n 24.22396   12.789694  10.270282  16.971071  13.212599  19.165863\n 15.639441  18.46508   13.961053  14.929804  12.241825  13.130468\n 13.967095  -4.225401  15.495119  18.00431    4.414036  15.745845\n 16.489685  10.223136  14.784329  13.45977   14.507885  26.599289\n 25.24287   14.154208  25.585854  14.307722  19.227953   8.898435\n 16.294651  14.450459  12.369766  10.599526 ]", "q_tp1_best_masked": "[10.336301   21.55106    19.097569   18.401789   13.607333   18.363096\n 16.892458   15.639523   21.399588   23.288965   19.262297   11.984952\n 12.998789   10.997105   18.45673    15.919549   14.520975   16.933823\n 21.633482   19.327316   13.253572   17.986818   15.750564   17.254345\n 11.862287   18.24816    17.336967   14.642653   17.400064   17.295124\n 22.73863    20.187735   16.349909   14.560839   17.420378   14.473818\n 11.394876   20.576752   17.869999   24.25216    18.993176   13.8298235\n 16.883179   22.29106    15.098669    8.399588   30.347645   13.259802\n 12.968818   15.270711   18.970108   14.922531   16.219309   14.688884\n 13.249958   16.925232    9.060028   13.242399   14.961302   14.8120575\n 15.175899   15.288553   14.120886   16.395906   20.991743   14.399205\n 11.694377   19.235159   17.947603   19.105679   19.24312    16.54611\n 15.315571   15.79554    13.68876    15.200176   18.364197   15.848993\n 17.131025   14.822295   16.993055   17.784233   14.609411   12.454478\n 13.958328   16.919088   17.876734   11.742278   16.941023   20.130812\n 12.842133   22.503649   17.534267    9.392856   19.29065    16.232084\n 15.073329    8.741046   10.379719   15.1027155  22.717545   15.319073\n 12.327954   14.494593   17.004486   17.951283   16.322731   14.324254\n 22.047974   20.927412   16.23208    11.997673   25.268423   20.146776\n  8.05403    13.350055   14.348033   13.338751   15.129829   17.761515\n 14.3928795  14.283213   18.52132    10.470485   15.56       13.595945\n 17.635962   15.200741   16.079008   14.168278   14.88664    12.560522\n 22.870205   13.266037   14.240931   22.645964   23.380993   22.015434\n  7.388551   13.227268   19.62668    18.380432   13.803458   15.060911\n 15.963746   13.860885    8.676469   13.537642   22.761812    9.094173\n 14.540956   13.983207   20.099709   14.895306   15.510507    4.7487984\n  6.8803105   0.97925574 13.203082   14.105176   11.265127   26.151722\n 19.851519   15.681859   14.389699   14.117691   15.193712   15.265023\n 14.375854   15.537824    9.408905   15.24786    11.526275   15.708796\n 19.571949   20.039488   19.595892   14.291478   13.117039   15.043542\n 16.61906    17.886858   14.095587   12.333037   14.367969   15.151092\n 15.774144   13.555814   15.896367   15.37337    13.668071   15.79919\n 14.934292   14.3493805  13.974951   17.036835   15.078366   21.946308\n 16.835316   23.200737   14.740805   13.988064   13.554431   11.667516\n 14.33016    22.215351   14.251097   20.541086   16.517992   13.86742\n  9.316047   16.266598   18.157215   12.477494   14.137192   14.485544\n 17.4808      9.799118   19.393888   19.4283     13.106691   11.651418\n 23.147085   13.2120075  12.0515785  18.496782   13.921483   19.272339\n 15.302381   18.145649   13.828305   15.791954   12.828083   13.600523\n 14.702937   -0.641358   16.767685   18.441284    6.542445   16.965734\n 16.713472   10.699382   15.475668   14.530934   14.752084   27.08469\n 23.532064   15.312614   26.317406   14.174725   19.707815   11.146067\n 16.373566   16.231548   12.742463   12.4228325 ]", "policy_t": "[[ 0.8026403   0.1757369  -0.26328564 -0.5661573   0.47540426 -0.72174484]\n [-0.62609655  0.7749026   0.34991455 -0.9129796  -0.52661264  0.72845435]\n [-0.8964381   0.97255194 -0.68659437 -0.73345697  0.9627905  -0.7749385 ]\n ...\n [ 0.369586    0.54497504 -0.00778514 -0.35235822 -0.75010115 -0.82219   ]\n [-0.10983223  0.9169595  -0.9032883  -0.82061327  0.14351201 -0.14187384]\n [ 0.5035659   0.09117198 -0.23632634 -0.81292254 -0.1891852   0.4708779 ]]", "td_error": "[0.8027053  0.51119137 1.0140285  1.9918289  0.42854357 1.0014019\n 0.34721375 0.12750149 0.4441061  1.302782   0.22665405 1.5610676\n 1.4845409  0.20262337 0.44523335 0.6183171  0.17369413 3.023808\n 1.1165638  0.31793022 0.81144285 0.64608574 0.2188344  1.0396128\n 0.06146526 0.46677876 1.6771379  0.54928017 1.1539383  0.7346277\n 1.5609112  1.2224998  1.1858172  1.3141313  1.0799208  0.8551812\n 1.2401848  2.0443726  1.4849029  0.9404211  0.3106451  1.2892919\n 1.6935234  0.09303093 0.89070606 1.1435938  1.5989199  1.1538868\n 0.4929061  1.1802683  0.35110378 0.4580884  1.3207889  0.14830971\n 0.4858265  0.21627665 0.8294053  1.086864   0.83296967 2.5628085\n 0.2862358  0.3805251  0.43583393 0.3456812  1.3713951  0.28283262\n 1.5679708  0.58256245 0.4026556  0.6573858  1.6596699  1.2665896\n 0.6024728  0.27708817 0.26651144 0.38387108 0.31238842 0.22551966\n 0.42108345 0.83549595 0.90344715 2.363254   1.5998025  0.99821043\n 1.261764   0.4420948  1.822887   0.7980213  0.7702904  0.4998455\n 0.231915   3.9269457  0.48312664 0.6404815  0.9094162  0.66459656\n 1.2424212  2.1188097  0.5394664  0.2777915  0.68461514 0.99287415\n 0.5131345  0.24126339 0.2796316  2.2028956  0.5357895  0.38934135\n 0.30838776 2.207468   0.77732563 0.95101213 1.6984396  0.8406849\n 0.32470083 1.8595428  0.8859997  0.88761187 1.2796345  1.3685646\n 0.8709893  0.6804061  0.4182415  1.0053062  1.0634155  1.4006243\n 1.7499313  2.3245382  1.0862083  1.1038332  0.35702324 0.9045377\n 0.6241169  0.676527   0.5077305  0.2611189  1.0422564  2.008418\n 1.5670004  1.6952534  2.0456753  0.95050716 0.71596    0.6525154\n 0.8346567  0.5847912  0.46900034 0.34005833 1.6778193  0.25549102\n 0.88757324 0.54166555 0.8355894  1.1227889  0.64940405 2.0962224\n 0.9916892  2.5671272  0.7288079  0.5361357  2.7467742  2.6519098\n 1.6107473  0.38303137 1.7207236  0.29625797 1.570529   2.028212\n 2.5646658  1.1640773  2.374115   0.7689619  0.74150276 0.36970425\n 0.6782856  0.36319733 0.8373127  1.9463916  0.39190292 3.4783554\n 1.5243073  0.5878973  0.8463254  1.2377238  0.7501879  0.86121607\n 0.886549   1.2165141  0.837193   0.13921595 0.58087206 2.062069\n 0.6428509  1.3926735  0.7079835  1.1627712  1.138516   1.7026758\n 1.3773556  1.212246   0.37062407 0.25970316 1.7793274  1.9009981\n 0.35483503 1.0314903  1.193006   1.375207   1.4042201  0.51794624\n 0.9860935  0.5005932  1.3257227  2.2109442  0.6595454  1.2406607\n 1.1236691  0.6224904  0.41181087 1.1650867  1.2715473  0.6357565\n 2.3850956  0.5550132  1.1172543  1.7132478  0.76854134 1.694581\n 0.38429594 0.4580145  0.86440563 0.73382425 0.88959885 0.17300558\n 0.19061613 0.56812906 2.0538912  1.5883508  1.4031997  2.5718336\n 1.2151566  1.3441181  0.24751043 1.3126073  0.44269133 1.6708546\n 0.41196728 1.0231562  3.4749556  0.64757395 0.79338074 0.90649414\n 0.33843803 0.25876427 1.5503402  0.4143362 ]", "mean_td_error": 1.0107307434082031, "actor_loss": -15.499149322509766, "critic_loss": 0.7858806848526001, "alpha_loss": -17.819602966308594, "alpha_value": 0.07268417626619339, "target_entropy": -6, "mean_q": 14.812021255493164, "max_q": 28.544572830200195, "min_q": -4.219845294952393, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 19000, "episodes_total": 19, "training_iteration": 19, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_14-56-21", "timestamp": 1587048981, "time_this_iter_s": 86.05885338783264, "time_total_s": 805.5864918231964, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 805.5864918231964, "timesteps_since_restore": 19000, "iterations_since_restore": 19, "perf": {"cpu_util_percent": 92.29652777777778, "ram_util_percent": 11.400000000000002}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -120.17660554248276, "episode_reward_min": -315.1821561043099, "episode_reward_mean": -230.8006873210765, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-120.17660554248276, -248.050122496964, -203.20809631862588, -278.6082477834911, -178.90655850935954, -315.1821561043099, -226.56431075447023, -235.69974291963766, -254.94372245909017, -246.66731032233363], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26416942946924066, "mean_processing_ms": 0.12018720338009799, "mean_inference_ms": 1.1801581182706784}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -97.0099034780624, "episode_reward_min": -234.58962540688248, "episode_reward_mean": -182.23644753403246, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-129.23521844030233, -220.91228762964403, -229.43520271527103, -234.58962540688248, -97.0099034780624], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.32499983055834836, "mean_processing_ms": 0.44262383708126973, "mean_inference_ms": 1.5837295773968116}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 10000, "num_steps_trained": 2558464, "num_steps_sampled": 20000, "sample_time_ms": 2.379, "replay_time_ms": 15.909, "grad_time_ms": 44.461, "update_time_ms": 0.004, "opt_peak_throughput": 5757.838, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[20.711733 ]\n [13.917956 ]\n [23.17358  ]\n [14.972419 ]\n [22.113247 ]\n [14.397954 ]\n [ 8.085576 ]\n [12.036104 ]\n [11.919744 ]\n [19.604923 ]\n [18.133022 ]\n [12.352173 ]\n [15.889054 ]\n [12.295257 ]\n [12.334726 ]\n [17.740923 ]\n [16.11542  ]\n [ 7.6233687]\n [14.100827 ]\n [12.247663 ]\n [23.27134  ]\n [11.943975 ]\n [18.475662 ]\n [14.335171 ]\n [13.144899 ]\n [17.881132 ]\n [14.309616 ]\n [20.363218 ]\n [10.676178 ]\n [17.367598 ]\n [17.691568 ]\n [11.930263 ]\n [12.501143 ]\n [16.562515 ]\n [10.897253 ]\n [17.506643 ]\n [19.990868 ]\n [ 7.9464765]\n [12.684742 ]\n [22.996035 ]\n [16.040232 ]\n [14.808085 ]\n [12.057245 ]\n [13.922281 ]\n [12.318167 ]\n [15.147132 ]\n [17.485632 ]\n [13.913834 ]\n [13.431848 ]\n [12.765648 ]\n [21.156218 ]\n [15.626971 ]\n [11.586027 ]\n [12.22141  ]\n [14.299389 ]\n [ 7.946384 ]\n [20.06794  ]\n [11.999364 ]\n [21.434856 ]\n [21.82829  ]\n [20.394648 ]\n [21.584642 ]\n [21.661825 ]\n [16.718939 ]\n [12.713587 ]\n [12.558331 ]\n [12.389166 ]\n [15.549831 ]\n [13.031851 ]\n [23.446655 ]\n [13.383912 ]\n [12.485034 ]\n [13.86869  ]\n [12.8622   ]\n [13.54796  ]\n [13.4721155]\n [14.100827 ]\n [22.188324 ]\n [ 5.8323374]\n [11.575705 ]\n [12.803902 ]\n [16.252882 ]\n [10.846713 ]\n [14.373135 ]\n [15.357093 ]\n [12.216218 ]\n [ 9.695072 ]\n [15.213736 ]\n [14.609835 ]\n [12.873598 ]\n [12.641505 ]\n [12.431818 ]\n [19.457762 ]\n [19.466463 ]\n [12.565896 ]\n [12.739644 ]\n [15.81864  ]\n [17.606464 ]\n [11.936194 ]\n [13.303204 ]\n [14.509262 ]\n [15.708755 ]\n [24.842394 ]\n [13.669436 ]\n [17.532747 ]\n [14.352909 ]\n [11.964029 ]\n [16.775654 ]\n [14.843203 ]\n [17.812387 ]\n [19.1794   ]\n [26.73688  ]\n [19.319613 ]\n [15.181225 ]\n [16.129606 ]\n [11.892668 ]\n [11.944358 ]\n [14.276466 ]\n [13.095264 ]\n [14.09634  ]\n [14.410367 ]\n [10.071254 ]\n [11.99141  ]\n [11.9621   ]\n [17.8955   ]\n [22.477037 ]\n [11.64659  ]\n [14.036694 ]\n [20.629904 ]\n [12.337257 ]\n [22.273197 ]\n [14.571288 ]\n [15.71503  ]\n [10.1377735]\n [12.665576 ]\n [12.631244 ]\n [10.044274 ]\n [21.528364 ]\n [12.6214075]\n [12.949904 ]\n [13.105942 ]\n [ 9.640524 ]\n [11.974998 ]\n [10.169841 ]\n [14.405379 ]\n [13.603572 ]\n [21.685472 ]\n [13.3615885]\n [11.242771 ]\n [13.15768  ]\n [20.28068  ]\n [20.134237 ]\n [14.433657 ]\n [19.368248 ]\n [12.099582 ]\n [18.018803 ]\n [14.8404045]\n [16.801521 ]\n [19.295229 ]\n [19.464813 ]\n [11.903966 ]\n [18.829706 ]\n [14.159584 ]\n [12.126688 ]\n [12.434054 ]\n [14.577878 ]\n [10.982211 ]\n [13.866053 ]\n [14.527685 ]\n [13.811366 ]\n [15.132528 ]\n [17.9989   ]\n [16.950014 ]\n [13.833874 ]\n [17.307825 ]\n [11.6044035]\n [ 7.4233155]\n [11.079135 ]\n [18.538513 ]\n [14.614289 ]\n [10.495901 ]\n [12.870184 ]\n [13.613143 ]\n [12.806236 ]\n [13.495609 ]\n [11.897295 ]\n [12.2413645]\n [11.67289  ]\n [17.367722 ]\n [15.180584 ]\n [10.002901 ]\n [12.112224 ]\n [12.415364 ]\n [14.193472 ]\n [11.987786 ]\n [18.114084 ]\n [23.745342 ]\n [14.899343 ]\n [14.234111 ]\n [19.396975 ]\n [14.305768 ]\n [11.470394 ]\n [12.260672 ]\n [16.378216 ]\n [14.779179 ]\n [18.68242  ]\n [13.06498  ]\n [19.94098  ]\n [12.399255 ]\n [13.037206 ]\n [22.189531 ]\n [14.02314  ]\n [12.526226 ]\n [14.985254 ]\n [13.167441 ]\n [12.408474 ]\n [21.584642 ]\n [18.287432 ]\n [12.031142 ]\n [17.209633 ]\n [11.381673 ]\n [15.649883 ]\n [17.25002  ]\n [12.269634 ]\n [21.449604 ]\n [18.89298  ]\n [12.317011 ]\n [31.259867 ]\n [21.683784 ]\n [17.12275  ]\n [10.643208 ]\n [11.784942 ]\n [19.202448 ]\n [12.767255 ]\n [13.325704 ]\n [13.91151  ]\n [12.794545 ]\n [12.62379  ]\n [15.85511  ]\n [18.568731 ]\n [19.168974 ]\n [13.481107 ]\n [28.445915 ]\n [14.26696  ]\n [20.178213 ]\n [17.557909 ]\n [13.583183 ]\n [17.411955 ]\n [16.942768 ]\n [12.688014 ]\n [24.936462 ]\n [15.552431 ]\n [13.586957 ]\n [13.740864 ]\n [26.095377 ]\n [11.250945 ]]", "q_t_selected": "[20.711733  13.917956  23.17358   14.972419  22.113247  14.397954\n  8.085576  12.036104  11.919744  19.604923  18.133022  12.352173\n 15.889054  12.295257  12.334726  17.740923  16.11542    7.6233687\n 14.100827  12.247663  23.27134   11.943975  18.475662  14.335171\n 13.144899  17.881132  14.309616  20.363218  10.676178  17.367598\n 17.691568  11.930263  12.501143  16.562515  10.897253  17.506643\n 19.990868   7.9464765 12.684742  22.996035  16.040232  14.808085\n 12.057245  13.922281  12.318167  15.147132  17.485632  13.913834\n 13.431848  12.765648  21.156218  15.626971  11.586027  12.22141\n 14.299389   7.946384  20.06794   11.999364  21.434856  21.82829\n 20.394648  21.584642  21.661825  16.718939  12.713587  12.558331\n 12.389166  15.549831  13.031851  23.446655  13.383912  12.485034\n 13.86869   12.8622    13.54796   13.4721155 14.100827  22.188324\n  5.8323374 11.575705  12.803902  16.252882  10.846713  14.373135\n 15.357093  12.216218   9.695072  15.213736  14.609835  12.873598\n 12.641505  12.431818  19.457762  19.466463  12.565896  12.739644\n 15.81864   17.606464  11.936194  13.303204  14.509262  15.708755\n 24.842394  13.669436  17.532747  14.352909  11.964029  16.775654\n 14.843203  17.812387  19.1794    26.73688   19.319613  15.181225\n 16.129606  11.892668  11.944358  14.276466  13.095264  14.09634\n 14.410367  10.071254  11.99141   11.9621    17.8955    22.477037\n 11.64659   14.036694  20.629904  12.337257  22.273197  14.571288\n 15.71503   10.1377735 12.665576  12.631244  10.044274  21.528364\n 12.6214075 12.949904  13.105942   9.640524  11.974998  10.169841\n 14.405379  13.603572  21.685472  13.3615885 11.242771  13.15768\n 20.28068   20.134237  14.433657  19.368248  12.099582  18.018803\n 14.8404045 16.801521  19.295229  19.464813  11.903966  18.829706\n 14.159584  12.126688  12.434054  14.577878  10.982211  13.866053\n 14.527685  13.811366  15.132528  17.9989    16.950014  13.833874\n 17.307825  11.6044035  7.4233155 11.079135  18.538513  14.614289\n 10.495901  12.870184  13.613143  12.806236  13.495609  11.897295\n 12.2413645 11.67289   17.367722  15.180584  10.002901  12.112224\n 12.415364  14.193472  11.987786  18.114084  23.745342  14.899343\n 14.234111  19.396975  14.305768  11.470394  12.260672  16.378216\n 14.779179  18.68242   13.06498   19.94098   12.399255  13.037206\n 22.189531  14.02314   12.526226  14.985254  13.167441  12.408474\n 21.584642  18.287432  12.031142  17.209633  11.381673  15.649883\n 17.25002   12.269634  21.449604  18.89298   12.317011  31.259867\n 21.683784  17.12275   10.643208  11.784942  19.202448  12.767255\n 13.325704  13.91151   12.794545  12.62379   15.85511   18.568731\n 19.168974  13.481107  28.445915  14.26696   20.178213  17.557909\n 13.583183  17.411955  16.942768  12.688014  24.936462  15.552431\n 13.586957  13.740864  26.095377  11.250945 ]", "twin_q_t_selected": "[21.341496  12.609231  22.592424  15.134894  22.0635    14.524384\n  7.9643445 12.436941  11.014945  19.599651  17.870024  11.563693\n 16.186346  11.0131855 11.566293  16.999876  15.394252   6.925308\n 15.221225  11.533911  23.762705  10.549008  19.486397  15.706142\n 14.42271   15.835581  14.765058  21.133331  10.989792  16.053642\n 17.096312  11.700909  12.332491  15.911049  11.691181  16.793213\n 20.471857   8.53797   12.928666  23.373585  14.732317  15.179287\n 11.581074  14.350072  11.790834  13.91678   16.97224   14.415435\n 12.747238  13.51152   22.043396  15.05186   11.645278  13.735915\n 13.003208   6.976383  20.44371   13.866307  21.68908   21.900488\n 19.55039   20.566454  19.637234  16.629366  12.240456  12.027564\n 12.000882  15.364879  13.714023  23.502068  14.958038  12.232818\n 13.337691  12.768956  13.029487  14.200647  15.221225  22.070007\n  6.3359485 11.988756  12.585114  15.869686   9.959849  13.227245\n 14.610242  12.51697   10.601272  15.4085655 14.096564  13.379407\n 12.876587  12.81015   18.22229   19.429037  11.723902  12.599286\n 16.266888  17.408897  12.278667  13.344256  15.934434  15.813038\n 23.061676  14.453174  17.466873  13.67395   12.785742  17.118284\n 14.680509  19.715134  20.225296  24.776342  18.727894  14.928332\n 15.742806  12.296313  11.057522  14.871044  12.477287  13.4987335\n 13.541009  10.989929  12.207729  12.27568   17.045048  22.790024\n 10.740729  13.373222  21.707857  12.313423  23.664526  15.837272\n 15.19723   10.867797  12.473561  13.236467  10.480774  21.636185\n 12.088814  12.061989  13.955345   9.941403  11.583505  10.049731\n 14.111347  12.883728  21.06047   13.07054   11.696359  12.897813\n 20.283756  19.531948  12.437461  19.169472  11.300608  16.318945\n 15.367807  18.075504  19.197077  19.384817  12.782112  18.64692\n 14.580236  12.383575  11.595167  13.552417  10.876258  14.769826\n 13.58594   13.711218  15.030755  17.566177  17.789015  14.099677\n 18.101837  11.29908    7.5727286 11.995222  18.506058  13.844085\n 10.501497  13.054583  14.071241  12.479498  13.335553  12.056703\n 11.879954  11.427545  17.70313   14.772601  10.934135  12.217052\n 12.577309  14.11668   13.086209  17.736473  24.172451  14.72147\n 14.674309  19.10503   13.600115  12.404889  11.678311  17.431644\n 15.041018  18.28587   13.835125  21.67092   12.340747  13.200509\n 22.551336  15.361903  12.142123  15.837213  12.292207  12.246665\n 20.566454  18.218798  13.0916    17.229403  10.132877  15.534507\n 18.239883  12.442573  22.285383  19.021023  12.811543  33.375793\n 20.7777    18.684546  11.311596  13.269203  18.058214  13.625512\n 13.460791  13.761966  13.154819  12.611722  16.30262   18.655537\n 18.585188  12.839482  28.250792  14.267223  18.870815  17.439169\n 13.141653  17.645002  16.910063  12.485877  24.5653    14.263966\n 14.215028  14.112437  25.307928  11.4597225]", "q_t_selected_target": "[20.053085  14.092171  22.746395  16.417536  24.659864  15.909787\n  4.8909225 12.020855  11.064286  20.320616  19.650734  12.359626\n 16.701172  10.2008915 12.365699  16.127796  16.350937   7.5165024\n 14.215413  11.031055  24.480076  10.002242  18.108864  13.685769\n 13.79325   17.068445  15.423626  19.67403   12.79198   16.575626\n 16.123617  13.712823  12.272699  13.528603   9.620568  15.971376\n 21.67206    8.441221  12.356845  21.649172  16.933546  14.931447\n 11.721566  14.247333  13.157282  14.881636  19.832886  15.922145\n 14.622894  14.366681  21.295563  14.547663  10.527068  13.279321\n 13.666194   8.39421   18.557343  12.75328   23.010164  20.995136\n 21.193138  19.15148   21.23902   17.471302  12.424655  12.474984\n 12.697193  15.530523  14.953199  23.02384   13.397882  12.588169\n 14.298524  13.517036  12.349222  14.05749   14.759625  20.808626\n  7.9986634 11.985008  13.418522  17.89459   10.068933  12.408462\n 14.978446  12.345481  10.51374   15.076425  14.01491   13.262889\n 11.122072  13.549268  17.233465  20.479153  10.728886  14.309349\n 14.093109  16.616007  14.518291  13.99233   16.684853  16.642288\n 23.195156  14.495893  16.475578  13.697945  12.528808  18.713205\n 16.41722   20.280447  19.410423  24.225437  19.372475  15.254188\n 14.8442135 12.092309   9.601366  13.889669  13.87892   13.189474\n 14.691888   9.564872  12.141853  12.355823  16.342077  22.091095\n 11.041998  14.194234  22.146618  13.160916  22.80475   13.242251\n 13.769636  10.043897  11.059822  14.190335  11.615124  22.404099\n 11.744779  12.152512  12.4473095  8.513983  10.964383   9.064577\n 14.529272  13.39674   22.833618  11.366241  10.944131  11.594399\n 21.798403  19.030437  13.470473  19.454039  10.397101  18.411062\n 17.018888  18.519133  18.648005  20.249296   9.994613  17.90027\n 13.680035  13.041963  10.7438    15.837074  12.601781  13.718323\n 13.515865  14.898905  15.284312  17.189104  17.860912  14.31574\n 16.871943   9.713158   4.7676353 10.199811  15.200131  15.572877\n  9.476804  13.213375  13.317114  12.630364  14.768932  11.408657\n 11.645317  11.660633  18.62624   15.047832  11.074437  12.117454\n 13.995217  14.654289  11.9264555 17.517605  22.130741  13.867831\n 14.25523   20.437613  14.950648  11.912215  11.2331705 17.30285\n 14.250686  16.878     12.862967  20.422335  13.195597  13.461117\n 22.05529   14.212043  13.213906  17.902357  10.767518  11.4839\n 19.67125   18.65699   12.831309  17.745333  12.988579  16.996471\n 18.171883  11.768846  21.697458  18.239769  11.010066  31.812325\n 23.935438  18.924034  11.573011  11.802354  16.607197  12.223469\n 12.340344  13.722457  12.65126   13.334991  14.826746  16.919073\n 16.989286  12.689927  29.869179  12.401683  19.909006  18.498621\n 12.726317  17.93682   17.363846  12.372494  25.10628   15.117717\n 12.014641  12.525262  27.857979   9.613318 ]", "q_tp1_best_masked": "[19.513935  14.639141  23.0856    17.655895  24.687786  16.310545\n  6.690301  12.70564   12.058239  19.80754   18.983353  12.259022\n 17.414083  12.122093  12.452012  16.265368  18.144104   8.876326\n 14.508907  11.8989525 24.937386  11.948062  18.509306  14.034284\n 14.255052  17.255943  14.939888  19.251902  13.71231   18.219513\n 15.581522  13.841516  11.892096  13.835823   9.861384  16.361217\n 23.837648  10.60264   13.021782  21.261894  18.636711  15.205157\n 11.109817  14.650001  14.184658  14.438774  19.957664  16.753342\n 14.527745  14.998263  21.658361  14.722792  11.431783  14.198952\n 14.077827  10.472406  19.210197  13.655118  24.286966  21.084118\n 22.790794  19.147783  22.736332  17.03672   12.820322  12.849078\n 13.861031  16.038187  15.520947  22.782305  14.858295  13.15026\n 15.091416  14.722381  12.538648  14.777082  15.058617  20.54789\n  9.085989  12.5231495 13.68477   16.965797  11.819651  13.06643\n 15.826701  12.109111  12.292189  16.054611  13.973814  14.0113945\n 10.974038  14.179488  17.289495  20.783693  13.236956  16.707287\n 13.632452  17.021095  15.956013  14.417527  16.29935   17.188133\n 22.563276  13.591395  16.984148  13.888753  12.604917  18.257769\n 18.171682  22.011917  19.17245   22.656557  19.9684    15.1756735\n 14.9785595 13.376792  10.653137  15.2220745 14.113639  13.960638\n 16.005892  11.470568  12.720203  12.943232  16.664593  21.93036\n 11.494538  14.989213  23.572268  13.766631  22.704163  13.153013\n 14.4690075 11.158533  10.263027  14.108676  12.991612  22.276419\n 12.519862  13.163258  11.931307   9.983972  12.796736   9.974561\n 14.318138  14.673039  21.938416  12.78818   12.75394   11.268182\n 21.271505  19.342546  14.563759  19.954401  11.764958  19.993437\n 17.636986  19.621407  18.357943  20.730083  10.030916  19.335905\n 13.110994  13.096155  11.733239  16.944304  13.181033  15.194353\n 14.115934  14.735829  14.733027  18.080555  18.123823  13.362152\n 17.909237  10.678987   6.645693  11.2178335 14.478595  14.838906\n 10.553856  13.591155  14.392685  12.62349   15.345978  12.606551\n 11.672509  11.541336  18.412447  15.395386  11.902424  13.70602\n 14.886792  13.966013  12.29636   18.487226  22.262074  14.444649\n 15.039868  20.172642  15.028777  13.044201  12.673317  17.855574\n 13.730903  17.449556  13.496434  21.263197  13.120942  14.723879\n 22.382792  14.070478  13.431086  17.985622  11.489494  11.597374\n 19.672804  18.587656  12.346081  17.26348   14.117035  17.421846\n 18.34031   13.335768  21.765532  18.69993   12.174331  30.066217\n 22.448034  18.857193  12.014501  12.271491  17.071081  12.832757\n 12.471538  14.358365  13.38884   14.650942  15.459322  18.220207\n 17.413729  13.079467  29.63652   13.870076  19.4942    19.861296\n 15.3649025 18.771843  16.959673  14.1038685 26.040405  15.736975\n 13.387678  12.7254505 27.526016  10.651725 ]", "policy_t": "[[-0.79453135  0.0055964   0.99323416  0.9889624   0.9815774   0.8367913 ]\n [ 0.91661286 -0.32841748 -0.86437964  0.7129587   0.37142932  0.15421271]\n [-0.2116543   0.35791814 -0.5675011  -0.91519296 -0.8335829  -0.09676838]\n ...\n [-0.09415549 -0.8089135   0.936327   -0.5878262  -0.80750835  0.6133367 ]\n [-0.9420394   0.6424928  -0.68030334  0.822109    0.66845775  0.32062602]\n [-0.05383885 -0.7141832  -0.35695648  0.8288436   0.730175    0.97138464]]", "td_error": "[0.97352886 0.82857704 0.2905779  1.3638792  2.5714912  1.4486184\n 3.1340377  0.21566772 0.45239925 0.7183285  1.6492109  0.40169287\n 0.6634717  1.4533296  0.41518927 1.2426033  0.5961013  0.34903026\n 0.5601988  0.8597312  0.9630537  1.2442498  0.8721657  1.3348875\n 0.6389055  1.0227757  0.8862891  1.0742445  1.9589949  0.65697765\n 1.2703228  1.8972373  0.14411783 2.7081795  1.6736488  1.1785517\n 1.4406977  0.29574656 0.44985914 1.5356379  1.5472717  0.18560076\n 0.23808575 0.21389532 1.1027813  0.6151757  2.6039495  1.7575107\n 1.5333514  1.228097   0.4435892  0.7917528  1.0885844  0.7572527\n 0.64809036 0.9328263  1.6984825  0.9334717  1.4481964  0.86925316\n 1.2206192  1.9240685  1.0122957  0.79714966 0.23656559 0.26538324\n 0.50216913 0.09247637 1.5802627  0.45052147 0.7870631  0.22924328\n 0.6953335  0.70145845 0.9395013  0.36426592 0.5601988  1.3205395\n 1.9145205  0.2065258  0.7240143  1.8333063  0.44343185 1.3917284\n 0.37342548 0.15037584 0.45309973 0.23472595 0.33828974 0.25290442\n 1.6369739  0.9282837  1.6065607  1.0314026  1.4160132  1.639884\n 1.9496546  0.89167404 2.4108605  0.6685996  1.4630046  0.881392\n 0.8903589  0.43458748 1.0242319  0.33947945 0.41085625 1.7662363\n 1.6553636  1.5166864  0.52294827 1.5311737  0.3487215  0.19940901\n 1.0919929  0.20182276 1.8995738  0.68408585 1.0926437  0.60806274\n 0.7161999  0.9657197  0.10815954 0.23693275 1.1281967  0.54243565\n 0.45293045 0.48927593 0.9777374  0.83557606 0.6956644  1.9620285\n 1.6864939  0.45888853 1.5097466  1.2564797  1.3525996  0.8218241\n 0.610332   0.4439578  1.083334   1.2769809  0.81486845 1.0452089\n 0.27090883 0.35992193 1.4606466  1.849823   0.525434   1.4333467\n 1.5161848  0.8026552  0.9980979  0.18517876 1.3029933  1.2421885\n 1.9147825  1.0806198  0.59814835 0.824481   2.3484263  0.83804226\n 0.6898756  0.7868309  1.2708106  1.7719269  1.6725464  0.5996165\n 0.54094744 1.1376128  0.20267057 0.59343433 0.49139786 0.3489642\n 0.8328886  1.738584   2.7303867  1.3373675  3.322154   1.3436899\n 1.0218954  0.25099182 0.5250783  0.16336918 1.3533511  0.56834173\n 0.41534233 0.12267256 1.0908136  0.20399141 0.6059189  0.05241442\n 1.4988809  0.49921322 0.6105423  0.40767384 1.8281555  0.942575\n 0.22009897 1.1866102  0.9977069  0.4672475  0.736321   0.5267143\n 0.6594124  1.6061449  0.5870857  0.86496925 0.82559586 0.3422594\n 0.3151436  0.6693816  0.87973166 2.4911237  1.962306   0.8436694\n 1.4042988  0.40387535 0.5302291  0.525815   2.2313037  1.4042764\n 0.49493217 0.58725786 0.4178896  0.7172327  1.5542111  1.0579634\n 2.7046957  1.0203867  0.59560966 0.74213076 2.0231342  0.9729147\n 1.0529027  0.11428118 0.32342148 0.71723557 1.2521191  1.6930609\n 1.8877945  0.47036743 1.5208254  1.8654089  0.6536989  1.000082\n 0.63610077 0.40834045 0.43743038 0.21445179 0.35539818 0.64423275\n 1.8863516  1.4013886  2.1563263  1.7420154 ]", "mean_td_error": 0.9820653200149536, "actor_loss": -15.763326644897461, "critic_loss": 0.727653980255127, "alpha_loss": -16.40143585205078, "alpha_value": 0.055852778255939484, "target_entropy": -6, "mean_q": 15.19624137878418, "max_q": 31.25986671447754, "min_q": 5.832337379455566, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 20000, "episodes_total": 20, "training_iteration": 20, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_14-58-04", "timestamp": 1587049084, "time_this_iter_s": 86.54830002784729, "time_total_s": 892.1347918510437, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 892.1347918510437, "timesteps_since_restore": 20000, "iterations_since_restore": 20, "perf": {"cpu_util_percent": 92.20205479452055, "ram_util_percent": 11.429452054794522}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -5.807225019476597, "episode_reward_min": -128.77419241997436, "episode_reward_mean": -56.24925737673137, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-83.16421875802665, -24.84243904865425, -41.14019322544369, -128.77419241997436, -83.28676981889441, -49.78267788012257, -88.0727330744594, -21.55184852160226, -5.807225019476597, -36.07027600065957], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26351823416282694, "mean_processing_ms": 0.1199386047613982, "mean_inference_ms": 1.178330788281514}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -97.0099034780624, "episode_reward_min": -234.58962540688248, "episode_reward_mean": -172.09617335307115, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-170.21091672483763, -229.43520271527103, -234.58962540688248, -97.0099034780624, -129.23521844030233], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.32949931846190905, "mean_processing_ms": 0.44673468011820966, "mean_inference_ms": 1.6092580113216275}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 10500, "num_steps_trained": 2814464, "num_steps_sampled": 21000, "sample_time_ms": 2.881, "replay_time_ms": 21.949, "grad_time_ms": 55.647, "update_time_ms": 0.005, "opt_peak_throughput": 4600.413, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[20.855385 ]\n [13.585246 ]\n [ 6.4979463]\n [11.709359 ]\n [15.016956 ]\n [19.992527 ]\n [16.977438 ]\n [21.184587 ]\n [13.768494 ]\n [15.709326 ]\n [15.515892 ]\n [12.963182 ]\n [14.028849 ]\n [ 9.528717 ]\n [11.931804 ]\n [11.387302 ]\n [11.013734 ]\n [ 9.27363  ]\n [15.563497 ]\n [20.489117 ]\n [13.490251 ]\n [16.92017  ]\n [15.128682 ]\n [ 8.723311 ]\n [10.452448 ]\n [24.181904 ]\n [22.869616 ]\n [22.106754 ]\n [13.969622 ]\n [11.818942 ]\n [ 8.007875 ]\n [ 9.213255 ]\n [11.576342 ]\n [17.018648 ]\n [15.46814  ]\n [13.687692 ]\n [12.061711 ]\n [10.531617 ]\n [14.910024 ]\n [15.444735 ]\n [12.187611 ]\n [ 9.799523 ]\n [16.945541 ]\n [11.282577 ]\n [13.288037 ]\n [13.205269 ]\n [20.91765  ]\n [21.11519  ]\n [13.944286 ]\n [15.465495 ]\n [15.162269 ]\n [11.319048 ]\n [15.189697 ]\n [15.551271 ]\n [13.96636  ]\n [ 7.125673 ]\n [21.341162 ]\n [11.953613 ]\n [17.740637 ]\n [18.739412 ]\n [10.8032875]\n [20.128315 ]\n [18.55893  ]\n [15.001136 ]\n [19.651133 ]\n [16.718243 ]\n [12.149837 ]\n [11.933252 ]\n [15.194101 ]\n [ 9.063374 ]\n [20.12452  ]\n [11.992789 ]\n [12.125317 ]\n [21.024378 ]\n [12.238475 ]\n [ 7.59339  ]\n [12.310204 ]\n [12.528072 ]\n [10.325459 ]\n [11.698766 ]\n [12.29705  ]\n [10.103237 ]\n [12.59891  ]\n [12.532647 ]\n [18.822924 ]\n [13.786247 ]\n [20.81246  ]\n [18.972694 ]\n [19.409039 ]\n [10.449876 ]\n [10.188017 ]\n [23.794558 ]\n [20.201803 ]\n [15.035712 ]\n [19.144371 ]\n [13.809155 ]\n [12.103774 ]\n [18.251158 ]\n [11.068942 ]\n [16.249823 ]\n [13.524973 ]\n [15.980124 ]\n [11.068942 ]\n [15.939825 ]\n [21.185219 ]\n [13.889473 ]\n [18.225418 ]\n [12.628183 ]\n [10.15987  ]\n [21.626917 ]\n [17.570452 ]\n [ 8.989952 ]\n [11.71349  ]\n [12.395376 ]\n [17.2303   ]\n [12.733822 ]\n [16.400097 ]\n [ 7.9435635]\n [13.50659  ]\n [12.415812 ]\n [12.778495 ]\n [12.836177 ]\n [16.297966 ]\n [13.720409 ]\n [15.387675 ]\n [13.748959 ]\n [14.752095 ]\n [11.809112 ]\n [10.320982 ]\n [18.215439 ]\n [15.409059 ]\n [19.75273  ]\n [10.832206 ]\n [20.192904 ]\n [ 9.660572 ]\n [17.99817  ]\n [24.438314 ]\n [16.426092 ]\n [14.173022 ]\n [18.182362 ]\n [12.388443 ]\n [20.860466 ]\n [11.807474 ]\n [24.751436 ]\n [ 9.937964 ]\n [17.984398 ]\n [23.339655 ]\n [11.53005  ]\n [17.952263 ]\n [12.29705  ]\n [18.542831 ]\n [11.768082 ]\n [24.589357 ]\n [17.281385 ]\n [ 9.009799 ]\n [13.239161 ]\n [11.293826 ]\n [12.163601 ]\n [21.248394 ]\n [14.503391 ]\n [25.277294 ]\n [22.684774 ]\n [11.517502 ]\n [21.823801 ]\n [12.587597 ]\n [12.763603 ]\n [22.863716 ]\n [10.683498 ]\n [12.759001 ]\n [31.934618 ]\n [19.54199  ]\n [13.109157 ]\n [14.006025 ]\n [15.961924 ]\n [11.345538 ]\n [14.309511 ]\n [18.485605 ]\n [14.221362 ]\n [10.40386  ]\n [17.78602  ]\n [14.356891 ]\n [12.781886 ]\n [ 9.424765 ]\n [14.782292 ]\n [15.822641 ]\n [14.586732 ]\n [10.280665 ]\n [12.813412 ]\n [11.476109 ]\n [ 9.784282 ]\n [27.027967 ]\n [14.030167 ]\n [17.380478 ]\n [13.290137 ]\n [13.095779 ]\n [15.109809 ]\n [14.797262 ]\n [12.499103 ]\n [13.555505 ]\n [11.790681 ]\n [14.708547 ]\n [18.28233  ]\n [11.804867 ]\n [19.947132 ]\n [13.881247 ]\n [11.743142 ]\n [15.267374 ]\n [22.462528 ]\n [21.805437 ]\n [11.011898 ]\n [12.904662 ]\n [ 6.5068064]\n [23.31281  ]\n [18.39078  ]\n [12.969085 ]\n [14.915147 ]\n [11.693427 ]\n [13.241855 ]\n [15.572571 ]\n [13.184875 ]\n [16.932693 ]\n [20.481197 ]\n [16.2539   ]\n [15.534868 ]\n [25.359955 ]\n [18.39078  ]\n [23.351278 ]\n [12.370278 ]\n [12.982991 ]\n [18.086155 ]\n [ 8.049048 ]\n [ 9.637996 ]\n [19.295465 ]\n [16.436579 ]\n [16.01865  ]\n [22.044403 ]\n [ 9.4287815]\n [12.859218 ]\n [18.20997  ]\n [27.740545 ]\n [20.047504 ]\n [12.637122 ]\n [18.546377 ]\n [11.130482 ]\n [28.475618 ]\n [26.464666 ]\n [12.723265 ]\n [14.377447 ]\n [16.340984 ]\n [11.936191 ]\n [14.826882 ]\n [11.554087 ]\n [14.336174 ]\n [11.234664 ]\n [13.209856 ]\n [12.364939 ]]", "q_t_selected": "[20.855385  13.585246   6.4979463 11.709359  15.016956  19.992527\n 16.977438  21.184587  13.768494  15.709326  15.515892  12.963182\n 14.028849   9.528717  11.931804  11.387302  11.013734   9.27363\n 15.563497  20.489117  13.490251  16.92017   15.128682   8.723311\n 10.452448  24.181904  22.869616  22.106754  13.969622  11.818942\n  8.007875   9.213255  11.576342  17.018648  15.46814   13.687692\n 12.061711  10.531617  14.910024  15.444735  12.187611   9.799523\n 16.945541  11.282577  13.288037  13.205269  20.91765   21.11519\n 13.944286  15.465495  15.162269  11.319048  15.189697  15.551271\n 13.96636    7.125673  21.341162  11.953613  17.740637  18.739412\n 10.8032875 20.128315  18.55893   15.001136  19.651133  16.718243\n 12.149837  11.933252  15.194101   9.063374  20.12452   11.992789\n 12.125317  21.024378  12.238475   7.59339   12.310204  12.528072\n 10.325459  11.698766  12.29705   10.103237  12.59891   12.532647\n 18.822924  13.786247  20.81246   18.972694  19.409039  10.449876\n 10.188017  23.794558  20.201803  15.035712  19.144371  13.809155\n 12.103774  18.251158  11.068942  16.249823  13.524973  15.980124\n 11.068942  15.939825  21.185219  13.889473  18.225418  12.628183\n 10.15987   21.626917  17.570452   8.989952  11.71349   12.395376\n 17.2303    12.733822  16.400097   7.9435635 13.50659   12.415812\n 12.778495  12.836177  16.297966  13.720409  15.387675  13.748959\n 14.752095  11.809112  10.320982  18.215439  15.409059  19.75273\n 10.832206  20.192904   9.660572  17.99817   24.438314  16.426092\n 14.173022  18.182362  12.388443  20.860466  11.807474  24.751436\n  9.937964  17.984398  23.339655  11.53005   17.952263  12.29705\n 18.542831  11.768082  24.589357  17.281385   9.009799  13.239161\n 11.293826  12.163601  21.248394  14.503391  25.277294  22.684774\n 11.517502  21.823801  12.587597  12.763603  22.863716  10.683498\n 12.759001  31.934618  19.54199   13.109157  14.006025  15.961924\n 11.345538  14.309511  18.485605  14.221362  10.40386   17.78602\n 14.356891  12.781886   9.424765  14.782292  15.822641  14.586732\n 10.280665  12.813412  11.476109   9.784282  27.027967  14.030167\n 17.380478  13.290137  13.095779  15.109809  14.797262  12.499103\n 13.555505  11.790681  14.708547  18.28233   11.804867  19.947132\n 13.881247  11.743142  15.267374  22.462528  21.805437  11.011898\n 12.904662   6.5068064 23.31281   18.39078   12.969085  14.915147\n 11.693427  13.241855  15.572571  13.184875  16.932693  20.481197\n 16.2539    15.534868  25.359955  18.39078   23.351278  12.370278\n 12.982991  18.086155   8.049048   9.637996  19.295465  16.436579\n 16.01865   22.044403   9.4287815 12.859218  18.20997   27.740545\n 20.047504  12.637122  18.546377  11.130482  28.475618  26.464666\n 12.723265  14.377447  16.340984  11.936191  14.826882  11.554087\n 14.336174  11.234664  13.209856  12.364939 ]", "twin_q_t_selected": "[20.050072  12.646199   6.212268  11.519094  13.877722  19.51526\n 18.0599    21.52568   12.294432  14.212913  14.312134  13.035939\n 12.6502     9.580025  10.757769  11.802568  11.298905   8.711938\n 16.454576  20.527689  12.9564495 17.173931  15.662724   7.6351714\n  9.199664  22.74808   22.41279   22.608124  13.762617  12.152888\n  7.3649545  8.4382925 11.043109  17.195662  15.727541  13.333301\n 12.109155   9.915241  13.796947  15.927407  11.916736   9.097478\n 16.946808  11.927711  12.613554  13.339089  21.533966  20.639565\n 13.890842  14.053016  15.789566  11.186527  16.034218  15.286296\n 12.138979   7.512053  21.150555  11.436589  17.413305  17.084621\n 10.254548  19.589674  18.196268  15.281427  18.809643  14.841851\n 12.31068   11.461149  15.81749    8.798267  19.125313  12.124503\n 11.720166  19.53684   10.741515   6.7112813 10.78303   12.52317\n  9.311938  12.726116  11.866625   9.747608  11.262446  12.247534\n 16.606672  12.635686  20.947952  17.762846  17.982468   9.960757\n 10.338008  22.392246  20.073196  15.546246  18.540936  12.108734\n 11.90648   16.96122   10.01534   15.694049  13.689116  16.18998\n 10.01534   15.507888  20.48791   13.1014385 15.963404  12.886538\n  9.537711  21.44133   18.449057   8.797766  12.290021  12.667611\n 15.886106  11.745721  15.113418   8.06734   12.854878  12.5919285\n 13.364529  13.117571  15.5336    13.779844  14.419493  13.820783\n 13.476713  12.153134  11.083717  17.029926  14.675743  20.923101\n 10.413991  20.077166  10.342678  17.985336  23.134722  16.036417\n 13.37577   16.170853  11.675616  19.300903  11.896418  24.528437\n  9.808179  16.950811  24.097994  11.368559  17.796183  11.866625\n 18.937702  10.437908  23.842384  17.502665   9.936488  12.987844\n 10.943758  11.718455  20.109787  14.855969  24.29186   21.636713\n 10.759402  20.875166  12.792642  11.923969  21.838882  10.413182\n 13.719099  31.334402  19.445606  11.444571  14.307757  15.268212\n 11.361186  12.961628  16.822176  14.333636   8.573985  17.9832\n 14.900174  12.619729   9.078631  13.279014  14.506925  13.7584915\n 10.897164  13.415523  10.852638  10.409907  26.743963  14.751257\n 15.919328  13.130819  11.612606  16.129921  14.700437  12.399015\n 13.943648  10.978378  13.721672  17.657627  11.452174  22.001057\n 14.631964  11.608752  16.292591  22.824314  21.723448  11.709629\n 13.41367    5.9754877 23.24944   18.67933   13.268731  14.084292\n 11.855212  13.536229  14.896589  12.432072  16.337484  20.47851\n 14.287154  15.304331  24.676489  18.67933   22.714382  13.048757\n 12.111584  18.284946   6.9891715  9.611019  17.705873  15.164387\n 15.544531  21.481405  10.678837  12.943976  18.463446  26.169256\n 20.256512  12.842358  18.778084  10.918634  27.952705  26.688719\n 12.246213  15.4811125 15.947847  12.45782   14.709233  11.344037\n 14.711856  11.462699  12.36959   12.422353 ]", "q_t_selected_target": "[20.683767  14.670647   7.349603  10.747131  15.033826  20.541765\n 18.848328  21.748117  11.725648  16.412184  14.440239  13.626846\n 13.818764   8.9781    11.168514  11.584404  10.678899   8.371094\n 15.696779  21.20462   12.329233  17.26125   16.128119  10.255651\n 10.641843  24.443615  24.823246  20.672974  15.229405  11.307786\n  8.962276   9.302035  11.042476  17.737125  16.858076  12.304565\n 11.866035  10.672101  15.946542  17.730032   9.877163  10.066484\n 17.793766  11.274056  12.29046   12.275425  20.871157  24.500902\n 15.038446  13.23341   18.186062  11.893679  14.158772  14.801571\n 12.358366   7.115841  22.339157  11.923356  16.958517  18.457067\n 10.907948  21.37304   17.891565  14.547488  20.47399   16.287588\n 11.220184  13.338902  16.924683   9.804229  21.520874  12.480277\n 12.623171  19.991451  12.131925   8.9523945 12.2431755 12.489792\n  9.295342  12.118779  11.410154  10.768339  13.223046  12.316\n 18.848965  12.4250965 21.14762   20.158003  20.016369  12.098612\n 10.268685  25.580105  18.994501  14.169957  18.36039   12.750976\n 12.089361  19.290808  10.672241  15.214575  15.066104  16.312487\n 10.819964  14.053125  22.78649   12.107216  18.93002   12.060706\n  9.397803  21.0465    17.390184   8.319906  11.573155  11.562401\n 18.831095  14.188467  15.367659   7.398454  12.872936  12.481872\n 13.324747  12.314921  14.832458  13.145352  14.729907  15.563096\n 13.958471  11.6741495 10.247823  19.492027  15.377864  20.852856\n 10.982531  21.17781   10.55721   17.916409  24.471722  16.910374\n 15.655845  16.61992   12.783004  21.185226  11.85767   22.541185\n 10.849467  17.693035  24.538267  10.570944  18.129366  11.308127\n 18.943542  11.16112   24.213755  19.68587    8.786685  14.729736\n 12.363374  12.435516  18.087814  15.596049  25.169119  23.47112\n 12.33936   21.675789  13.711243  13.941999  23.050364  10.303014\n 15.725133  30.650635  20.876387  11.769812  13.319091  16.860535\n 11.478353  13.230863  17.699121  15.329892  10.385433  16.549341\n 13.944849  11.8831625  8.555447  14.409612  16.369886  14.032259\n  9.602035  14.3587475 12.783022   9.569291  27.9829    12.048646\n 18.310062  13.817804  12.774277  15.580497  16.533907  12.879873\n 14.0307045 11.390596  15.558449  18.364944  11.462899  22.698835\n 13.344832  12.535724  16.34738   23.761604  22.70186   11.255937\n 10.315086   5.045285  20.961685  17.507927  14.439338  13.610434\n 14.086099  14.534278  14.315192  10.369912  15.142457  20.208616\n 17.324734  16.099312  27.266582  17.795534  25.638212  14.739809\n 13.850086  18.902578   7.769793   9.781052  17.540546  15.8078165\n 17.348219  20.086489   8.989605  11.596197  19.030115  27.878897\n 18.39384   11.371362  19.84569   11.89281   27.638264  27.934977\n 12.855413  13.773118  14.973357  11.168423  14.18961   11.6243\n 15.798612  12.095143  13.331258  11.774945 ]", "q_tp1_best_masked": "[22.034513  15.938086   8.661657  11.357959  14.617039  21.1495\n 21.055502  20.489029  12.513197  17.318462  15.100811  13.41811\n 13.837851   9.463979  11.303423  12.952408  11.97175    9.338888\n 16.865242  21.314528  13.0188265 17.726091  16.975183  12.391574\n 11.138709  25.33022   26.20681   20.743082  15.453319  10.550406\n 10.5625    11.466924  12.233812  17.566076  17.772144  12.88538\n 12.109559  11.497974  17.244802  16.89488   10.860227  12.757478\n 17.8544    11.132918  11.843954  12.294436  21.724295  25.253315\n 14.208063  13.313778  20.26156   11.922523  15.074596  14.816894\n 15.115348   8.35736   22.788393  12.224173  18.896198  18.394236\n 11.647094  22.196556  17.729273  14.842764  20.590742  16.089327\n 12.585197  13.061812  17.688765  10.444841  23.356524  12.433752\n 14.097413  20.000816  12.434638  10.312451  12.2141905 13.593615\n 11.55369   12.790996  11.487073  11.956485  14.70496   12.669804\n 20.263851  12.213017  22.20762   21.07908   20.410032  12.531774\n 11.36659   26.061201  18.872007  14.955639  19.168     13.553429\n 11.728547  19.7333    11.107761  15.932202  14.637849  16.173813\n 11.256977  14.892377  23.292501  12.008946  20.78441   12.847415\n 10.963715  21.714966  17.666378   9.391806  11.759451  12.599931\n 20.495108  14.813803  16.303572   8.625619  13.212309  12.681622\n 12.886004  12.310693  17.430899  14.470239  15.117291  15.998745\n 14.597745  12.347798  11.286744  18.934793  16.046114  20.761219\n 11.401059  20.918413  11.512661  17.852238  23.920021  17.217548\n 14.874138  17.34377   12.340288  21.872787  12.95694   21.835562\n 11.152404  17.625181  24.404692  12.17876   19.237478  11.384015\n 18.508251  11.745446  23.863703  21.378      9.883748  16.444477\n 12.785625  13.156986  18.936008  14.1451235 25.780489  25.049267\n 12.867414  21.761     13.964203  14.620195  22.84675   11.253039\n 14.670934  30.04132   20.606016  11.533042  12.789898  17.14994\n 12.058186  12.047424  18.14121   14.884465  10.764408  16.12268\n 14.461742  13.182942  10.672008  15.39926   15.205358  14.686419\n 10.373433  13.710812  12.324094  10.977191  27.528854  11.749815\n 18.145262  12.832505  12.1173935 15.9594555 16.63567   12.855761\n 14.283164  11.36828   18.061342  19.022768  11.771625  23.723106\n 14.34543   12.002872  16.7321    24.244589  23.720947  11.945057\n 11.466472   7.180071  20.882824  17.214382  16.26725   14.152338\n 14.679733  14.921315  15.263833  12.336524  16.045666  20.716068\n 18.345512  16.562782  27.984158  17.504894  25.0712    16.3852\n 15.863444  20.329893   9.3940525 11.063738  17.670252  16.45726\n 18.229086  19.647188  10.758285  13.370533  18.343946  29.295122\n 19.106703  11.708792  20.718683  11.610026  28.211855  27.354641\n 13.00397   13.819336  15.812853  11.528985  13.880906  11.5779085\n 16.361425  12.466579  13.899925  11.714275 ]", "policy_t": "[[-0.7972719   0.982826    0.29464602  0.9732313   0.7754842   0.5538573 ]\n [-0.86288023 -0.04913992 -0.9005488   0.49330807 -0.6555588  -0.8942433 ]\n [-0.9145543  -0.61039144 -0.3334757   0.9479531   0.91626847  0.5514238 ]\n ...\n [ 0.34500802  0.6097524   0.674325   -0.8747505  -0.58438206 -0.3571449 ]\n [ 0.9697552   0.8187903   0.89571023 -0.5683914   0.09484291  0.3273021 ]\n [ 0.40732813  0.7827623   0.9324262  -0.91574997  0.16062534  0.6143904 ]]", "td_error": "[0.40265656 1.554924   0.9944961  0.8670955  0.5864868  0.78787136\n 1.3296585  0.39298344 1.3058147  1.4510646  0.6018791  0.6272855\n 0.6893244  0.57627106 0.58701754 0.20763302 0.4774208  0.6216903\n 0.44553995 0.69621754 0.8941169  0.21419907 0.7324157  2.07641\n 0.81578684 0.9786234  2.182043   1.6844654  1.363286   0.6781292\n 1.2758615  0.47626162 0.26724958 0.62997055 1.2602358  1.2059307\n 0.21939754 0.44867182 1.5930567  2.043961   2.1750102  0.6179838\n 0.8475914  0.3310871  0.660336   0.99675417 0.35465145 3.6235247\n 1.120882   1.5258455  2.7101445  0.6408911  1.4531851  0.6172128\n 0.91369057 0.203022   1.0932989  0.25851202 0.618454   0.82739544\n 0.37902975 1.5140448  0.48603344 0.5937934  1.2436018  0.9381957\n 1.0100741  1.6417017  1.4188871  0.8734083  1.895958   0.42163086\n 0.70042944 0.7437687  0.74847984 1.8000588  0.763587   0.03582954\n 0.52335596 0.5136752  0.6716833  0.8429165  1.2923679  0.14255667\n 1.1341667  0.7858701  0.26741314 1.7902327  1.3206158  1.8932953\n 0.07499552 2.486703   1.1429987  1.1210217  0.48226357 0.85021067\n 0.09864712 1.684619   0.5268011  0.75736094 1.4590597  0.22743464\n 0.5268011  1.6707311  1.9499254  1.3882399  1.8356085  0.6966543\n 0.45098734 0.48762417 0.6195698  0.5739527  0.42859983 0.96909285\n 2.2728915  1.9486957  0.64333963 0.6069975  0.32585573 0.08805847\n 0.2930169  0.6619525  1.0833254  0.6047745  0.48409128 1.7782254\n 0.637691   0.30697346 0.4545269  1.8693447  0.36665773 0.585186\n 0.35943222 1.0427761  0.5555849  0.07534504 0.68520355 0.6791191\n 1.8814487  1.0057545  0.7509742  1.1045418  0.04447174 2.098751\n 0.9763956  0.51679325 0.81944275 0.87836075 0.25514317 0.77371025\n 0.20327568 0.66508675 0.37348652 2.2938442  0.6864586  1.6162338\n 1.2445817  0.49448824 2.5912762  0.91636896 0.4927168  1.3103771\n 1.2009082  0.47431755 1.0211234  1.5982132  0.69906425 0.24532652\n 2.486083   0.9838753  1.3825884  0.83229303 0.8378005  1.2454667\n 0.12499046 0.6739416  0.83171463 1.052393   0.9149375  1.335269\n 0.6836834  0.8176451  0.6962514  0.75163937 1.2051034  0.4141202\n 0.9868803  1.2442803  1.6186485  0.5278034  1.0969353  2.3420658\n 1.6601596  0.60732603 0.7415867  0.510056   1.7850575  0.43081427\n 0.28112793 0.4061513  1.3433394  0.39496613 0.1763463  1.724741\n 0.9117727  0.8597765  0.5673971  1.1181831  0.937418   0.3488655\n 2.8440795  1.1958618  2.31944    1.0271282  1.3204298  0.88928604\n 2.311779   1.145236   0.9193878  2.438561   1.4926319  0.27123737\n 2.0542064  0.6797123  2.2483606  0.739521   2.605382   2.0302916\n 1.3027987  0.71702766 0.52993846 0.15654421 0.96012306 0.636096\n 1.5666285  1.6764154  1.0642042  1.3053999  0.69340706 0.923996\n 1.7581673  1.3683782  1.1834593  0.8682518  0.5758982  1.358284\n 0.3706746  1.1561618  1.1710587  1.0285826  0.5784483  0.17523813\n 1.2745967  0.74646187 0.5415349  0.6187005 ]", "mean_td_error": 0.9808669686317444, "actor_loss": -15.526989936828613, "critic_loss": 0.7367011308670044, "alpha_loss": -16.136306762695312, "alpha_value": 0.04314098879694939, "target_entropy": -6, "mean_q": 15.141371726989746, "max_q": 31.93461799621582, "min_q": 6.497946262359619, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 21000, "episodes_total": 21, "training_iteration": 21, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_14-59-45", "timestamp": 1587049185, "time_this_iter_s": 86.34145331382751, "time_total_s": 978.4762451648712, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 978.4762451648712, "timesteps_since_restore": 21000, "iterations_since_restore": 21, "perf": {"cpu_util_percent": 92.26180555555555, "ram_util_percent": 11.5}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -57.68519877936203, "episode_reward_min": -179.50855091734607, "episode_reward_mean": -124.2191640656669, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-117.46461461747894, -130.66396790326164, -119.85723092785194, -179.50855091734607, -152.7796024370482, -143.68683083472482, -139.56986086064447, -131.67216577474602, -57.68519877936203, -69.3036176042049], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26322745135176506, "mean_processing_ms": 0.11984532484812352, "mean_inference_ms": 1.1778958508476998}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -97.0099034780624, "episode_reward_min": -234.58962540688248, "episode_reward_mean": -159.5216777378115, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-166.56272463897275, -234.58962540688248, -97.0099034780624, -129.23521844030233, -170.21091672483763], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.3337327414362444, "mean_processing_ms": 0.4503450292984866, "mean_inference_ms": 1.6321454308812278}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 11000, "num_steps_trained": 3070464, "num_steps_sampled": 22000, "sample_time_ms": 2.99, "replay_time_ms": 21.475, "grad_time_ms": 51.482, "update_time_ms": 0.005, "opt_peak_throughput": 4972.595, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[12.1732   ]\n [11.278841 ]\n [11.045451 ]\n [11.032828 ]\n [30.230034 ]\n [10.705702 ]\n [12.183099 ]\n [17.270815 ]\n [12.621977 ]\n [15.2997265]\n [15.578991 ]\n [14.181137 ]\n [ 9.1163845]\n [10.78049  ]\n [18.94942  ]\n [18.70731  ]\n [11.998927 ]\n [11.349654 ]\n [17.65353  ]\n [11.507555 ]\n [30.991844 ]\n [15.836424 ]\n [ 7.305913 ]\n [11.275516 ]\n [11.950385 ]\n [10.169383 ]\n [20.854723 ]\n [11.581171 ]\n [22.170982 ]\n [10.888686 ]\n [26.49281  ]\n [11.774314 ]\n [11.016342 ]\n [11.4936075]\n [14.242845 ]\n [11.747808 ]\n [11.99394  ]\n [26.620546 ]\n [21.088129 ]\n [ 9.950829 ]\n [11.396555 ]\n [10.347545 ]\n [22.74162  ]\n [12.188277 ]\n [15.753092 ]\n [10.201273 ]\n [13.271709 ]\n [20.601727 ]\n [12.938026 ]\n [19.18138  ]\n [12.895395 ]\n [21.4903   ]\n [20.044512 ]\n [13.016605 ]\n [28.503254 ]\n [15.57619  ]\n [12.5924635]\n [11.596167 ]\n [16.177856 ]\n [18.068752 ]\n [11.361466 ]\n [16.899302 ]\n [24.540567 ]\n [ 8.377538 ]\n [14.256331 ]\n [18.808426 ]\n [15.05988  ]\n [15.402986 ]\n [ 9.9303   ]\n [ 9.060626 ]\n [17.965414 ]\n [ 9.446119 ]\n [24.848368 ]\n [16.517797 ]\n [22.837402 ]\n [24.399765 ]\n [12.889256 ]\n [12.6526575]\n [16.62185  ]\n [21.180965 ]\n [17.925056 ]\n [12.565663 ]\n [11.872372 ]\n [25.780516 ]\n [16.67246  ]\n [18.68784  ]\n [12.610283 ]\n [11.733114 ]\n [21.686426 ]\n [27.350594 ]\n [ 1.1030791]\n [12.965578 ]\n [10.839565 ]\n [ 9.891893 ]\n [11.489792 ]\n [24.458054 ]\n [21.416594 ]\n [13.030859 ]\n [10.131149 ]\n [11.406116 ]\n [22.398436 ]\n [12.990386 ]\n [18.628468 ]\n [12.472639 ]\n [23.422363 ]\n [15.613134 ]\n [24.13828  ]\n [ 8.715348 ]\n [15.900797 ]\n [10.404818 ]\n [11.180261 ]\n [28.4027   ]\n [10.903673 ]\n [14.910315 ]\n [10.162233 ]\n [26.932058 ]\n [18.495848 ]\n [11.287434 ]\n [13.430107 ]\n [12.881636 ]\n [19.053228 ]\n [ 8.89984  ]\n [15.733425 ]\n [18.697107 ]\n [12.250168 ]\n [11.756041 ]\n [12.799183 ]\n [11.674654 ]\n [11.543797 ]\n [12.375557 ]\n [13.2521   ]\n [21.412071 ]\n [17.88147  ]\n [17.194548 ]\n [15.171153 ]\n [ 9.937405 ]\n [18.525602 ]\n [26.755377 ]\n [15.221365 ]\n [21.546665 ]\n [10.153449 ]\n [23.478271 ]\n [11.118931 ]\n [15.126622 ]\n [11.482094 ]\n [16.911146 ]\n [12.519965 ]\n [11.689876 ]\n [11.770443 ]\n [15.547968 ]\n [11.678238 ]\n [17.417173 ]\n [15.429129 ]\n [20.486036 ]\n [28.00849  ]\n [ 9.497642 ]\n [ 8.158925 ]\n [15.575726 ]\n [13.2478895]\n [11.269151 ]\n [13.839197 ]\n [18.841097 ]\n [ 9.728944 ]\n [10.482451 ]\n [15.209537 ]\n [11.590463 ]\n [14.304782 ]\n [23.627    ]\n [30.508768 ]\n [10.649613 ]\n [13.924191 ]\n [21.785086 ]\n [13.052039 ]\n [13.918769 ]\n [12.699641 ]\n [10.209376 ]\n [12.311696 ]\n [20.540976 ]\n [13.509487 ]\n [27.750896 ]\n [11.172868 ]\n [11.558855 ]\n [18.697432 ]\n [11.883431 ]\n [13.447543 ]\n [14.705225 ]\n [11.251081 ]\n [20.692886 ]\n [22.659712 ]\n [ 9.8859415]\n [ 8.980016 ]\n [16.132587 ]\n [28.511103 ]\n [24.78649  ]\n [22.714846 ]\n [22.998005 ]\n [16.290003 ]\n [24.274132 ]\n [12.610283 ]\n [12.755438 ]\n [15.117829 ]\n [10.18791  ]\n [14.784442 ]\n [20.532314 ]\n [16.733093 ]\n [12.448251 ]\n [12.988894 ]\n [19.794775 ]\n [14.828156 ]\n [11.239122 ]\n [11.047108 ]\n [11.37696  ]\n [19.890703 ]\n [16.324684 ]\n [18.758757 ]\n [17.41884  ]\n [11.683313 ]\n [20.34774  ]\n [11.105517 ]\n [14.573096 ]\n [ 9.356431 ]\n [ 9.215252 ]\n [18.298222 ]\n [11.635696 ]\n [18.968857 ]\n [ 9.990064 ]\n [ 9.425035 ]\n [11.249606 ]\n [12.36709  ]\n [21.56616  ]\n [17.130451 ]\n [28.323923 ]\n [ 9.836817 ]\n [25.416409 ]\n [21.637278 ]\n [11.350309 ]\n [31.261473 ]\n [22.425867 ]\n [17.490662 ]\n [10.482451 ]\n [11.58162  ]\n [13.181471 ]\n [13.239009 ]\n [10.331348 ]\n [14.373925 ]\n [12.605751 ]\n [10.472789 ]\n [23.501577 ]\n [14.567815 ]\n [15.816662 ]\n [ 9.994713 ]\n [29.353981 ]\n [24.182901 ]\n [11.102441 ]\n [13.011772 ]\n [11.383559 ]]", "q_t_selected": "[12.1732    11.278841  11.045451  11.032828  30.230034  10.705702\n 12.183099  17.270815  12.621977  15.2997265 15.578991  14.181137\n  9.1163845 10.78049   18.94942   18.70731   11.998927  11.349654\n 17.65353   11.507555  30.991844  15.836424   7.305913  11.275516\n 11.950385  10.169383  20.854723  11.581171  22.170982  10.888686\n 26.49281   11.774314  11.016342  11.4936075 14.242845  11.747808\n 11.99394   26.620546  21.088129   9.950829  11.396555  10.347545\n 22.74162   12.188277  15.753092  10.201273  13.271709  20.601727\n 12.938026  19.18138   12.895395  21.4903    20.044512  13.016605\n 28.503254  15.57619   12.5924635 11.596167  16.177856  18.068752\n 11.361466  16.899302  24.540567   8.377538  14.256331  18.808426\n 15.05988   15.402986   9.9303     9.060626  17.965414   9.446119\n 24.848368  16.517797  22.837402  24.399765  12.889256  12.6526575\n 16.62185   21.180965  17.925056  12.565663  11.872372  25.780516\n 16.67246   18.68784   12.610283  11.733114  21.686426  27.350594\n  1.1030791 12.965578  10.839565   9.891893  11.489792  24.458054\n 21.416594  13.030859  10.131149  11.406116  22.398436  12.990386\n 18.628468  12.472639  23.422363  15.613134  24.13828    8.715348\n 15.900797  10.404818  11.180261  28.4027    10.903673  14.910315\n 10.162233  26.932058  18.495848  11.287434  13.430107  12.881636\n 19.053228   8.89984   15.733425  18.697107  12.250168  11.756041\n 12.799183  11.674654  11.543797  12.375557  13.2521    21.412071\n 17.88147   17.194548  15.171153   9.937405  18.525602  26.755377\n 15.221365  21.546665  10.153449  23.478271  11.118931  15.126622\n 11.482094  16.911146  12.519965  11.689876  11.770443  15.547968\n 11.678238  17.417173  15.429129  20.486036  28.00849    9.497642\n  8.158925  15.575726  13.2478895 11.269151  13.839197  18.841097\n  9.728944  10.482451  15.209537  11.590463  14.304782  23.627\n 30.508768  10.649613  13.924191  21.785086  13.052039  13.918769\n 12.699641  10.209376  12.311696  20.540976  13.509487  27.750896\n 11.172868  11.558855  18.697432  11.883431  13.447543  14.705225\n 11.251081  20.692886  22.659712   9.8859415  8.980016  16.132587\n 28.511103  24.78649   22.714846  22.998005  16.290003  24.274132\n 12.610283  12.755438  15.117829  10.18791   14.784442  20.532314\n 16.733093  12.448251  12.988894  19.794775  14.828156  11.239122\n 11.047108  11.37696   19.890703  16.324684  18.758757  17.41884\n 11.683313  20.34774   11.105517  14.573096   9.356431   9.215252\n 18.298222  11.635696  18.968857   9.990064   9.425035  11.249606\n 12.36709   21.56616   17.130451  28.323923   9.836817  25.416409\n 21.637278  11.350309  31.261473  22.425867  17.490662  10.482451\n 11.58162   13.181471  13.239009  10.331348  14.373925  12.605751\n 10.472789  23.501577  14.567815  15.816662   9.994713  29.353981\n 24.182901  11.102441  13.011772  11.383559 ]", "twin_q_t_selected": "[10.947143  11.049981  11.489443  10.906335  29.561527  11.206387\n 11.634773  17.008904  12.6717205 15.478003  14.938808  13.741425\n  9.8342285 11.279583  19.036104  17.383234  11.203073  10.798684\n 17.08545   11.401276  30.91452   15.781862   6.458175   9.654754\n 11.008172   9.707988  20.059391  12.704869  22.957016  11.087858\n 26.671663  11.204444  11.496337  10.527464  13.737336  12.019106\n 10.602131  26.792591  21.466417   9.606416  10.787313   9.504014\n 22.067072  11.508699  16.333189  10.160793  12.786101  20.78195\n 13.236636  18.677103  12.45537   19.647491  19.424202  13.165066\n 29.62886   16.722973  13.329353  10.881163  15.618336  17.216808\n 10.986986  17.360565  25.514534   8.185553  13.764335  18.600348\n 15.86554   13.884066  10.705506   8.940294  17.75147    9.151736\n 25.627077  17.017776  23.885992  24.592808  12.3420725 13.236473\n 16.19161   21.573616  17.783047  12.051085  11.313884  26.387785\n 17.332289  17.322983  11.506204  12.931795  21.839731  27.968596\n  2.4642584 12.127528  11.080249  10.233641  11.912615  25.130732\n 22.467167  12.120102  10.465682  10.698905  20.402496  12.699846\n 19.033607  13.674082  21.117712  15.446725  23.55139    8.863082\n 16.855331  10.301328  10.212353  29.328398  10.323063  14.187911\n 10.381684  26.008625  19.172342  11.566238  13.093234  12.462893\n 19.960472   9.734953  15.219217  17.627447  11.280137   9.573573\n 11.955144  11.487959  11.190659  11.464663  13.357299  21.626009\n 19.229572  17.24721   15.294055  10.489021  19.177094  27.618832\n 15.817382  22.53727   10.146991  23.702757  10.415757  15.770572\n 12.221726  17.579863  12.832615  12.619256  10.926789  14.496269\n 11.982824  17.561335  14.4968195 19.219952  26.895937   8.953653\n  7.71977   14.915372  13.420835  12.3781395 13.054456  19.011303\n 10.601311  11.30158   16.158054  11.502395  12.433655  24.508835\n 30.864683   8.962383  14.082897  22.78936   12.724795  13.66729\n 11.870196   9.367456  13.2553425 19.839443  13.560373  28.234003\n 11.630165  11.281504  19.369802  11.805266  13.029948  14.435004\n 10.087721  20.239685  22.873344   9.369653   7.8207245 15.695888\n 28.401062  24.362413  22.524998  23.376078  15.8704605 24.949903\n 11.506204  12.103285  15.276959   9.342381  13.56752   20.596703\n 17.156572  12.337093  11.75339   19.094347  14.786687   9.629063\n 11.887908  11.902182  20.54041   17.098022  18.820766  16.790802\n 11.165168  20.7324    10.631422  14.250118   8.368184  10.054218\n 19.265207  11.427525  18.386417  10.377968   9.546559  10.381651\n 13.15599   22.085794  16.609634  29.93891   10.079123  25.594992\n 20.441925  11.0180235 32.1622    24.057457  19.669909  11.30158\n 10.880832  12.475003  13.488675   9.851098  15.336532  12.349608\n  9.045254  23.220398  13.115009  16.211363  10.150964  28.727018\n 24.103064  11.637555  11.527489  11.2424135]", "q_t_selected_target": "[10.953816  12.821979  11.694718   9.797875  29.1749    11.564192\n 11.228569  17.869307  11.303148  16.010618  14.561492  13.6470785\n 10.460414   9.764483  17.368917  19.798494  11.71362   10.546119\n 18.639988  11.080088  32.29198   14.198591   5.7096233 11.307158\n 11.94603    9.018544  20.433224  11.169942  23.760603  10.210731\n 25.797438  12.683113  12.444431   9.854865  14.702326  13.81879\n 10.690211  27.979074  21.8152     9.745795  10.766082   9.850737\n 25.407694  12.435771  17.258163  11.457499  11.940654  21.59916\n 15.350197  18.596525  13.215136  22.77852   19.569225  11.34374\n 30.351048  15.273124  13.175871  11.665567  14.505973  17.5919\n 10.244481  17.3766    25.242336   7.720307  14.604333  17.189154\n 13.970221  15.356114  10.889572  10.305375  19.4141     9.752208\n 24.474848  17.451609  23.732975  27.61485   10.727799  12.279689\n 17.915306  22.663261  16.736746  12.938154  11.94588   25.544235\n 17.770555  18.25574   12.372142  12.492565  22.62333   28.245901\n  1.6878028 11.498702  10.678425   9.877577  12.443694  25.643301\n 19.388008  13.77557   10.465571  12.04595   21.018309  13.416869\n 16.902298  11.867771  21.38798   14.511516  25.39614   11.217206\n 16.494051   6.7047625  9.818673  29.423895  11.562711  15.129161\n 10.555424  27.740957  18.03474   11.281754  13.524566  13.836304\n 19.586613   8.9968605 15.698584  18.480448  12.330063   9.808219\n 13.386168  10.976055  11.2227    10.832024  14.25175   20.631323\n 17.292282  20.51329   15.912087   8.423905  18.8422    26.216446\n 16.424997  20.878113  12.041294  24.01335   11.033146  17.517282\n 11.562182  18.147694  12.236922  11.345035  11.1393585 13.63511\n 10.6606865 15.358432  15.657893  19.422054  27.516623   8.048746\n  9.383653  16.010735  12.146862  11.504483  13.348717  18.419935\n  8.490393   9.0749    15.798294  11.932866  13.196673  23.527252\n 32.729565  10.017537  14.020693  22.762959  13.438854  14.637155\n 11.578511  10.749616  13.096837  19.53767   11.460047  28.647432\n 11.810833  10.801544  20.044249  13.053402  14.690957  12.842864\n 11.006189  18.406067  19.711357  11.170169   8.773479  14.287691\n 26.086739  24.924881  24.1683    24.350569  15.228968  25.723415\n 12.271702  12.670555  15.948311   8.390196  11.732639  20.839993\n 17.545494  12.088462  12.501497  19.967905  15.413211   9.880089\n 11.741494  12.281477  21.294262  15.988315  16.689358  18.862625\n 12.966592  20.699963  10.592746  11.040605   8.07831   11.162084\n 17.682701  11.120533  17.107765   9.805718   8.982557  10.34304\n 11.316487  20.961302  17.412298  28.352898   9.587824  24.30689\n 21.294432  12.015766  31.851168  22.539385  19.411325   9.209985\n 11.8527975 12.965696  11.134735   8.852126  14.933156  13.898209\n  9.462989  23.158722  13.324502  16.324371  12.088738  29.953815\n 22.690392  11.608244  12.236186  10.932647 ]", "q_tp1_best_masked": "[10.400944  14.059455  11.93988   11.054014  27.75358   12.065087\n 11.269694  17.833029  10.803406  16.862408  15.779418  13.688697\n 11.041503   9.990554  17.156202  18.969496  12.601219  11.246942\n 20.360323  11.018262  31.330153  15.913299   7.9894915 12.845368\n 12.831018  10.296788  22.025143  12.57091   22.937595   9.831362\n 27.045168  12.497656  14.017931   9.088458  16.595867  14.38566\n 10.919082  27.319212  21.673529  10.113187  11.419932  10.520545\n 26.169266  12.342962  17.568851  11.301568  13.781435  22.36523\n 17.73608   18.457561  12.988552  22.905893  20.398413  13.082521\n 30.884497  16.051693  12.25804   12.27676   15.538375  17.391756\n  9.750761  18.066637  26.05977    8.951245  14.465495  17.929241\n 15.527966  13.913498  11.243729  11.636738  19.890839  11.00719\n 23.701431  18.493345  24.434784  28.621618  11.796229  12.470453\n 17.915922  23.723907  18.24647   12.584298  12.448839  24.19655\n 18.40634   17.80856   12.453506  12.941228  21.981598  28.265226\n  5.032793  12.519487  10.797539  10.825174  13.3354645 26.14692\n 19.612545  14.527295  10.684925  12.95705   21.575909  13.545397\n 17.78706   11.07313   22.099154  14.956068  26.407953  12.452148\n 16.675503   7.8354273 11.443443  28.886383  11.763211  14.240718\n 11.058924  28.39188   18.136961  11.122946  13.861672  14.488077\n 20.92968   11.0048485 16.792114  18.647696  13.751944  11.692963\n 12.529732  11.789775  10.9194    10.646445  15.013472  20.726624\n 17.698496  20.406914  16.830704   8.291227  19.23733   27.505686\n 16.37164   21.860123  14.890508  24.210209  11.58895   17.770773\n 11.369063  18.983402  11.647929  12.001126  12.261626  14.113691\n 10.237492  15.037014  16.305382  20.51679   27.603241   9.4779625\n 10.565203  16.707016  13.096163  11.155763  13.497575  20.514051\n  9.107464   9.899218  16.106743  12.90121   12.379197  23.161253\n 33.329445  11.626315  13.936866  22.92308   12.546362  15.969389\n 12.557489  11.240662  12.750897  19.759888  11.613725  29.769196\n 12.112734  12.180091  19.962952  13.419162  15.535893  12.740836\n 12.7690325 17.759102  19.343819  12.779076  10.154419  14.690048\n 25.303211  25.107634  24.140446  25.328226  15.455344  26.080929\n 12.352053  12.873761  15.772765   9.814804  11.284321  22.389492\n 18.321836  12.0972185 12.300501  22.037973  14.730756  10.336532\n 11.821215  12.978966  20.74021   16.0959    17.128984  19.706686\n 12.249499  20.437994  12.171062  11.246555  10.258065  12.044668\n 18.855686  12.183714  17.178932  10.794966  10.542572  11.403799\n 10.396679  21.5617    18.137815  28.5168    10.182254  24.363976\n 21.160847  12.014364  32.16338   22.576403  20.019653  10.035666\n 12.488513  12.069101  11.32303    9.665491  15.748119  14.879043\n 10.888579  24.727646  14.41004   16.34226   12.633188  29.860455\n 23.032408  12.221945  11.953567  11.928611 ]", "policy_t": "[[-0.88245666  0.98847866  0.9441531  -0.50376195 -0.42773867  0.48355997]\n [-0.33077705 -0.7391907   0.40485632 -0.7599393   0.50199234 -0.5191648 ]\n [ 0.98203945 -0.10127836  0.9017575  -0.5730564   0.82420564  0.3259765 ]\n ...\n [-0.7088488  -0.54179734 -0.9207744  -0.7557041  -0.71338385  0.29332936]\n [ 0.2917738   0.95788205 -0.1805442  -0.7398672  -0.84607476 -0.19870532]\n [ 0.6818423  -0.3876567  -0.89697117  0.871266   -0.01132864  0.7683892 ]]", "td_error": "[0.6130285  1.6575675  0.42727137 1.1717062  0.7208805  0.6081476\n 0.680367   0.72944736 1.3437004  0.6217537  0.6974077  0.3142023\n 0.9851074  1.265553   1.6238451  1.7532225  0.39792728 0.5280504\n 1.2704983  0.37432766 1.3387985  1.6105518  1.1724207  0.84202385\n 0.47110653 0.9201412  0.39766598 0.97307825 1.1966038  0.77754164\n 0.7847986  1.1937342  1.1880918  1.1556706  0.71223545 1.9353333\n 0.69590473 1.2725058  0.5379276  0.1722064  0.3258524  0.42176533\n 3.0033484  0.58728266 1.215023   1.2764654  1.0882516  0.907321\n 2.2628655  0.332716   0.53975296 2.2096252  0.3101549  1.7470951\n 1.2849903  0.8764577  0.36844492 0.42690277 1.3921232  0.42597198\n 0.9297452  0.24666691 0.4869833  0.5612383  0.59399986 1.515233\n 1.4924893  0.75946    0.5716691  1.304915   1.5556583  0.45327997\n 0.7628746  0.6838217  0.52429485 3.1185627  1.8878646  0.66487646\n 1.5085764  1.2859707  1.1173058  0.6297798  0.3527522  0.5399151\n 0.76818085 0.68242836 0.5520396  0.59934044 0.8602505  0.5863066\n 0.6805897  1.0478511  0.28148222 0.1851902  0.74249077 0.8489084\n 2.553872   1.2000895  0.16726637 0.9934397  0.9979696  0.571753\n 1.9287395  1.2055893  1.1523256  1.018414   1.5513039  2.427991\n 0.47726727 3.6483102  0.8776336  0.5583458  0.9493427  0.5800481\n 0.2834649  1.2706156  0.79935455 0.14508247 0.2628951  1.1640396\n 0.45362186 0.4175563  0.25710392 0.5348301  0.5649104  1.0912337\n 1.0090041  0.6052513  0.17656898 1.0880861  0.9470506  0.88771725\n 1.2632389  3.2924118  0.6794834  1.7893076  0.32574558 0.9706583\n 0.9056239  1.1638546  1.8910742  0.42283535 0.35158682 2.0686855\n 0.3698163  0.90218925 0.43936777 0.8095312  0.42182684 1.3870087\n 1.1698446  2.1308222  0.6949191  0.63304234 0.5562763  1.1769013\n 1.4443052  0.76518583 1.1875     0.5544944  0.3923707  0.5062647\n 1.6747346  1.8171163  0.4742589  0.38643742 0.93556356 0.5406656\n 2.042839   0.84361506 0.07935286 0.5021372  0.550437   0.8441253\n 0.70640755 0.9611993  0.47182322 0.65253925 2.0748835  0.65498257\n 0.40931654 0.6186352  1.0106316  1.209053   1.4522114  1.7272506\n 0.5816803  2.0602188  3.055171   1.5423717  0.57964563 1.6265464\n 2.3693438  0.35042953 1.548379   1.1635275  0.851264   1.1113977\n 0.5520396  0.3260765  0.7509165  1.3749495  2.4433417  0.27548409\n 0.6006613  0.3042102  0.6177521  0.52334404 0.6057892  0.80502987\n 0.42040014 0.64190626 1.0787058  0.7230387  2.1004038  1.7578039\n 1.5423512  0.19233036 0.27572393 3.3710027  0.78399754 1.5273485\n 1.0990133  0.4110775  1.5698719  0.37829733 0.5032401  0.47258806\n 1.4450526  0.8646755  0.5422554  0.8074932  0.3701458  1.1988096\n 0.5976763  0.8315997  0.4503641  0.81579494 1.0896235  1.6820312\n 0.62157154 0.3532338  2.229107   1.2390971  0.48130322 1.4205289\n 0.7137675  0.20226574 0.72640276 0.310359   2.0159001  0.9133158\n 1.452591   0.26755714 0.7421417  0.38033962]", "mean_td_error": 0.9659215211868286, "actor_loss": -16.00674819946289, "critic_loss": 0.716620922088623, "alpha_loss": -12.932543754577637, "alpha_value": 0.03361665830016136, "target_entropy": -6, "mean_q": 15.569100379943848, "max_q": 31.261472702026367, "min_q": 1.103079080581665, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 22000, "episodes_total": 22, "training_iteration": 22, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_15-01-28", "timestamp": 1587049288, "time_this_iter_s": 87.36350631713867, "time_total_s": 1065.83975148201, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 1065.83975148201, "timesteps_since_restore": 22000, "iterations_since_restore": 22, "perf": {"cpu_util_percent": 92.32671232876713, "ram_util_percent": 11.5}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -40.290339934463475, "episode_reward_min": -173.18230856759467, "episode_reward_mean": -107.76813759225558, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-87.02063084334823, -130.93089117629478, -117.02551721952013, -76.32105838648309, -40.290339934463475, -84.03627509544215, -167.85624698531896, -83.25356786392791, -117.76453985016245, -173.18230856759467], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26381941000027365, "mean_processing_ms": 0.11996123557319402, "mean_inference_ms": 1.1815791672985902}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -97.0099034780624, "episode_reward_min": -170.21091672483763, "episode_reward_mean": -136.65084994445849, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-120.23548644011734, -97.0099034780624, -129.23521844030233, -170.21091672483763, -166.56272463897275], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.33762283050676584, "mean_processing_ms": 0.45360863677333085, "mean_inference_ms": 1.6522045042937843}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 11500, "num_steps_trained": 3326464, "num_steps_sampled": 23000, "sample_time_ms": 2.876, "replay_time_ms": 22.573, "grad_time_ms": 58.792, "update_time_ms": 0.005, "opt_peak_throughput": 4354.364, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[11.25341  ]\n [10.494604 ]\n [16.755999 ]\n [ 8.52922  ]\n [16.43684  ]\n [ 8.689297 ]\n [10.655373 ]\n [ 9.117673 ]\n [13.060359 ]\n [12.49108  ]\n [10.126287 ]\n [10.945625 ]\n [11.746476 ]\n [ 8.746202 ]\n [ 9.369064 ]\n [17.809906 ]\n [23.050053 ]\n [11.459664 ]\n [11.863769 ]\n [16.404196 ]\n [12.228327 ]\n [11.0241165]\n [10.635048 ]\n [11.156507 ]\n [22.77603  ]\n [21.065147 ]\n [12.552766 ]\n [17.306944 ]\n [26.394062 ]\n [25.707016 ]\n [29.479202 ]\n [ 9.702997 ]\n [19.666595 ]\n [12.763011 ]\n [ 8.71609  ]\n [15.501305 ]\n [13.772229 ]\n [16.326962 ]\n [10.542847 ]\n [ 9.59984  ]\n [16.886917 ]\n [12.769634 ]\n [11.600783 ]\n [10.377181 ]\n [13.193966 ]\n [ 7.0377316]\n [12.250628 ]\n [18.531664 ]\n [ 9.651744 ]\n [12.850679 ]\n [17.039331 ]\n [11.113483 ]\n [18.700184 ]\n [18.872868 ]\n [15.450008 ]\n [20.987259 ]\n [17.088823 ]\n [ 9.196728 ]\n [19.388727 ]\n [14.039533 ]\n [18.070349 ]\n [24.962574 ]\n [ 9.062962 ]\n [19.01747  ]\n [12.109125 ]\n [ 7.006564 ]\n [17.052279 ]\n [10.698042 ]\n [24.485422 ]\n [ 7.3377166]\n [13.950848 ]\n [10.35539  ]\n [17.51242  ]\n [11.87113  ]\n [22.939638 ]\n [11.748794 ]\n [11.590694 ]\n [10.727467 ]\n [27.676105 ]\n [11.580027 ]\n [20.584303 ]\n [16.408133 ]\n [ 9.438293 ]\n [10.2241   ]\n [16.400583 ]\n [26.351326 ]\n [11.344324 ]\n [22.031961 ]\n [13.412862 ]\n [10.178105 ]\n [ 8.046979 ]\n [20.890532 ]\n [10.204841 ]\n [20.418777 ]\n [10.5495405]\n [10.665482 ]\n [28.69227  ]\n [10.953038 ]\n [ 9.772485 ]\n [26.811756 ]\n [25.66976  ]\n [15.956031 ]\n [15.812792 ]\n [11.683331 ]\n [21.666946 ]\n [ 8.507431 ]\n [14.508215 ]\n [17.435404 ]\n [18.463612 ]\n [18.810907 ]\n [25.295208 ]\n [11.185103 ]\n [11.432037 ]\n [ 9.91417  ]\n [11.401864 ]\n [25.664845 ]\n [10.197361 ]\n [19.828285 ]\n [23.515678 ]\n [18.279089 ]\n [15.837478 ]\n [18.22484  ]\n [12.390093 ]\n [25.792578 ]\n [14.10879  ]\n [14.31616  ]\n [27.601244 ]\n [10.2608385]\n [21.573587 ]\n [10.7773075]\n [14.52093  ]\n [17.832127 ]\n [ 9.810929 ]\n [10.796786 ]\n [10.555042 ]\n [16.835365 ]\n [21.720512 ]\n [17.18268  ]\n [10.895111 ]\n [13.644192 ]\n [22.654263 ]\n [28.040243 ]\n [10.688336 ]\n [25.38176  ]\n [ 9.302082 ]\n [11.813102 ]\n [16.696066 ]\n [17.842749 ]\n [11.750149 ]\n [21.295864 ]\n [ 9.548814 ]\n [10.8756275]\n [10.418993 ]\n [15.221876 ]\n [12.778289 ]\n [10.681181 ]\n [13.547392 ]\n [19.01747  ]\n [15.514222 ]\n [25.652468 ]\n [14.56514  ]\n [17.39488  ]\n [17.617037 ]\n [ 9.951702 ]\n [10.239131 ]\n [28.143639 ]\n [11.246725 ]\n [21.521235 ]\n [10.723289 ]\n [ 8.51213  ]\n [ 9.721735 ]\n [11.407793 ]\n [18.480976 ]\n [10.49946  ]\n [23.38263  ]\n [27.186832 ]\n [11.694101 ]\n [20.839315 ]\n [19.772781 ]\n [10.412104 ]\n [15.823625 ]\n [17.909222 ]\n [ 9.656441 ]\n [25.20648  ]\n [21.512142 ]\n [27.4098   ]\n [18.2555   ]\n [11.683331 ]\n [24.795792 ]\n [18.868223 ]\n [23.733797 ]\n [ 9.4624605]\n [13.368723 ]\n [ 9.157938 ]\n [ 8.930915 ]\n [23.38263  ]\n [24.330833 ]\n [11.214619 ]\n [18.006485 ]\n [21.883905 ]\n [10.076314 ]\n [29.454737 ]\n [18.87707  ]\n [12.6484165]\n [27.527332 ]\n [11.923727 ]\n [31.13287  ]\n [14.240462 ]\n [25.855225 ]\n [13.089886 ]\n [10.658448 ]\n [14.686849 ]\n [ 9.708821 ]\n [10.692668 ]\n [10.861175 ]\n [23.515434 ]\n [17.511015 ]\n [10.863415 ]\n [19.578848 ]\n [14.5847025]\n [11.019258 ]\n [13.003731 ]\n [23.389074 ]\n [14.73384  ]\n [13.595942 ]\n [18.426638 ]\n [ 9.172999 ]\n [ 8.776088 ]\n [22.544083 ]\n [21.93423  ]\n [17.066328 ]\n [15.318475 ]\n [ 5.637005 ]\n [17.181952 ]\n [16.327187 ]\n [19.134296 ]\n [ 9.790582 ]\n [ 8.34714  ]\n [ 9.368518 ]\n [10.700839 ]\n [10.900576 ]\n [23.129734 ]\n [17.91909  ]\n [ 9.431456 ]\n [20.281199 ]\n [ 9.349682 ]\n [17.221136 ]\n [15.47204  ]\n [ 8.503484 ]\n [12.2408695]\n [16.463367 ]\n [19.909594 ]\n [26.486153 ]\n [23.206388 ]\n [20.725801 ]\n [26.579819 ]]", "q_t_selected": "[11.25341   10.494604  16.755999   8.52922   16.43684    8.689297\n 10.655373   9.117673  13.060359  12.49108   10.126287  10.945625\n 11.746476   8.746202   9.369064  17.809906  23.050053  11.459664\n 11.863769  16.404196  12.228327  11.0241165 10.635048  11.156507\n 22.77603   21.065147  12.552766  17.306944  26.394062  25.707016\n 29.479202   9.702997  19.666595  12.763011   8.71609   15.501305\n 13.772229  16.326962  10.542847   9.59984   16.886917  12.769634\n 11.600783  10.377181  13.193966   7.0377316 12.250628  18.531664\n  9.651744  12.850679  17.039331  11.113483  18.700184  18.872868\n 15.450008  20.987259  17.088823   9.196728  19.388727  14.039533\n 18.070349  24.962574   9.062962  19.01747   12.109125   7.006564\n 17.052279  10.698042  24.485422   7.3377166 13.950848  10.35539\n 17.51242   11.87113   22.939638  11.748794  11.590694  10.727467\n 27.676105  11.580027  20.584303  16.408133   9.438293  10.2241\n 16.400583  26.351326  11.344324  22.031961  13.412862  10.178105\n  8.046979  20.890532  10.204841  20.418777  10.5495405 10.665482\n 28.69227   10.953038   9.772485  26.811756  25.66976   15.956031\n 15.812792  11.683331  21.666946   8.507431  14.508215  17.435404\n 18.463612  18.810907  25.295208  11.185103  11.432037   9.91417\n 11.401864  25.664845  10.197361  19.828285  23.515678  18.279089\n 15.837478  18.22484   12.390093  25.792578  14.10879   14.31616\n 27.601244  10.2608385 21.573587  10.7773075 14.52093   17.832127\n  9.810929  10.796786  10.555042  16.835365  21.720512  17.18268\n 10.895111  13.644192  22.654263  28.040243  10.688336  25.38176\n  9.302082  11.813102  16.696066  17.842749  11.750149  21.295864\n  9.548814  10.8756275 10.418993  15.221876  12.778289  10.681181\n 13.547392  19.01747   15.514222  25.652468  14.56514   17.39488\n 17.617037   9.951702  10.239131  28.143639  11.246725  21.521235\n 10.723289   8.51213    9.721735  11.407793  18.480976  10.49946\n 23.38263   27.186832  11.694101  20.839315  19.772781  10.412104\n 15.823625  17.909222   9.656441  25.20648   21.512142  27.4098\n 18.2555    11.683331  24.795792  18.868223  23.733797   9.4624605\n 13.368723   9.157938   8.930915  23.38263   24.330833  11.214619\n 18.006485  21.883905  10.076314  29.454737  18.87707   12.6484165\n 27.527332  11.923727  31.13287   14.240462  25.855225  13.089886\n 10.658448  14.686849   9.708821  10.692668  10.861175  23.515434\n 17.511015  10.863415  19.578848  14.5847025 11.019258  13.003731\n 23.389074  14.73384   13.595942  18.426638   9.172999   8.776088\n 22.544083  21.93423   17.066328  15.318475   5.637005  17.181952\n 16.327187  19.134296   9.790582   8.34714    9.368518  10.700839\n 10.900576  23.129734  17.91909    9.431456  20.281199   9.349682\n 17.221136  15.47204    8.503484  12.2408695 16.463367  19.909594\n 26.486153  23.206388  20.725801  26.579819 ]", "twin_q_t_selected": "[11.134463  10.957314  16.112217   9.373609  15.781188  10.069889\n 11.991413   8.385427  14.282474  12.996996  10.878512   9.523141\n 11.127222   8.089878  10.36331   18.691055  22.233982  11.531341\n 12.073852  16.699238  12.6708555 11.987744  11.136249  10.926674\n 23.24535   21.573196  12.14718   17.270046  25.491459  27.401482\n 29.971298   9.561329  19.241207  13.458498   9.182381  17.02422\n 13.687996  16.30288   10.34471    9.155456  16.38276   13.355617\n 12.097629   9.387473  13.164644   7.914729  12.626967  18.390575\n 10.78237   12.496076  16.399578  10.853453  20.464603  19.799011\n 16.018368  21.662155  17.097023  10.460496  20.688877  13.568082\n 16.57492   24.69151    9.965806  20.573433  12.593066   4.8236585\n 17.315334  10.795508  25.727402   9.693471  15.223662   9.733302\n 17.206245  12.543599  22.955906  12.201459  10.27031   10.442076\n 26.212933  11.319801  21.51672   17.718876   9.636467   9.635828\n 16.094727  25.337017  11.423646  22.119167  13.892038   9.906761\n  8.15706   21.5107    10.642547  19.604721  11.451502  11.813376\n 28.087837  12.384024  10.895689  27.50327   26.22839   16.36472\n 16.3392    11.074654  21.811443   9.552754  15.059857  18.126656\n 19.120964  19.843922  25.04301   11.149252  11.592914  10.389106\n 11.147228  26.245892  10.290479  20.388939  25.528881  18.528446\n 15.5390625 18.191189  11.775647  25.628464  14.677647  14.791215\n 26.865784  10.352676  21.71473   10.920412  15.781806  17.888275\n 10.5930195 11.263041  11.171744  16.130835  20.098675  17.632637\n 10.87972   14.057003  21.918034  28.980343  11.146979  25.577\n  9.05163   11.185152  16.539402  17.27763   12.905117  22.575184\n  9.315553  10.893331  10.669998  15.321909  12.880014  11.166478\n 13.849776  20.573433  16.158054  26.568626  15.189444  16.290165\n 17.750423   9.701525  10.034969  29.098404  10.599907  20.764217\n 10.338357   9.1889305  9.712735  10.965006  19.32903   12.065836\n 22.428314  27.289207  11.664127  20.293282  18.632793  10.685581\n 15.47456   18.402725  11.109885  23.834885  20.251324  27.411598\n 19.218212  11.074654  24.344189  19.858788  23.98278   10.247009\n 13.898954   9.352683   9.588442  22.428314  23.88913   11.717537\n 17.927746  21.222706  10.666117  28.832924  17.28777   12.046546\n 26.530148  11.375018  29.985233  14.514582  25.57246   13.8489895\n 10.727852  15.653787   8.72869   10.580224  10.826066  22.89188\n 17.246109  11.154348  19.531754  14.334887  11.107485  13.907763\n 23.427925  14.533751  13.651552  18.200127   9.4588     9.617558\n 21.834297  21.466602  18.462223  14.428971   5.741627  18.138412\n 16.995155  20.449718   9.626409   9.616047  10.567374  10.625842\n 11.455424  21.556835  17.415554  10.181967  20.44122    9.245552\n 17.465828  16.261307   7.6452084 13.8442335 16.357197  20.94292\n 26.532356  24.189865  21.413685  24.640839 ]", "q_t_selected_target": "[11.06783   10.391187  14.755982  10.569768  13.524676   9.845016\n 10.056902   9.148984  13.492324  12.062207   9.269048  12.225443\n 11.892186   9.677311  10.405447  16.596947  21.85556    9.549929\n 11.536886  15.778056  14.469232  10.868788   9.764732  10.219535\n 22.55001   19.021618  12.038096  21.06654   26.930984  28.118004\n 29.689087   9.196885  19.714018  13.290982   9.417856  15.790197\n 13.886361  14.661881  11.487646   9.620597  16.393026  12.672195\n 11.022914   9.424188  14.365114   8.026295  12.042554  17.752468\n  9.484887  13.296703  17.985638  10.123105  22.124342  19.601707\n 14.1546755 23.346285  17.632631   8.517763  20.943457  14.204661\n 16.811516  23.732368  10.944048  21.577179  11.075476   4.8084598\n 16.38546    8.894282  24.590696   9.7440815 13.239294   9.687317\n 17.522697  13.212346  25.353266  13.159013  10.781914   9.870132\n 26.824001  13.321636  22.155481  16.595045  10.67079   10.792257\n 17.085836  26.04575   11.573817  23.120527  15.448426   8.017712\n  8.517458  20.21354   11.512879  21.223932  10.341238  11.128546\n 27.658934  10.760183   8.618574  28.382948  26.203491  15.515349\n 17.582098  10.890059  23.319187   7.542226  13.170241  18.870829\n 18.795725  20.669502  23.12382    9.602646  10.6561365  8.405061\n 12.338763  27.976408  11.097941  19.338728  25.31724   18.012016\n 15.594042  17.053999  12.676916  25.571882  15.217491  17.14164\n 27.334707  11.508001  24.022526  11.078864  15.930683  16.315527\n 10.968535  10.697047  11.753551  17.08028   21.080568  15.07111\n 11.176996  16.87047   23.640083  27.765438  11.110068  26.76878\n 10.197926   9.461615  16.201597  17.916574  12.569277  21.768322\n  9.069149  11.627226  11.188486  16.482943  14.17247   12.215402\n 14.616529  21.624336  16.542015  24.26263   16.14432   18.398096\n 19.26649   10.119981  10.447962  27.397785  11.0324745 21.267523\n  8.950839   9.222084   8.415073  10.302088  19.919111  10.843836\n 23.51293   26.494745  10.235592  20.87319   20.515167  10.202358\n 17.599363  19.09441   10.836063  24.49793   22.222063  25.745735\n 18.930101  10.832405  23.985563  19.425095  25.422415  10.335015\n 14.856389   9.436675   9.77881   23.360802  22.447474  11.311535\n 21.105553  21.659786  11.216448  27.737839  18.728537  13.666409\n 29.846678  12.444947  30.705555  14.531908  27.661499  12.287992\n  9.933205  15.146361   9.960063  12.424713  10.744942  21.911928\n 16.915663  11.090428  21.50345   15.265773  10.900796  14.347539\n 22.111559  18.070372  14.556497  17.963974   9.460573   8.92084\n 21.403568  22.62752   19.359295  15.439823   7.2327595 15.289156\n 15.614774  15.776483  10.440553  10.250289   9.035809   9.898924\n 12.671784  20.828943  20.320705  10.355456  23.059     10.389538\n 17.457233  17.819767   8.911686  12.600627  16.640125  20.491825\n 27.592445  25.849558  19.943913  26.122269 ]", "q_tp1_best_masked": "[11.460249  11.114771  15.419522  11.324878  13.971819  10.707271\n 11.173922   9.733412  12.447812  12.945132  10.4146805 13.658499\n 12.830731   9.517954  11.112054  17.258917  21.4375     9.526353\n 11.187404  16.31336   14.18991   12.287684  10.244963  10.033591\n 23.827658  18.38087   13.058368  21.260902  26.218754  28.441063\n 27.763369  10.389239  20.517166  13.358577  10.926405  16.794638\n 13.3575115 15.557236  11.0964575  9.97547   16.925592  12.657686\n 10.369996  10.641448  16.940596   9.685776  12.5339365 17.847252\n 10.074876  12.512053  19.47445   11.966197  22.23356   19.663383\n 15.125282  24.780643  17.778988   9.2337475 21.30598   13.916791\n 16.637337  24.030659  12.167814  21.499517  11.53945    7.111481\n 17.463276   9.255818  24.569592  10.360938  13.520536  10.314332\n 18.410236  12.111369  26.149849  12.28242   10.449568  10.213147\n 28.039783  13.896314  22.99684   17.33572   11.959094  11.212642\n 17.581041  27.221382  12.335454  23.496616  17.560751   9.075181\n  9.082791  20.74292   11.898265  22.499014  11.107355  11.334301\n 26.7125    11.890203   8.93598   29.464691  26.30029   15.552803\n 17.262154  11.371051  23.614912   7.702443  13.976751  19.685698\n 20.02993   21.929113  23.364744   9.5049515 10.870703   9.914866\n 13.588048  26.997269  11.443928  19.063675  24.627697  18.604544\n 16.080984  17.218302  13.715592  25.855036  14.810539  17.661268\n 27.83639   10.915112  23.517616  11.683702  16.386185  16.464737\n 11.379757  10.90518   11.635694  18.779282  21.852753  15.343392\n 11.1433735 17.879847  23.546177  28.064718  11.442491  27.908632\n 11.069946  10.358444  16.579418  19.296621  13.183748  21.3811\n  9.315224  12.333424  11.21374   16.749424  15.255458  11.4292345\n 15.794053  21.547152  17.351595  23.753094  17.627895  20.929674\n 18.809101   9.733987  10.42749   29.265682  10.322282  21.858673\n 10.614679  10.383876   7.5915627 11.015938  21.166687  11.274391\n 24.03498   27.252686  10.831774  21.551449  21.500196  10.7589855\n 17.030087  19.336086  11.787862  25.232027  22.322716  25.100418\n 20.056524  11.312814  24.277252  20.64434   27.372568  10.225\n 15.735502   9.623242  11.080389  23.881317  21.859938  11.433872\n 22.174189  22.997366  11.979082  28.153488  19.183899  13.550234\n 31.498009  12.573822  31.292664  14.528503  27.91292   12.37807\n  9.421845  15.371903   9.604589  14.156707  11.622594  21.842361\n 17.240982  11.427801  20.264929  15.081258  11.235014  13.898415\n 23.751453  19.874249  15.253106  19.11739   10.374338   9.871238\n 21.344196  22.439009  18.578632  15.660138   9.936211  15.337184\n 16.689968  16.773146  11.417666  10.887104   9.299108   9.939162\n 13.2909565 21.23932   21.559612   9.568566  22.88188   10.504921\n 17.176956  18.350555   9.296047  13.943609  16.636961  20.661612\n 27.093536  28.057343  19.436958  25.518188 ]", "policy_t": "[[ 0.32562196  0.17961335 -0.9158036   0.40465593  0.76908183  0.702685  ]\n [ 0.5489111  -0.41200626  0.9241072   0.01180351 -0.14753234 -0.20486856]\n [ 0.01301432 -0.96440977 -0.549944   -0.42899203  0.9847014  -0.8623033 ]\n ...\n [ 0.24119687 -0.58069587 -0.850359   -0.8914686  -0.9083643  -0.34431863]\n [ 0.6262603  -0.24445736  0.11896777  0.932688    0.9255713   0.82228327]\n [ 0.99745214 -0.5403627   0.85251427  0.689623    0.75211716 -0.64105314]]", "td_error": "[0.12610674 0.3347721  1.6781254  1.6183538  2.5843377  0.6902962\n 1.2664909  0.39743376 0.6110573  0.6818309  1.2333522  1.9910598\n 0.45533705 1.2592707  0.5392599  1.6535339  0.78645706 1.9455738\n 0.43192387 0.77366066 2.0196404  0.63714266 1.1209159  0.82205534\n 0.46067905 2.297554   0.3118763  3.7780447  0.988224   1.563755\n 0.24604797 0.43527794 0.26011658 0.3477435  0.46862078 0.76145744\n 0.15624857 1.653039   1.0438676  0.24294901 0.252079   0.39042997\n 0.82629204 0.49485397 1.1858091  0.5500643  0.39624405 0.70865154\n 0.7321696  0.6233258  1.2661829  0.860363   2.5419483  0.46307182\n 1.5795126  2.0215778  0.53970814 1.3108487  0.9046545  0.4008541\n 0.74771404 1.0946732  1.4296641  1.7817278  1.27562    1.1066515\n 0.7983465  1.8524928  0.6209898  1.2284877  1.347961   0.35702896\n 0.16336441 1.0049815  2.4054937  1.1838865  0.660192   0.7146387\n 0.73158646 1.8717222  1.10497    0.65537167 1.1334095  0.86229324\n 0.8381815  0.50715446 0.18983221 1.0449629  1.7959762  2.0247216\n 0.41543865 0.98707676 1.0891857  1.212183   0.65928316 0.5739474\n 0.7311201  0.9083476  1.7155128  1.2254353  0.279315   0.64502573\n 1.5061026  0.4889326  1.5799923  1.4878669  1.6137948  1.0897989\n 0.32867622 1.3420877  2.04529    1.5645318  0.856339   1.7465773\n 1.0642171  2.02104    0.85402155 0.7698841  1.0066013  0.3917513\n 0.14920759 1.1540155  0.5940461  0.1386385  0.82427263 2.5879521\n 0.36773014 1.2012439  2.3783674  0.23000431 0.779315   1.5446739\n 0.76656103 0.3328662  0.8901572  0.59718037 0.8109188  2.3365488\n 0.28958082 3.0198727  1.3539352  0.7448549  0.22932148 1.2894001\n 1.0210695  2.0375123  0.41613674 0.35638523 0.57748413 0.6396599\n 0.36303425 0.74274683 0.6439905  1.21105    1.3433185  1.2915721\n 0.9179454  1.8288851  0.7058768  1.8479166  1.2670279  1.5555735\n 1.5827608  0.2933674  0.31091166 1.2232361  0.32340908 0.37850857\n 1.5799837  0.3715539  1.3021617  0.8843117  1.0141087  0.78318787\n 0.6074581  0.7432747  1.4435225  0.30689144 1.3123798  0.34648418\n 1.9502711  0.9384365  0.72672224 0.6857977  1.3403301  1.6649637\n 0.48135567 0.546587   0.5844269  0.49528217 1.564126   0.4802804\n 1.2225504  0.18136454 0.5191312  0.4771576  1.662508   0.25145912\n 3.1384373  0.33059978 0.8452325  1.4059916  0.7946501  1.3189273\n 2.8179379  0.79557467 0.5738182  0.15438604 1.9476566  1.1814461\n 0.7599454  0.483469   0.74130726 1.7882671  0.09867859 1.291729\n 0.4628992  0.1454668  1.9481497  0.8059783  0.16257524 0.8917923\n 1.2969408  3.4365764  0.93274975 0.34940815 0.14467335 0.42073488\n 0.78562164 0.92710304 1.5950193  0.5661001  1.5434434  2.371026\n 1.0463972  4.015525   0.7320576  1.2686954  0.9321375  0.7644167\n 1.4937844  1.5143414  2.6533833  0.54874516 2.697791   1.0919209\n 0.12234592 1.9530935  0.8373399  0.801682   0.22984314 0.51666355\n 1.0831909  2.151431   1.1258307  0.96949005]", "mean_td_error": 1.036008358001709, "actor_loss": -16.258995056152344, "critic_loss": 0.8334827423095703, "alpha_loss": -11.161215782165527, "alpha_value": 0.026458637788891792, "target_entropy": -6, "mean_q": 15.638249397277832, "max_q": 31.132869720458984, "min_q": 5.637004852294922, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 23000, "episodes_total": 23, "training_iteration": 23, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_15-03-12", "timestamp": 1587049392, "time_this_iter_s": 86.79083824157715, "time_total_s": 1152.630589723587, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 1152.630589723587, "timesteps_since_restore": 23000, "iterations_since_restore": 23, "perf": {"cpu_util_percent": 92.26802721088436, "ram_util_percent": 11.5}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -109.98643387491958, "episode_reward_min": -198.79531094651156, "episode_reward_mean": -153.2008508228431, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-109.98643387491958, -128.07929702967309, -144.47271617841545, -134.6251987675144, -164.32043348444572, -164.52843293873, -198.79531094651156, -138.96336352423964, -174.6135512464928, -173.62377023748903], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26382210923264543, "mean_processing_ms": 0.11999238751810433, "mean_inference_ms": 1.1828412852122123}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -87.16804782430631, "episode_reward_min": -170.21091672483763, "episode_reward_mean": -134.6824788137073, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-87.16804782430631, -129.23521844030233, -170.21091672483763, -166.56272463897275, -120.23548644011734], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.34085850820985336, "mean_processing_ms": 0.45672869848540704, "mean_inference_ms": 1.6706613203941214}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 12000, "num_steps_trained": 3582464, "num_steps_sampled": 24000, "sample_time_ms": 3.446, "replay_time_ms": 24.221, "grad_time_ms": 59.735, "update_time_ms": 0.005, "opt_peak_throughput": 4285.628, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[10.442576 ]\n [14.623355 ]\n [10.090422 ]\n [10.160344 ]\n [24.811594 ]\n [ 9.99559  ]\n [ 8.395335 ]\n [16.848385 ]\n [17.674648 ]\n [ 7.5114274]\n [ 9.673299 ]\n [32.447845 ]\n [19.70052  ]\n [10.33824  ]\n [18.74129  ]\n [ 7.1223536]\n [26.015017 ]\n [ 9.346386 ]\n [ 9.490514 ]\n [ 9.29978  ]\n [ 8.387162 ]\n [25.242775 ]\n [ 9.885921 ]\n [28.312355 ]\n [18.36147  ]\n [10.5737295]\n [ 9.884108 ]\n [ 9.003512 ]\n [ 6.414668 ]\n [19.881918 ]\n [16.180681 ]\n [ 9.865995 ]\n [26.714031 ]\n [ 9.499435 ]\n [11.727053 ]\n [ 9.360181 ]\n [ 7.871107 ]\n [28.266186 ]\n [18.286602 ]\n [ 9.821689 ]\n [16.902378 ]\n [11.444792 ]\n [10.560523 ]\n [20.128376 ]\n [17.066452 ]\n [ 8.734415 ]\n [ 9.434648 ]\n [12.190789 ]\n [ 7.620697 ]\n [ 6.4771414]\n [21.776106 ]\n [ 7.898268 ]\n [10.999462 ]\n [ 9.9230795]\n [12.257117 ]\n [20.386251 ]\n [10.292347 ]\n [25.86215  ]\n [10.05874  ]\n [29.27483  ]\n [27.695864 ]\n [17.950056 ]\n [29.079227 ]\n [10.637594 ]\n [31.071321 ]\n [31.813313 ]\n [10.651319 ]\n [ 9.955682 ]\n [10.753266 ]\n [21.23919  ]\n [ 7.3657107]\n [ 9.1253195]\n [ 9.527998 ]\n [34.667183 ]\n [ 9.993424 ]\n [10.9714155]\n [13.61717  ]\n [19.890495 ]\n [11.388761 ]\n [23.025002 ]\n [13.490248 ]\n [30.78771  ]\n [ 9.944038 ]\n [10.535514 ]\n [11.923903 ]\n [20.813951 ]\n [ 8.770781 ]\n [17.791077 ]\n [ 8.951402 ]\n [18.970375 ]\n [ 9.783259 ]\n [ 9.167646 ]\n [24.932297 ]\n [10.668882 ]\n [ 9.643915 ]\n [21.899004 ]\n [ 8.799094 ]\n [25.081144 ]\n [10.274058 ]\n [14.116089 ]\n [ 9.904461 ]\n [11.50353  ]\n [12.414572 ]\n [28.58814  ]\n [15.941475 ]\n [17.356445 ]\n [ 8.023737 ]\n [ 7.6493974]\n [10.691632 ]\n [11.200306 ]\n [26.969612 ]\n [ 7.739565 ]\n [24.314837 ]\n [27.241451 ]\n [ 9.843126 ]\n [20.758154 ]\n [28.576439 ]\n [ 9.305432 ]\n [ 8.591789 ]\n [21.839802 ]\n [23.738688 ]\n [ 9.0038   ]\n [30.87734  ]\n [14.579587 ]\n [ 9.718352 ]\n [ 8.536805 ]\n [10.24288  ]\n [18.27565  ]\n [12.006344 ]\n [19.617996 ]\n [23.094671 ]\n [ 9.538934 ]\n [ 9.096188 ]\n [21.421303 ]\n [12.963135 ]\n [ 8.425871 ]\n [10.371136 ]\n [13.211413 ]\n [19.080456 ]\n [10.172375 ]\n [ 9.001485 ]\n [10.295942 ]\n [11.536182 ]\n [ 8.908953 ]\n [18.72816  ]\n [ 8.239599 ]\n [18.880758 ]\n [23.752748 ]\n [19.668903 ]\n [ 9.379928 ]\n [27.272825 ]\n [10.8858795]\n [ 9.321805 ]\n [ 9.710685 ]\n [ 8.342498 ]\n [23.128315 ]\n [10.38161  ]\n [ 8.652623 ]\n [ 9.84604  ]\n [21.83883  ]\n [24.831596 ]\n [19.611214 ]\n [ 9.213275 ]\n [21.476934 ]\n [ 9.412893 ]\n [ 9.849884 ]\n [20.530544 ]\n [37.08104  ]\n [14.207961 ]\n [18.442999 ]\n [10.975733 ]\n [18.978014 ]\n [19.939924 ]\n [27.272486 ]\n [10.729523 ]\n [25.162031 ]\n [14.574143 ]\n [21.879044 ]\n [ 7.1605215]\n [ 9.156233 ]\n [22.567255 ]\n [ 9.933227 ]\n [18.889244 ]\n [ 8.64578  ]\n [10.8039055]\n [ 8.274134 ]\n [18.632956 ]\n [10.356894 ]\n [ 7.7556877]\n [29.057035 ]\n [ 9.423358 ]\n [ 7.9167686]\n [ 9.895174 ]\n [15.021283 ]\n [19.841255 ]\n [22.197401 ]\n [20.132015 ]\n [28.563208 ]\n [23.69618  ]\n [ 8.484662 ]\n [15.876718 ]\n [21.954037 ]\n [18.466404 ]\n [ 9.419634 ]\n [23.353336 ]\n [ 9.309101 ]\n [ 8.753677 ]\n [12.518023 ]\n [ 9.922866 ]\n [17.893642 ]\n [19.593876 ]\n [12.109689 ]\n [12.047389 ]\n [20.046223 ]\n [30.696447 ]\n [19.544018 ]\n [26.45459  ]\n [ 7.2611713]\n [ 9.440391 ]\n [12.669461 ]\n [10.076463 ]\n [14.723616 ]\n [ 8.699706 ]\n [ 9.21395  ]\n [24.159363 ]\n [13.841001 ]\n [14.607822 ]\n [12.857351 ]\n [11.873389 ]\n [ 8.989926 ]\n [16.60919  ]\n [24.322996 ]\n [17.71655  ]\n [ 8.646618 ]\n [ 6.315162 ]\n [16.135496 ]\n [ 9.239896 ]\n [18.72816  ]\n [13.627264 ]\n [ 8.5648365]\n [ 7.60901  ]\n [24.9454   ]\n [28.372372 ]\n [ 9.03846  ]\n [ 9.336211 ]\n [21.205263 ]\n [20.816174 ]\n [10.694506 ]\n [10.863956 ]\n [ 8.5741625]\n [ 8.333618 ]\n [ 8.000364 ]\n [10.869849 ]\n [23.704556 ]\n [18.532082 ]\n [ 9.083325 ]]", "q_t_selected": "[10.442576  14.623355  10.090422  10.160344  24.811594   9.99559\n  8.395335  16.848385  17.674648   7.5114274  9.673299  32.447845\n 19.70052   10.33824   18.74129    7.1223536 26.015017   9.346386\n  9.490514   9.29978    8.387162  25.242775   9.885921  28.312355\n 18.36147   10.5737295  9.884108   9.003512   6.414668  19.881918\n 16.180681   9.865995  26.714031   9.499435  11.727053   9.360181\n  7.871107  28.266186  18.286602   9.821689  16.902378  11.444792\n 10.560523  20.128376  17.066452   8.734415   9.434648  12.190789\n  7.620697   6.4771414 21.776106   7.898268  10.999462   9.9230795\n 12.257117  20.386251  10.292347  25.86215   10.05874   29.27483\n 27.695864  17.950056  29.079227  10.637594  31.071321  31.813313\n 10.651319   9.955682  10.753266  21.23919    7.3657107  9.1253195\n  9.527998  34.667183   9.993424  10.9714155 13.61717   19.890495\n 11.388761  23.025002  13.490248  30.78771    9.944038  10.535514\n 11.923903  20.813951   8.770781  17.791077   8.951402  18.970375\n  9.783259   9.167646  24.932297  10.668882   9.643915  21.899004\n  8.799094  25.081144  10.274058  14.116089   9.904461  11.50353\n 12.414572  28.58814   15.941475  17.356445   8.023737   7.6493974\n 10.691632  11.200306  26.969612   7.739565  24.314837  27.241451\n  9.843126  20.758154  28.576439   9.305432   8.591789  21.839802\n 23.738688   9.0038    30.87734   14.579587   9.718352   8.536805\n 10.24288   18.27565   12.006344  19.617996  23.094671   9.538934\n  9.096188  21.421303  12.963135   8.425871  10.371136  13.211413\n 19.080456  10.172375   9.001485  10.295942  11.536182   8.908953\n 18.72816    8.239599  18.880758  23.752748  19.668903   9.379928\n 27.272825  10.8858795  9.321805   9.710685   8.342498  23.128315\n 10.38161    8.652623   9.84604   21.83883   24.831596  19.611214\n  9.213275  21.476934   9.412893   9.849884  20.530544  37.08104\n 14.207961  18.442999  10.975733  18.978014  19.939924  27.272486\n 10.729523  25.162031  14.574143  21.879044   7.1605215  9.156233\n 22.567255   9.933227  18.889244   8.64578   10.8039055  8.274134\n 18.632956  10.356894   7.7556877 29.057035   9.423358   7.9167686\n  9.895174  15.021283  19.841255  22.197401  20.132015  28.563208\n 23.69618    8.484662  15.876718  21.954037  18.466404   9.419634\n 23.353336   9.309101   8.753677  12.518023   9.922866  17.893642\n 19.593876  12.109689  12.047389  20.046223  30.696447  19.544018\n 26.45459    7.2611713  9.440391  12.669461  10.076463  14.723616\n  8.699706   9.21395   24.159363  13.841001  14.607822  12.857351\n 11.873389   8.989926  16.60919   24.322996  17.71655    8.646618\n  6.315162  16.135496   9.239896  18.72816   13.627264   8.5648365\n  7.60901   24.9454    28.372372   9.03846    9.336211  21.205263\n 20.816174  10.694506  10.863956   8.5741625  8.333618   8.000364\n 10.869849  23.704556  18.532082   9.083325 ]", "twin_q_t_selected": "[11.197728  14.580703  11.154959   8.717698  23.8768    10.385147\n  8.697613  17.702654  16.544449   7.1916456  8.667661  32.31702\n 20.165216  10.630807  19.28462    7.9469805 24.66781    9.928659\n  9.265257   9.226624   8.095925  24.692171   9.393038  28.09051\n 17.35917   11.396565   9.249015   8.920829   8.207485  19.05633\n 15.756483   8.620428  24.323658   9.115152  10.180075   9.942913\n  6.4212923 28.850965  18.438007   9.531026  16.82929   10.546676\n 10.04056   18.958508  15.776677   8.6886425 10.263184  12.645417\n  7.1929784  7.5452785 23.000824   7.62999    9.498685  10.0332575\n 12.930698  20.337008   9.22487   25.456694  10.6642065 30.125097\n 27.471085  17.868734  29.326187  10.256482  30.544159  30.865067\n 11.069188  10.477653  11.090922  21.73839    8.819389   9.074925\n  8.926292  34.34533    9.678657  11.07856   13.179615  18.850931\n 11.292262  22.140436  12.523345  28.172255   9.2991085 11.282967\n 12.523027  19.916912   9.132063  17.791428   9.089519  19.257572\n  9.284294   8.720354  24.322454  10.8256     9.5585575 21.479977\n  8.281648  25.73938   11.400259  14.591983   8.949737  11.543669\n 11.588734  29.640888  17.128536  17.866732   7.0604954  8.575671\n 11.765721  13.105254  27.73309    7.803728  25.367449  29.521635\n  9.9054165 21.195372  30.552519  10.218766   6.7023373 21.95525\n 23.114092   9.041808  31.531052  15.476693  10.364763   8.555389\n  9.700264  16.050076  11.747257  19.41562   23.375473   8.621661\n  9.091409  20.819016  12.6040325  8.739483  10.522592  12.854056\n 20.121023   9.479979   7.478482  11.579111  11.597962   9.2273035\n 18.304106   8.798399  18.605188  21.88798   20.920345   8.72352\n 27.546446  12.202424  10.4080515  8.636877   8.8962965 22.113693\n 10.602384   9.209213   9.391932  22.53603   26.893791  20.268387\n  9.613061  22.458162  10.1163645 10.345337  19.874311  35.631084\n 14.373509  17.518938  10.987638  18.173359  20.916931  27.784567\n 10.900743  26.542313  14.830523  21.577955   6.817119  10.224853\n 21.842777   9.814738  19.060015   8.068206  11.236593   7.660339\n 18.903069  10.161593   8.670461  27.209528   9.899814   8.46063\n 10.39838   16.157143  18.39479   20.541677  18.651243  28.782248\n 23.847895   7.7234335 15.772646  21.933752  19.24245   10.783004\n 23.642393   9.887352   8.902178  11.448237  10.206345  18.317606\n 18.992336  12.4890175 11.361115  19.569096  30.38687   19.116179\n 26.89184    7.7161593 10.498315  12.422202  10.324918  14.20937\n  8.931092   8.870549  24.152246  13.119546  14.767238  12.363152\n 10.978857   8.926994  16.608644  24.638338  18.324923   8.742161\n  6.986021  16.285212   8.653312  18.304106  13.628714   8.685399\n  7.198488  24.390388  30.82459    8.801504  10.018553  21.266487\n 20.31851   11.992559  10.293768   8.407351   8.010193   7.880496\n 10.283684  23.449614  18.713476   8.953924 ]", "q_t_selected_target": "[ 9.208355  13.797485   8.232311   9.593096  22.621103  10.143763\n  9.78751   17.112432  17.489998   7.679972  10.08138   31.736938\n 17.508074   9.397541  16.824366   7.436175  25.835724   9.690905\n 10.224158   7.83865    7.96668   23.973166   8.968968  30.085558\n 16.559862  10.922145   9.667526   9.813405   6.979417  18.225277\n 14.757562  10.062909  26.694384   8.575476  11.58281   10.410149\n  7.8594522 28.661583  20.635277   9.014684  14.911396  11.354245\n 10.5617    18.268507  15.684824   7.6231604 10.9298725 13.547802\n  7.695605   7.0119658 22.608412   7.406754   8.920116   9.631619\n 13.594637  18.220055   9.455309  26.459402  11.033808  30.45525\n 29.223543  16.61975   28.87855   10.97172   32.99519   30.315939\n 11.270479   9.913622  10.613233  22.612864   7.9609423  7.8643475\n  8.4564705 32.01115    8.09809   11.452521  14.99759   21.15775\n  9.673844  20.559534  12.439024  29.772608   9.840238   9.609467\n 10.960724  21.404463   8.206093  17.267696   7.757437  16.440737\n  9.609201   9.495291  24.590433   9.567702  10.698234  23.760342\n  8.997436  24.157345  12.159546  12.652961   9.505902   9.4733515\n 10.556139  31.352312  16.035425  19.125402   8.275738   8.914879\n 12.43969   11.087722  26.678473   7.756878  24.726189  29.383564\n  9.1170025 22.325705  28.544922   9.650193   7.04233   19.994324\n 23.3999     7.580959  30.889875  15.549097  10.072236   7.4299736\n 10.658356  17.930195  12.013746  20.283773  25.991678   9.3822155\n  8.499901  22.651062  13.0745535  7.5381947 11.068447  12.689558\n 18.688288   9.109932   7.7912025 11.848511  14.16027    9.59634\n 21.604156   8.234773  19.599428  22.57509   18.649097   8.922233\n 25.979326  12.405044  10.691831   9.658741   9.369602  22.043045\n 11.51555    8.458102   8.889923  23.851799  24.574379  22.92318\n 10.144716  21.192026   8.78565    8.661295  22.073229  35.559765\n 15.912374  17.773682  10.244308  18.035467  22.226076  26.948639\n 11.306696  26.153858  16.006231  20.857141   7.6136594  8.755723\n 20.324333   9.240435  17.698004   7.205894  11.659467   8.629896\n 20.339373  10.026448   8.338315  28.283981   8.697226   8.009549\n  8.958734  16.289186  19.56532   20.335073  19.526814  30.932055\n 22.054564   8.541246  14.3248825 22.690031  19.016958   9.256579\n 21.773989   9.896314   8.875617  13.205893   9.82445   18.6703\n 20.662678  11.662394  10.920227  18.904936  29.402872  16.972559\n 27.466625   8.319925   9.694098  11.283673   9.221797  14.388497\n 10.566039   8.010096  26.081371  11.88316   15.58688   14.311951\n  8.8663645  9.898313  14.570158  25.191784  18.17607    9.871463\n  8.770855  17.422556   8.468066  21.772144  14.275852   8.605579\n  8.49926   25.307123  28.64625    7.5484014  7.7172966 22.93146\n 22.86429   12.224671  11.714008   8.633166   8.9963455  8.8246355\n 10.941931  27.10099   19.73854    8.568611 ]", "q_tp1_best_masked": "[ 8.801929  15.269635   9.2770605 10.201726  22.03532   10.384056\n 10.346635  18.65436   18.433508   8.454154  11.193051  31.50026\n 18.05433    9.497868  16.40275    9.353183  28.234203  10.484237\n 10.768033   9.190673   9.434665  25.04463    9.315904  29.641703\n 16.993597  10.35783   10.389363  10.639101   8.447464  18.11439\n 15.486042   8.988627  26.256916   9.453241  11.744502  11.254628\n  9.90321   29.611153  21.276772   9.213605  15.718466  11.222026\n 10.107535  18.658913  16.718952   7.868412  11.409255  12.907025\n  9.04575    8.288425  22.574722   8.740875   8.592413  10.180618\n 13.106971  19.076023   8.996676  27.013823  12.954217  32.303013\n 28.803316  17.91092   29.867641  11.193931  34.172184  30.666735\n 11.2306385 10.441782  10.720711  22.730837   7.759285   8.288058\n  8.560918  33.45568    8.895878  10.890252  15.665227  22.273254\n  9.441772  19.53664   12.390301  30.29749   10.604799   9.986908\n 12.456134  22.27644    7.9654355 18.005775   8.023959  18.29497\n 10.120564  10.396705  24.829382  10.900179  11.522083  23.33039\n  9.700583  23.058746  11.497201  13.711926   9.963158   8.858254\n 10.439216  31.766684  16.301117  20.874325   9.608723   9.82907\n 13.048243  11.761811  27.154123   9.808055  24.673677  30.44047\n  9.434557  22.041964  29.62303    9.348507   8.576945  21.585655\n 24.911278   7.00008   30.395042  17.255016  10.0366955  8.093578\n 10.8778305 18.999876  11.52693   21.120012  25.622396   9.481851\n  9.477282  24.12565   11.587052   9.1530905 10.0220785 11.522877\n 18.582865   9.531918   8.032126  12.459584  16.363699   9.419505\n 21.80395    9.5175495 21.022476  22.00263   17.773323   9.7920265\n 25.525581  11.865321  12.3392105 10.006915   9.839772  21.730965\n 11.475773  10.004784   9.864684  24.250326  23.69115   24.239634\n 11.35314   21.747936   9.063657  10.284082  23.88685   35.666245\n 15.8457365 17.908337  10.385508  18.784672  23.045105  28.297966\n 11.459622  25.96154   16.583694  21.201593   7.5644884 11.565398\n 20.720284   9.044519  18.445282   7.427378  10.68704    8.782385\n 21.583582   9.685403   8.5276575 28.58297    8.347038   7.891172\n  9.515139  16.52766   19.729643  19.869205  19.572773  30.821222\n 21.788649   8.748958  14.772007  23.433004  19.17407    9.92128\n 22.022627  10.158049   9.214452  13.461505  10.1241045 19.070438\n 20.605797  10.783825  12.180881  18.9564    30.724298  16.800983\n 27.33293    9.272037  11.04774   11.43557    9.217219  14.583659\n 11.593395   8.867095  26.190174  11.8827715 16.029856  14.671559\n 10.046191  10.172261  15.78758   25.787352  18.007742  12.298338\n 10.924083  17.807573   8.748165  21.973635  12.622591   9.626079\n  8.826488  24.54415   28.281597   8.997262   7.39975   22.475403\n 22.827145  13.420822  11.404695   9.484418   8.995923   9.786674\n 10.859334  27.540459  20.61192    9.924923 ]", "policy_t": "[[-0.5160358   0.6790581   0.42841935 -0.20720422  0.15591025 -0.9042203 ]\n [-0.6898345  -0.9671456  -0.9262088   0.94508505  0.971961    0.14194739]\n [ 0.45511556  0.7103641   0.90214825 -0.8862101  -0.40486002  0.20025885]\n ...\n [-0.49238837  0.9387505  -0.84842783 -0.47851682 -0.34224927 -0.9709324 ]\n [-0.9327708  -0.3217945  -0.43860948  0.6711159   0.7479613   0.9418037 ]\n [-0.9088704   0.32027185 -0.64202714  0.85026884  0.58768606 -0.9397767 ]]", "td_error": "[1.6117973  0.8045435  2.390379   0.721323   1.723094   0.19477844\n 1.2410359  0.4271345  0.5650997  0.32843566 0.9109001  0.64549446\n 2.4247942  1.0869823  2.188589   0.41231346 0.67360306 0.29113674\n 0.84627295 1.4245515  0.27486372 0.9943075  0.67051077 1.8841257\n 1.300458   0.41141796 0.31754637 0.85123444 0.89640856 1.2438469\n 1.2110205  0.8196974  1.1951866  0.7318182  0.773489   0.75860167\n 0.7249074  0.29238987 2.272972   0.66167355 1.9544382  0.44905806\n 0.26115847 1.2749348  0.7367406  1.0883684  1.0809569  1.1296988\n 0.2887671  0.5340686  0.61235905 0.35737514 1.3289571  0.34654903\n 1.0007291  2.1415749  0.5337386  0.79998016 0.6723347  0.7552862\n 1.640069   1.2896461  0.3241577  0.52468157 2.1874495  1.0232506\n 0.41022587 0.30304527 0.30886173 1.124074   0.7268393  1.235775\n 0.7706747  2.4951057  1.7379503  0.42753363 1.5991974  1.787036\n 1.666667   2.0231848  0.5677724  1.3077278  0.32246494 1.2997737\n 1.2627416  1.039031   0.7453289  0.52355576 1.2630229  2.6732368\n 0.24948263 0.5512905  0.30492115 1.1795387  1.0969973  2.0708513\n 0.45706463 1.2529173  1.3223872  1.7010751  0.47736216 2.0502477\n 1.4455137  2.2377977  0.59353065 1.513814   0.7336216  0.80234456\n 1.2110128  1.0650582  0.67287827 0.0320816  0.52630615 1.1400919\n 0.7572689  1.3489418  1.019557   0.45666695 0.944726   1.903202\n 0.31229782 1.4418454  0.32685566 0.520957   0.32320547 1.1161237\n 0.6867838  1.1127872  0.13694572 0.7669649  2.756606   0.45863628\n 0.59389734 1.5309029  0.29096985 1.0444822  0.62158346 0.34317684\n 0.91245174 0.7162447  0.76150155 0.91098404 2.5931973  0.5282121\n 3.0880232  0.28422642 0.85645485 0.9323845  1.6455269  0.32820368\n 1.4303093  0.8608918  0.8269024  0.53690386 0.75020504 0.57795906\n 1.0235529  0.472816   0.72906256 1.6643686  1.2883148  2.9833794\n 0.7315483  0.77552223 0.97897863 1.4363155  1.870801   0.7962971\n 1.6216383  0.4620304  0.73737717 0.5402193  1.7976484  0.5798874\n 0.49156332 0.6901407  1.3038983  0.8713579  0.62483907 0.9348197\n 1.880683   0.6335478  1.2766256  1.1510987  0.6392174  0.6626599\n 1.5713606  0.23279524 0.4573865  0.92375374 0.96436024 0.27193093\n 1.1880436  0.6999736  0.72323227 1.0344658  0.740386   2.259327\n 1.717474   0.43719864 1.4997993  0.74613667 0.38802338 0.84473944\n 1.723876   0.29808712 0.07425022 1.2227626  0.2401557  0.56467533\n 1.3695717  0.63695955 0.7840252  0.9027233  1.1387863  2.3575392\n 0.7934103  0.83125997 0.52896214 1.2621584  0.9788933  0.257123\n 1.7506399  1.0321541  1.9255667  1.5971136  0.8993497  1.7016993\n 2.5597587  0.93985224 2.0387592  0.7111168  0.30418587 1.1770735\n 2.1202633  1.2122021  0.47853756 3.256011   0.6478634  0.06028128\n 1.0955107  0.6392288  1.2261095  1.3715806  1.9600854  1.6955843\n 2.2969484  0.8811388  1.1351461  0.1424098  0.82444    0.88420534\n 0.36516428 3.5239058  1.1157618  0.45001364]", "mean_td_error": 1.029435634613037, "actor_loss": -15.51461410522461, "critic_loss": 0.7936826348304749, "alpha_loss": -11.657763481140137, "alpha_value": 0.021049072965979576, "target_entropy": -6, "mean_q": 15.114665985107422, "max_q": 37.08103942871094, "min_q": 6.315162181854248, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 24000, "episodes_total": 24, "training_iteration": 24, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_15-04-55", "timestamp": 1587049495, "time_this_iter_s": 86.7001121044159, "time_total_s": 1239.330701828003, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 1239.330701828003, "timesteps_since_restore": 24000, "iterations_since_restore": 24, "perf": {"cpu_util_percent": 92.29383561643837, "ram_util_percent": 11.567123287671231}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": 4.210933176356208, "episode_reward_min": -127.48625041778679, "episode_reward_mean": -76.4500563668789, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-97.64756665984355, -100.63164354801985, -127.48625041778679, -10.466586624604483, -70.98276782978817, -84.7308271891561, -88.28151803714141, 4.210933176356208, -96.8772032319363, -91.60713330686852], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26381310769679855, "mean_processing_ms": 0.12005213743929041, "mean_inference_ms": 1.1844034396130996}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
{"episode_reward_max": -57.878292463322765, "episode_reward_min": -170.21091672483763, "episode_reward_mean": -120.41109361831136, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-57.878292463322765, -170.21091672483763, -166.56272463897275, -120.23548644011734, -87.16804782430631], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.3428194347647627, "mean_processing_ms": 0.4592071692510508, "mean_inference_ms": 1.687636301508985}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 12500, "num_steps_trained": 3838464, "num_steps_sampled": 25000, "sample_time_ms": 2.946, "replay_time_ms": 21.638, "grad_time_ms": 50.089, "update_time_ms": 0.005, "opt_peak_throughput": 5110.954, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[ 7.879407 ]\n [11.887585 ]\n [10.822785 ]\n [ 7.393693 ]\n [11.802575 ]\n [ 7.707739 ]\n [ 7.7701254]\n [10.849269 ]\n [25.900719 ]\n [ 9.737611 ]\n [34.16939  ]\n [21.431126 ]\n [20.343653 ]\n [17.122948 ]\n [19.302711 ]\n [ 9.790118 ]\n [ 8.988017 ]\n [26.055527 ]\n [12.969948 ]\n [28.190102 ]\n [18.777834 ]\n [ 6.5005665]\n [ 6.9924364]\n [26.772345 ]\n [21.647776 ]\n [ 8.457176 ]\n [20.236664 ]\n [14.6394005]\n [ 9.906699 ]\n [31.471975 ]\n [11.325887 ]\n [10.935711 ]\n [17.069464 ]\n [ 9.565542 ]\n [26.726213 ]\n [10.296619 ]\n [11.980338 ]\n [ 7.3796864]\n [21.064438 ]\n [21.842752 ]\n [16.605364 ]\n [10.6562805]\n [19.48384  ]\n [12.70952  ]\n [ 7.861907 ]\n [22.636684 ]\n [ 8.721886 ]\n [ 8.247418 ]\n [23.940195 ]\n [45.638237 ]\n [ 9.737611 ]\n [10.9522705]\n [ 9.460017 ]\n [ 7.237067 ]\n [12.956661 ]\n [30.005394 ]\n [30.195112 ]\n [ 8.26008  ]\n [10.906606 ]\n [13.906227 ]\n [12.461424 ]\n [23.387945 ]\n [14.35752  ]\n [ 8.107258 ]\n [18.28216  ]\n [21.852432 ]\n [22.325455 ]\n [10.856189 ]\n [10.716746 ]\n [22.965122 ]\n [ 7.972039 ]\n [ 9.620415 ]\n [ 9.1729355]\n [ 9.051949 ]\n [29.175886 ]\n [ 7.5697308]\n [ 8.32229  ]\n [33.03491  ]\n [17.405313 ]\n [13.027204 ]\n [ 8.695995 ]\n [23.474766 ]\n [ 8.129738 ]\n [18.637106 ]\n [ 8.988126 ]\n [ 9.031474 ]\n [20.5212   ]\n [30.057928 ]\n [ 8.842965 ]\n [27.725168 ]\n [10.3026   ]\n [12.050108 ]\n [19.361807 ]\n [17.8588   ]\n [29.15116  ]\n [ 7.9063654]\n [17.19638  ]\n [ 7.4250383]\n [ 8.596396 ]\n [26.405962 ]\n [ 6.244587 ]\n [24.88393  ]\n [12.614745 ]\n [16.444853 ]\n [ 6.5311527]\n [ 7.776699 ]\n [10.103218 ]\n [ 8.698698 ]\n [30.077827 ]\n [ 7.8722563]\n [23.80447  ]\n [29.00715  ]\n [20.249949 ]\n [10.715323 ]\n [ 8.752084 ]\n [19.399347 ]\n [10.522754 ]\n [25.459105 ]\n [11.441669 ]\n [ 7.7518244]\n [12.527975 ]\n [19.72414  ]\n [19.055784 ]\n [19.02317  ]\n [ 6.6104565]\n [31.517038 ]\n [ 8.230139 ]\n [ 8.208497 ]\n [30.460318 ]\n [17.206863 ]\n [ 7.4542665]\n [25.698727 ]\n [15.93838  ]\n [22.982576 ]\n [27.955877 ]\n [11.850387 ]\n [ 9.654037 ]\n [16.095146 ]\n [ 9.618553 ]\n [12.095458 ]\n [ 9.263889 ]\n [25.140785 ]\n [12.695902 ]\n [15.691525 ]\n [ 9.712424 ]\n [16.423779 ]\n [25.805704 ]\n [14.910523 ]\n [ 9.549924 ]\n [14.651858 ]\n [ 8.369929 ]\n [11.469985 ]\n [17.095345 ]\n [24.339886 ]\n [19.906944 ]\n [22.183655 ]\n [ 8.741269 ]\n [24.03125  ]\n [ 8.4070215]\n [35.340935 ]\n [10.2542   ]\n [13.273978 ]\n [ 8.257022 ]\n [22.946117 ]\n [ 7.9460697]\n [11.285611 ]\n [14.290219 ]\n [17.89482  ]\n [ 7.6223917]\n [27.79308  ]\n [ 8.793628 ]\n [25.674757 ]\n [17.279062 ]\n [34.38323  ]\n [18.70838  ]\n [10.633633 ]\n [ 7.363509 ]\n [24.221958 ]\n [ 7.458786 ]\n [29.312616 ]\n [ 7.997093 ]\n [10.850843 ]\n [25.606668 ]\n [ 9.209841 ]\n [24.516953 ]\n [16.089138 ]\n [26.143284 ]\n [10.371477 ]\n [10.313638 ]\n [13.47147  ]\n [23.345003 ]\n [21.793442 ]\n [20.154211 ]\n [25.630653 ]\n [ 8.586256 ]\n [15.545195 ]\n [ 9.401187 ]\n [24.640156 ]\n [18.488022 ]\n [11.3503685]\n [28.203041 ]\n [27.95519  ]\n [ 8.365686 ]\n [28.02166  ]\n [19.801792 ]\n [19.624725 ]\n [27.228804 ]\n [24.446695 ]\n [21.717192 ]\n [16.267788 ]\n [11.089725 ]\n [19.821478 ]\n [13.673899 ]\n [22.592415 ]\n [ 8.672299 ]\n [ 8.359578 ]\n [16.275108 ]\n [25.008606 ]\n [10.3351145]\n [ 8.746842 ]\n [14.215139 ]\n [ 9.050991 ]\n [10.685915 ]\n [ 8.647124 ]\n [19.195715 ]\n [18.943394 ]\n [16.955235 ]\n [21.506525 ]\n [ 8.379616 ]\n [17.8051   ]\n [18.636263 ]\n [10.188576 ]\n [10.474958 ]\n [18.030956 ]\n [10.70766  ]\n [11.985705 ]\n [29.771772 ]\n [19.301523 ]\n [12.167791 ]\n [11.241438 ]\n [24.516953 ]\n [ 9.490973 ]\n [21.310171 ]\n [13.755671 ]\n [ 7.0891266]\n [18.797997 ]\n [ 8.898927 ]\n [13.437808 ]\n [16.729376 ]\n [21.318913 ]\n [ 7.17208  ]\n [10.886245 ]\n [18.140621 ]\n [21.12244  ]\n [10.481776 ]\n [ 6.4983196]]", "q_t_selected": "[ 7.879407  11.887585  10.822785   7.393693  11.802575   7.707739\n  7.7701254 10.849269  25.900719   9.737611  34.16939   21.431126\n 20.343653  17.122948  19.302711   9.790118   8.988017  26.055527\n 12.969948  28.190102  18.777834   6.5005665  6.9924364 26.772345\n 21.647776   8.457176  20.236664  14.6394005  9.906699  31.471975\n 11.325887  10.935711  17.069464   9.565542  26.726213  10.296619\n 11.980338   7.3796864 21.064438  21.842752  16.605364  10.6562805\n 19.48384   12.70952    7.861907  22.636684   8.721886   8.247418\n 23.940195  45.638237   9.737611  10.9522705  9.460017   7.237067\n 12.956661  30.005394  30.195112   8.26008   10.906606  13.906227\n 12.461424  23.387945  14.35752    8.107258  18.28216   21.852432\n 22.325455  10.856189  10.716746  22.965122   7.972039   9.620415\n  9.1729355  9.051949  29.175886   7.5697308  8.32229   33.03491\n 17.405313  13.027204   8.695995  23.474766   8.129738  18.637106\n  8.988126   9.031474  20.5212    30.057928   8.842965  27.725168\n 10.3026    12.050108  19.361807  17.8588    29.15116    7.9063654\n 17.19638    7.4250383  8.596396  26.405962   6.244587  24.88393\n 12.614745  16.444853   6.5311527  7.776699  10.103218   8.698698\n 30.077827   7.8722563 23.80447   29.00715   20.249949  10.715323\n  8.752084  19.399347  10.522754  25.459105  11.441669   7.7518244\n 12.527975  19.72414   19.055784  19.02317    6.6104565 31.517038\n  8.230139   8.208497  30.460318  17.206863   7.4542665 25.698727\n 15.93838   22.982576  27.955877  11.850387   9.654037  16.095146\n  9.618553  12.095458   9.263889  25.140785  12.695902  15.691525\n  9.712424  16.423779  25.805704  14.910523   9.549924  14.651858\n  8.369929  11.469985  17.095345  24.339886  19.906944  22.183655\n  8.741269  24.03125    8.4070215 35.340935  10.2542    13.273978\n  8.257022  22.946117   7.9460697 11.285611  14.290219  17.89482\n  7.6223917 27.79308    8.793628  25.674757  17.279062  34.38323\n 18.70838   10.633633   7.363509  24.221958   7.458786  29.312616\n  7.997093  10.850843  25.606668   9.209841  24.516953  16.089138\n 26.143284  10.371477  10.313638  13.47147   23.345003  21.793442\n 20.154211  25.630653   8.586256  15.545195   9.401187  24.640156\n 18.488022  11.3503685 28.203041  27.95519    8.365686  28.02166\n 19.801792  19.624725  27.228804  24.446695  21.717192  16.267788\n 11.089725  19.821478  13.673899  22.592415   8.672299   8.359578\n 16.275108  25.008606  10.3351145  8.746842  14.215139   9.050991\n 10.685915   8.647124  19.195715  18.943394  16.955235  21.506525\n  8.379616  17.8051    18.636263  10.188576  10.474958  18.030956\n 10.70766   11.985705  29.771772  19.301523  12.167791  11.241438\n 24.516953   9.490973  21.310171  13.755671   7.0891266 18.797997\n  8.898927  13.437808  16.729376  21.318913   7.17208   10.886245\n 18.140621  21.12244   10.481776   6.4983196]", "twin_q_t_selected": "[ 7.7962584 12.0540285  9.879794   5.786927  12.719399   7.808423\n  9.107861  11.297502  27.479223   9.690356  31.719706  23.04592\n 19.093304  17.554571  19.812586  10.009667   8.310752  26.368322\n 12.8481245 27.037954  18.225695   7.1175632  6.9411535 27.113792\n 21.353195   7.4288373 20.27071   13.4804535  8.784707  29.699713\n 10.336427  10.973177  16.282145  10.423976  27.111767  10.262013\n 12.818265   7.7315583 20.719522  21.74986   16.997206  11.418111\n 19.835003  12.088167   6.2465363 22.302898  10.167806   7.4815884\n 24.429857  44.108093   9.690356  10.404173  11.042275   7.086642\n 11.926094  29.068045  30.64676    8.945352  11.380988  14.796245\n 12.916875  22.772255  14.084389   9.736845  18.302166  20.801956\n 21.747332  10.1434555 10.308777  23.436768   9.500546   9.153775\n  8.857869   9.092868  30.761826   7.1810985  7.703166  32.5151\n 18.036541  12.163654   9.709964  23.432005   9.001428  17.683092\n 10.007795   9.043929  20.821476  30.58844    9.68182   26.59795\n 10.092825  12.781246  19.310936  16.53082   27.107931   7.36498\n 17.173443   7.6774545  9.0113535 25.43838    5.795879  25.13437\n 12.679131  15.188167   6.4665017  6.5736976 10.277088   7.3949676\n 29.212078   8.070057  22.119946  29.127731  21.764147  10.631377\n  8.57709   19.796957  10.946746  24.926914  10.256769   8.895344\n 12.3883295 19.113934  17.405272  20.439892   5.626239  30.722853\n  8.345828   9.071023  30.224903  17.669317   7.7902126 25.169012\n 14.348879  22.22657   28.15413   12.323785   9.954029  15.965273\n  9.865216  11.864291   8.869972  24.373589  11.710304  15.328513\n 10.123012  15.694075  24.83513   14.738239  11.360135  14.859203\n  8.340394  11.228965  17.612648  24.08998   19.785557  23.67442\n  7.8354053 24.154848   8.568817  33.89757    9.521673  13.789346\n  8.439102  22.659534   8.230413  11.510539  14.507587  17.798872\n  8.909268  27.782597   8.344939  24.49965   16.69924   34.192867\n 18.880693  12.747727   8.739339  23.830046   6.3509316 28.893427\n  8.067056  10.507906  25.005672   9.409329  24.498627  16.155111\n 24.387213  10.799188  10.168189  13.183246  22.581264  22.106739\n 19.539673  26.16757    9.202953  15.458698   8.73427   24.468582\n 17.797379  10.0228195 26.792986  30.633667   9.049157  28.210081\n 20.103998  20.329481  26.789421  23.983732  21.930254  16.541138\n 12.137596  20.422045  13.544663  24.354492   9.719663   8.1284895\n 17.728123  25.26293   10.481491   8.625332  12.987423   9.126424\n 10.916875   8.840979  19.857227  20.443657  16.83821   20.55084\n  8.060874  17.91974   19.696201  10.286319  10.751395  18.106947\n  9.59293   13.100542  29.701212  20.330647  12.205579  10.227025\n 24.498627   8.439399  22.205685  14.267223   8.213061  18.074032\n  8.803009  13.909824  16.313957  22.579187   7.386953  10.521792\n 18.271677  21.17354   11.808263   7.1868997]", "q_t_selected_target": "[ 8.633877  11.313895  10.77456    6.995753  11.3895     6.046724\n  9.351823  11.900669  27.967888   8.758497  31.682856  23.306852\n 18.761646  19.60146   19.650995   9.897676   9.210285  24.987679\n 13.144574  28.976667  21.181795   8.117566   6.902757  28.674126\n 23.857906   9.0022545 18.1183    13.735786   8.135712  29.448385\n  9.407877  10.214766  16.733736  10.551766  25.94273    8.781768\n 14.074137   8.24597   19.960722  22.40001   15.4170475  9.25954\n 21.911352  12.281019   8.085375  23.824615  10.076883   8.555451\n 23.56557   46.53183    8.46139   11.441478   9.820465   5.27485\n 11.513536  27.758465  28.967695   8.206662  11.370614  14.705363\n 11.203531  22.928503  14.926222   8.156108  16.373198  19.515911\n 22.379505  10.489286  11.730304  21.524755   8.492699  10.011201\n  9.014619   8.941528  28.689104   7.7027893  9.169218  33.992374\n 16.648888  12.988529   9.63349   22.236877   9.141002  18.570282\n 10.720223   8.716405  22.477926  30.996626  10.577163  25.350084\n  9.984646  13.383975  17.760664  18.244902  27.85612    8.702216\n 15.070588   6.883192   9.766602  27.197384   6.2229543 26.752127\n 10.558421  18.17907    5.3505845  6.516136  11.884711   8.7225065\n 28.689177   8.133138  23.737537  27.535246  19.685404  10.112355\n  8.237591  18.648724   9.712887  26.589394  10.531189   8.867559\n 11.425926  18.50529   17.556568  18.784815   7.134806  31.812042\n  9.11441    8.215691  32.811512  18.44472    8.020132  23.102692\n 15.391536  22.34681   28.895138  12.832833   7.531454  18.860458\n 10.55614   12.476272  10.400913  24.851978  11.28669   15.163878\n  9.561268  15.465919  24.763494  13.806779  11.7342    13.290064\n  8.173248  12.127161  19.409681  22.768082  21.529512  23.932592\n  8.044951  25.058222  10.220655  35.974365  11.809459  15.179918\n 10.146434  22.46171    7.743318  10.532048  13.692144  17.616482\n  9.016069  25.259033   9.72819   25.036955  15.681583  35.12548\n 20.431112  13.353853   8.426161  24.833647   7.6857076 28.480303\n  8.317711   9.625577  25.626019   8.117694  23.73677   17.29851\n 24.093332   9.953788  11.34367   14.352549  21.71824   22.672106\n 17.360748  25.182648   9.766316  18.017603   8.531316  25.208696\n 19.894583  10.305009  26.946434  30.007921  10.584069  28.281048\n 19.045189  19.4093    25.284376  26.247229  21.237108  18.18757\n 12.113791  20.06661   12.26502   22.489489   7.8646026  8.268679\n 15.253752  26.576616   9.178073  10.124357  14.763071  10.245805\n 11.536614   8.560706  19.134613  18.902456  15.839167  19.968891\n  8.1902485 16.215084  17.666843   9.740597  10.87057   18.73906\n  8.79035   13.521281  28.96218   21.771086  13.110348  13.288803\n 23.598606   6.703315  21.522982  12.295233   6.6355453 21.353876\n 11.495199  15.700736  14.8368845 20.083996   8.27354    9.793816\n 16.158794  23.047987  12.76092    7.6669717]", "q_tp1_best_masked": "[ 8.848888  11.784371  10.559903   7.7395773 11.785806   8.023345\n 10.782977  10.9293785 28.347628   9.24341   32.04746   23.803589\n 19.731842  20.908566  20.859539  11.350988  10.576908  24.479298\n 12.869588  29.941416  23.311226   9.805099   7.1359057 29.401155\n 24.542006  10.218238  18.286562  14.56993    8.907444  30.43234\n  9.642983  10.461833  15.901125  10.755904  25.773731   8.449938\n 14.0810995  8.123531  20.311213  23.198986  16.559763  10.405076\n 20.909893  13.497964   8.06751   24.04347   10.318293  10.197113\n 22.140291  46.09995    8.943301  11.000744  10.1026     4.8324265\n 12.267771  29.070301  31.12618    7.690415  12.409882  14.133807\n 11.417275  22.797895  14.9273405  8.80224   16.454962  19.641352\n 20.923052  10.598407  12.337355  21.594057   9.081306   9.743215\n  9.357514  10.119873  29.151485   9.596504  10.271675  34.885597\n 17.512566  13.766927   8.90002   23.533335   9.722222  18.345287\n 11.0504     9.58466   22.373241  30.687101  10.325527  25.31555\n  9.95066   12.124513  19.509794  18.67038   26.763914   9.149626\n 16.09782    8.137703  10.436734  28.082556   7.7541656 27.96734\n 11.5024    18.55007    5.36469    7.290238  11.171653   9.412265\n 28.995005   8.079511  25.2231    27.097218  19.984781   9.682193\n  8.577139  19.105692  10.411636  28.497803  11.362492   8.403002\n 11.892635  20.174303  18.978855  20.220303   7.138089  32.888664\n 10.536989   9.477568  33.585094  19.30265    8.569576  23.889212\n 17.068773  23.634256  31.548693  11.995692   8.075712  18.801943\n 10.394275  12.059426  11.307726  24.728064  12.091444  16.479551\n  9.021194  16.929268  25.615808  13.892068  12.02198   14.405634\n  8.215633  11.994981  19.471634  23.399313  22.785744  23.901823\n  8.214952  25.19876   11.370185  34.630695  12.420413  16.105156\n 11.019605  22.868542   7.530781  10.619325  13.83011   17.641523\n 10.429741  24.502539   8.728264  25.693172  16.401548  35.747917\n 20.410204  12.062901   9.665156  25.134802   7.6351767 29.355886\n  7.8616247 10.058767  27.789303   8.213285  23.073412  17.343561\n 25.257656   9.290846  11.464722  14.500676  21.26618   23.375507\n 17.949167  25.123226   9.971846  17.462173   8.961822  25.680676\n 20.949892   9.173203  26.816496  29.400492  11.107684  28.779503\n 19.806076  19.567825  24.2583    27.667456  20.827328  18.155165\n 11.899678  20.917442  14.276969  22.128788   7.43065    9.701547\n 15.385919  26.490332   9.215795  10.054133  16.610336  10.583205\n 11.63649    9.992963  18.866613  19.556067  15.757907  20.011057\n  8.330448  17.811308  17.752394   9.925316  10.492365  18.532288\n  9.128831  12.890975  29.328012  22.584122  12.2906885 12.742285\n 22.933851   7.828914  22.23301   13.180512   8.042213  21.418207\n 11.73372   15.484596  15.106788  20.65278    8.783228  10.012148\n 17.392262  25.41673   13.685082   7.829234 ]", "policy_t": "[[-0.979842    0.7523236   0.28249443 -0.38130623 -0.24515647  0.7477293 ]\n [ 0.90279555  0.4249916  -0.8685857  -0.76670533 -0.77235603 -0.96134067]\n [-0.9865583   0.8725339   0.9823233  -0.84945947 -0.83488977  0.23662674]\n ...\n [-0.95498663  0.9555721  -0.99683416 -0.69116324  0.5114132   0.9792526 ]\n [-0.9530559   0.9565815  -0.1750521  -0.97190505 -0.31972736 -0.69442666]\n [-0.5276545  -0.79427385  0.69057393  0.95922506 -0.9777135   0.823845  ]]", "td_error": "[0.7960441  0.6569114  0.47149563 0.8033829  0.8714876  1.7113571\n 0.9128299  0.82728386 1.2779169  0.9554863  1.261693   1.0683298\n 0.95683193 2.2627     0.25493717 0.10977459 0.5609007  1.224246\n 0.235538   1.3626394  2.6800308  1.3085012  0.0640378  1.7310572\n 2.357421   1.0592477  2.1353874  0.5794735  1.2099915  1.1374588\n 1.4232798  0.7396784  0.3936596  0.5570073  0.9762602  1.4975486\n 1.6748352  0.69034743 0.9312582  0.60370255 1.3842373  1.7776561\n 2.2519302  0.31067657 1.0311532  1.3548241  0.72296    0.690948\n 0.61945534 1.6586647  1.252594   0.7632561  0.7911291  1.8870046\n 0.9278412  1.7782545  1.4532413  0.3960538  0.2371912  0.44500875\n 1.4856181  0.30784512 0.7052674  0.8147936  1.9189653  1.8112831\n 0.343112   0.35636663 1.2175422  1.6761894  0.7642536  0.62410593\n 0.15753317 0.13087988 1.2797518  0.3273747  1.1564898  1.217371\n 1.0720396  0.43177462 0.50698423 1.2165079  0.57541895 0.4770069\n 1.2222629  0.3212967  1.8065882  0.6734419  1.3147702  1.8114748\n 0.21306658 0.96829796 1.5757074  1.0500908  1.0216141  1.0665433\n 2.1143236  0.66805434 0.9627266  1.2752132  0.22435403 1.7429762\n 2.0885167  2.3625598  1.1482427  0.65906215 1.6945581  0.6756737\n 0.9557762  0.1619811  0.84226227 1.5321951  1.3216438  0.5609951\n 0.42699623 0.94942856 1.021863   1.3963842  0.59245014 0.5717597\n 1.0322261  0.9137478  0.82525635 0.9467163  1.0164585  0.6920967\n 0.826427   0.43126297 2.4689016  1.006629   0.39789248 2.3311777\n 0.7947507  0.37800312 0.84013367 0.74574757 2.2725792  2.8302488\n 0.81425524 0.49639702 1.3339825  0.38359833 0.9164133  0.34614086\n 0.35645008 0.59300804 0.5569229  1.0176024  1.27917    1.465467\n 0.18191338 0.7776861  2.055685   1.4468508  1.6832619  1.0035553\n 0.45293188 0.96517277 1.7327361  1.3551121  1.9215221  1.6482563\n 1.7983718  0.34111595 0.3449235  0.8660269  0.706759   0.23036385\n 0.7502394  2.5288048  1.1589069  0.587554   1.3075676  0.83743286\n 1.6365757  1.6631732  0.68791485 0.80764484 0.78084874 0.6227188\n 0.28563643 1.0537977  0.31984806 1.1918912  0.771019   1.1763849\n 1.171916   0.6315446  1.1027565  1.0251908  1.2448931  0.7220154\n 2.4861937  0.71646404 0.87171173 2.5156565  0.5364127  0.6543274\n 1.7518826  0.6637745  0.7050276  1.3392382  1.8766475  0.16517735\n 0.90770626 0.5678034  1.7247362  2.0320148  0.5866146  1.7831078\n 0.5239358  0.30028343 1.3442607  0.9839649  1.3313785  0.11554432\n 1.7478638  1.4408484  1.2302299  1.4382701  1.1617899  1.1570973\n 0.7352195  0.18334532 0.3918581  0.79106903 1.0575552  1.0597916\n 0.1593709  1.647336   1.4993887  0.4968505  0.25739336 0.67010784\n 1.3599448  0.9781575  0.774313   1.9550009  0.92366266 2.5545716\n 0.9091835  2.2618713  0.44775677 1.7162142  1.0155487  2.917862\n 2.6442313  2.0269198  1.684782   1.8650541  0.9940231  0.910203\n 2.0473547  1.8999968  1.6159     0.82436204]", "mean_td_error": 1.0760157108306885, "actor_loss": -16.495344161987305, "critic_loss": 0.8353381752967834, "alpha_loss": -5.046690940856934, "alpha_value": 0.01728249527513981, "target_entropy": -6, "mean_q": 15.978450775146484, "max_q": 45.63823699951172, "min_q": 6.244586944580078, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 25000, "episodes_total": 25, "training_iteration": 25, "experiment_id": "ec0bb59a456d49efa83285a885025910", "date": "2020-04-16_15-06-36", "timestamp": 1587049596, "time_this_iter_s": 84.77720808982849, "time_total_s": 1324.1079099178314, "pid": 7344, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10006, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 1324.1079099178314, "timesteps_since_restore": 25000, "iterations_since_restore": 25, "perf": {"cpu_util_percent": 91.25384615384615, "ram_util_percent": 11.325174825174825}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": 82.25179965137423, "episode_reward_min": -90.50861263133554, "episode_reward_mean": 5.4215240493521595, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [73.35633758228438, -62.68751893289974, 9.589835212044092, 77.64491707171872, -62.270059456845296, 44.94277270728755, 82.25179965137423, -90.50861263133554, -43.34489764337708, 25.240666933270276], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2634650935624162, "mean_processing_ms": 0.11996569807757633, "mean_inference_ms": 1.1829644406199598}, "off_policy_estimator": {}}, "trial_id": "00005", "experiment_tag": "5_learning_starts=10006"}
