{"episode_reward_max": -382.5493118599235, "episode_reward_min": -382.5493118599235, "episode_reward_mean": -382.5493118599235, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-382.5493118599235], "episode_lengths": [1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2682259032776306, "mean_processing_ms": 0.37732705488786117, "mean_inference_ms": 1.3119628974845954}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 500, "num_steps_trained": 0, "num_steps_sampled": 1000, "sample_time_ms": 2.066, "replay_time_ms": 0.0, "grad_time_ms": 0.0, "update_time_ms": 0.003, "opt_peak_throughput": 0.0, "opt_samples": 0.0, "learner": {}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 1000, "episodes_total": 1, "training_iteration": 1, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_14-38-14", "timestamp": 1587047894, "time_this_iter_s": 3.3527097702026367, "time_total_s": 3.3527097702026367, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 3.3527097702026367, "timesteps_since_restore": 1000, "iterations_since_restore": 1, "perf": {"cpu_util_percent": 90.03999999999999, "ram_util_percent": 10.120000000000001}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -181.21117068493004, "episode_reward_min": -455.2114733778109, "episode_reward_mean": -358.7325826756049, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-429.3717071889963, -455.2114733778109, -432.99499870215226, -293.40991990984014, -284.2240471952261, -277.51693767548977, -450.2453462851109, -339.0178890510686, -181.21117068493004, -444.12233668542365], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2670112150619655, "mean_processing_ms": 0.12385355283136239, "mean_inference_ms": 1.2224570427783878}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -382.5493118599235, "episode_reward_min": -511.3548347089806, "episode_reward_mean": -446.952073284452, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-511.3548347089806, -382.5493118599235], "episode_lengths": [1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.275577322774323, "mean_processing_ms": 0.38957919767201843, "mean_inference_ms": 1.3076394768830735}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 1000, "num_steps_trained": 0, "num_steps_sampled": 2000, "sample_time_ms": 2.583, "replay_time_ms": 0.0, "grad_time_ms": 0.0, "update_time_ms": 0.003, "opt_peak_throughput": 0.0, "opt_samples": 0.0, "learner": {}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 2000, "episodes_total": 2, "training_iteration": 2, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_14-38-36", "timestamp": 1587047916, "time_this_iter_s": 3.071564197540283, "time_total_s": 6.42427396774292, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 6.42427396774292, "timesteps_since_restore": 2000, "iterations_since_restore": 2, "perf": {"cpu_util_percent": 92.028125, "ram_util_percent": 10.575}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -217.95781136055518, "episode_reward_min": -474.4617336633556, "episode_reward_mean": -409.1295677772094, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-431.2743569568582, -461.079296697841, -457.6566543688111, -217.95781136055518, -474.4617336633556, -454.8201365632732, -310.08823523448245, -458.0784865633927, -394.1438052539321, -431.7351611095923], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26182809845637484, "mean_processing_ms": 0.12144075203618969, "mean_inference_ms": 1.1852836198827266}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -382.5493118599235, "episode_reward_min": -512.8930092176803, "episode_reward_mean": -468.9323852621948, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-512.8930092176803, -382.5493118599235, -511.3548347089806], "episode_lengths": [1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2795385441399343, "mean_processing_ms": 0.39547171497507705, "mean_inference_ms": 1.2991436639168519}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 1500, "num_steps_trained": 0, "num_steps_sampled": 3000, "sample_time_ms": 2.339, "replay_time_ms": 0.0, "grad_time_ms": 0.0, "update_time_ms": 0.003, "opt_peak_throughput": 0.0, "opt_samples": 0.0, "learner": {}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 3000, "episodes_total": 3, "training_iteration": 3, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_14-38-55", "timestamp": 1587047935, "time_this_iter_s": 2.9732141494750977, "time_total_s": 9.397488117218018, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 9.397488117218018, "timesteps_since_restore": 3000, "iterations_since_restore": 3, "perf": {"cpu_util_percent": 91.98846153846155, "ram_util_percent": 10.600000000000001}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -313.9667893392854, "episode_reward_min": -482.75072400542257, "episode_reward_mean": -388.12311965205214, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-374.33470309397694, -385.86250162724747, -413.1212813171036, -434.02913999259926, -482.75072400542257, -313.9667893392854, -341.7024108912493, -357.7224658150693, -362.74923497679487, -414.99194546177233], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.260994214303676, "mean_processing_ms": 0.12042762350986005, "mean_inference_ms": 1.1790951849329425}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -382.5493118599235, "episode_reward_min": -512.8930092176803, "episode_reward_mean": -458.78612357070085, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-428.34733849621904, -382.5493118599235, -511.3548347089806, -512.8930092176803], "episode_lengths": [1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.28094714141054644, "mean_processing_ms": 0.3976257038943252, "mean_inference_ms": 1.2920036819464997}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 2000, "num_steps_trained": 0, "num_steps_sampled": 4000, "sample_time_ms": 1.8, "replay_time_ms": 0.0, "grad_time_ms": 0.0, "update_time_ms": 0.003, "opt_peak_throughput": 0.0, "opt_samples": 0.0, "learner": {}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 4000, "episodes_total": 4, "training_iteration": 4, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_14-39-13", "timestamp": 1587047953, "time_this_iter_s": 2.9440932273864746, "time_total_s": 12.341581344604492, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 12.341581344604492, "timesteps_since_restore": 4000, "iterations_since_restore": 4, "perf": {"cpu_util_percent": 92.03461538461539, "ram_util_percent": 10.688461538461537}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -294.11849005300445, "episode_reward_min": -485.9641564196669, "episode_reward_mean": -389.96517364684166, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-306.3616483147971, -400.57385636257686, -396.7772791151776, -294.11849005300445, -351.28452608269765, -485.9641564196669, -391.89802828559044, -437.8821432969433, -422.96833425091023, -411.823274287052], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2600538924629606, "mean_processing_ms": 0.12022763329003323, "mean_inference_ms": 1.1716809964997152}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -264.6237950840552, "episode_reward_min": -512.8930092176803, "episode_reward_mean": -419.9536578733717, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-264.6237950840552, -382.5493118599235, -511.3548347089806, -512.8930092176803, -428.34733849621904], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2820206402732328, "mean_processing_ms": 0.3988631238296357, "mean_inference_ms": 1.286071344165686}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 2500, "num_steps_trained": 0, "num_steps_sampled": 5000, "sample_time_ms": 2.284, "replay_time_ms": 0.0, "grad_time_ms": 0.0, "update_time_ms": 0.003, "opt_peak_throughput": 0.0, "opt_samples": 0.0, "learner": {}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 5000, "episodes_total": 5, "training_iteration": 5, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_14-39-31", "timestamp": 1587047971, "time_this_iter_s": 2.909275770187378, "time_total_s": 15.25085711479187, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 15.25085711479187, "timesteps_since_restore": 5000, "iterations_since_restore": 5, "perf": {"cpu_util_percent": 91.88076923076922, "ram_util_percent": 10.699999999999998}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -162.60433703859346, "episode_reward_min": -515.0295587620591, "episode_reward_mean": -353.72994793393934, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-398.27570063663836, -162.60433703859346, -345.0331264987598, -258.33339609763635, -515.0295587620591, -470.1474714581189, -357.57225922161217, -312.94627867369957, -408.36661837836436, -308.99073257391126], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26104303887432995, "mean_processing_ms": 0.12007013029752836, "mean_inference_ms": 1.1751262243431886}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -264.6237950840552, "episode_reward_min": -512.8930092176803, "episode_reward_mean": -433.511787138239, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-450.33995818426035, -511.3548347089806, -512.8930092176803, -428.34733849621904, -264.6237950840552], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2849165388815956, "mean_processing_ms": 0.4034230499683221, "mean_inference_ms": 1.2717269354961918}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 3000, "num_steps_trained": 0, "num_steps_sampled": 6000, "sample_time_ms": 1.698, "replay_time_ms": 0.0, "grad_time_ms": 0.0, "update_time_ms": 0.002, "opt_peak_throughput": 0.0, "opt_samples": 0.0, "learner": {}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 6000, "episodes_total": 6, "training_iteration": 6, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_14-39-50", "timestamp": 1587047990, "time_this_iter_s": 2.682389974594116, "time_total_s": 17.933247089385986, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 17.933247089385986, "timesteps_since_restore": 6000, "iterations_since_restore": 6, "perf": {"cpu_util_percent": 92.09259259259261, "ram_util_percent": 10.699999999999998}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -283.40239982918683, "episode_reward_min": -548.9458951895654, "episode_reward_mean": -401.6154933230064, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-548.9458951895654, -283.40239982918683, -416.7484518382065, -486.59448099672545, -390.38400064087966, -294.58922369675525, -382.14994617999207, -472.644025271685, -292.96107495905824, -447.7354346280088], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26016974357368083, "mean_processing_ms": 0.11970375746906295, "mean_inference_ms": 1.1710200603241798}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -264.6237950840552, "episode_reward_min": -512.8930092176803, "episode_reward_mean": -421.56222127219417, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-451.6070053787561, -512.8930092176803, -428.34733849621904, -264.6237950840552, -450.33995818426035], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.28519217420850423, "mean_processing_ms": 0.40374002651227026, "mean_inference_ms": 1.260476345589646}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 3500, "num_steps_trained": 0, "num_steps_sampled": 7000, "sample_time_ms": 2.436, "replay_time_ms": 0.0, "grad_time_ms": 0.0, "update_time_ms": 0.003, "opt_peak_throughput": 0.0, "opt_samples": 0.0, "learner": {}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 7000, "episodes_total": 7, "training_iteration": 7, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_14-40-08", "timestamp": 1587048008, "time_this_iter_s": 3.030424118041992, "time_total_s": 20.96367120742798, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 20.96367120742798, "timesteps_since_restore": 7000, "iterations_since_restore": 7, "perf": {"cpu_util_percent": 91.99230769230768, "ram_util_percent": 10.715384615384616}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -255.7188281874035, "episode_reward_min": -425.590395073867, "episode_reward_mean": -348.50069609338584, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-350.7413238868023, -394.25073083584977, -334.6996932554459, -388.48389608615537, -396.30634714060363, -324.9214947309096, -277.050944259894, -425.590395073867, -255.7188281874035, -337.24330747692755], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2601437496390694, "mean_processing_ms": 0.11967498250260418, "mean_inference_ms": 1.172005102478813}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -264.6237950840552, "episode_reward_min": -451.6070053787561, "episode_reward_mean": -407.42290663498477, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-442.1964360316331, -428.34733849621904, -264.6237950840552, -450.33995818426035, -451.6070053787561], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2844412386115177, "mean_processing_ms": 0.4027393117186674, "mean_inference_ms": 1.252046822314063}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 4000, "num_steps_trained": 0, "num_steps_sampled": 8000, "sample_time_ms": 2.454, "replay_time_ms": 0.0, "grad_time_ms": 0.0, "update_time_ms": 0.003, "opt_peak_throughput": 0.0, "opt_samples": 0.0, "learner": {}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 8000, "episodes_total": 8, "training_iteration": 8, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_14-40-27", "timestamp": 1587048027, "time_this_iter_s": 2.8493576049804688, "time_total_s": 23.813028812408447, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 23.813028812408447, "timesteps_since_restore": 8000, "iterations_since_restore": 8, "perf": {"cpu_util_percent": 92.01538461538462, "ram_util_percent": 10.800000000000002}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -240.71747917957822, "episode_reward_min": -426.4210124475606, "episode_reward_mean": -354.6963041202484, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-361.99979458873827, -371.99252644417777, -318.7874267195363, -322.7183267491889, -372.22207075130694, -426.4210124475606, -406.3225671793478, -358.6572899933474, -240.71747917957822, -367.12454714970136], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26107074615336995, "mean_processing_ms": 0.11989443935833402, "mean_inference_ms": 1.1762305245745357}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -264.6237950840552, "episode_reward_min": -451.6070053787561, "episode_reward_mean": -400.8973285020378, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-395.7194478314839, -264.6237950840552, -450.33995818426035, -451.6070053787561, -442.1964360316331], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2841137309344497, "mean_processing_ms": 0.40225696152174156, "mean_inference_ms": 1.245022137229941}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 4500, "num_steps_trained": 0, "num_steps_sampled": 9000, "sample_time_ms": 2.286, "replay_time_ms": 0.0, "grad_time_ms": 0.0, "update_time_ms": 0.003, "opt_peak_throughput": 0.0, "opt_samples": 0.0, "learner": {}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 9000, "episodes_total": 9, "training_iteration": 9, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_14-40-46", "timestamp": 1587048046, "time_this_iter_s": 2.8639791011810303, "time_total_s": 26.677007913589478, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 26.677007913589478, "timesteps_since_restore": 9000, "iterations_since_restore": 9, "perf": {"cpu_util_percent": 92.07777777777775, "ram_util_percent": 10.800000000000002}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -127.22851004017554, "episode_reward_min": -636.524250380366, "episode_reward_mean": -382.466502819296, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-459.8223502161181, -127.22851004017554, -384.53515091803615, -386.82481308112517, -344.92159889477693, -398.895006001863, -636.524250380366, -424.27643410778427, -441.5620284875794, -220.07488606513547], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26139327853819067, "mean_processing_ms": 0.12013820834645213, "mean_inference_ms": 1.1763893945091488}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -395.7194478314839, "episode_reward_min": -452.38971113478243, "episode_reward_mean": -438.4505117121832, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-452.38971113478243, -450.33995818426035, -451.6070053787561, -442.1964360316331, -395.7194478314839], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.28348285687768027, "mean_processing_ms": 0.40175127246689674, "mean_inference_ms": 1.2391895793785046}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 5000, "num_steps_trained": 0, "num_steps_sampled": 10000, "sample_time_ms": 2.929, "replay_time_ms": 0.0, "grad_time_ms": 0.0, "update_time_ms": 0.003, "opt_peak_throughput": 0.0, "opt_samples": 0.0, "learner": {}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 10000, "episodes_total": 10, "training_iteration": 10, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_14-41-04", "timestamp": 1587048064, "time_this_iter_s": 2.8540217876434326, "time_total_s": 29.53102970123291, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 29.53102970123291, "timesteps_since_restore": 10000, "iterations_since_restore": 10, "perf": {"cpu_util_percent": 92.02592592592593, "ram_util_percent": 10.800000000000002}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -153.8944218314525, "episode_reward_min": -617.5755074609318, "episode_reward_mean": -383.83746255568093, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-428.1044949127144, -248.61877949725968, -408.3337471587138, -617.5755074609318, -512.8148705532348, -153.8944218314525, -454.01842589968504, -421.9513353153098, -274.9363655341364, -318.126677393371], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2600688962745382, "mean_processing_ms": 0.11969750869813081, "mean_inference_ms": 1.169486991920547}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -306.56299656689714, "episode_reward_min": -452.38971113478243, "episode_reward_mean": -409.69511938871057, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-306.56299656689714, -451.6070053787561, -442.1964360316331, -395.7194478314839, -452.38971113478243], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.28533218025380114, "mean_processing_ms": 0.40511869470444645, "mean_inference_ms": 1.2521962420644175}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 5500, "num_steps_trained": 254720, "num_steps_sampled": 11000, "sample_time_ms": 3.073, "replay_time_ms": 23.953, "grad_time_ms": 55.116, "update_time_ms": 0.004, "opt_peak_throughput": 4644.746, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[ 6.021546 ]\n [ 7.1459694]\n [ 7.6303363]\n [ 7.3503175]\n [ 5.7582374]\n [ 5.806253 ]\n [ 6.238563 ]\n [ 7.755293 ]\n [ 6.852913 ]\n [ 6.465022 ]\n [ 6.795162 ]\n [ 6.8268533]\n [ 4.074785 ]\n [ 7.2881703]\n [10.514497 ]\n [ 9.0550375]\n [ 7.7612276]\n [ 5.09708  ]\n [ 6.835952 ]\n [ 6.5443816]\n [ 6.625332 ]\n [ 4.83611  ]\n [ 5.4368258]\n [ 7.3344274]\n [ 5.200565 ]\n [ 7.744951 ]\n [ 5.2185025]\n [ 5.589958 ]\n [ 7.722257 ]\n [ 8.110455 ]\n [ 6.766716 ]\n [ 6.102967 ]\n [ 5.937665 ]\n [ 6.192793 ]\n [ 5.35445  ]\n [ 8.000986 ]\n [ 7.0157094]\n [ 8.424996 ]\n [ 6.410319 ]\n [ 5.487419 ]\n [ 7.066672 ]\n [ 7.74119  ]\n [ 6.915974 ]\n [ 7.2804418]\n [ 7.38452  ]\n [ 6.808472 ]\n [ 6.090673 ]\n [ 5.9530144]\n [ 8.4236965]\n [ 7.788331 ]\n [ 6.408653 ]\n [ 7.0648174]\n [ 8.25159  ]\n [ 7.13865  ]\n [ 7.6212153]\n [ 6.8440433]\n [ 6.828658 ]\n [ 6.8663774]\n [ 7.2536283]\n [ 6.8344593]\n [ 7.5962973]\n [ 7.529781 ]\n [ 8.257955 ]\n [ 5.8244677]\n [ 7.267512 ]\n [ 7.4136662]\n [ 7.679941 ]\n [ 5.973453 ]\n [ 7.3280473]\n [ 6.026004 ]\n [ 5.3407845]\n [ 7.5165644]\n [ 7.644201 ]\n [10.024547 ]\n [ 6.5027246]\n [ 6.009627 ]\n [ 6.0968137]\n [ 6.565275 ]\n [ 6.469416 ]\n [ 6.296116 ]\n [ 7.851736 ]\n [ 5.13476  ]\n [ 7.488879 ]\n [ 6.422452 ]\n [ 6.7614875]\n [ 5.448886 ]\n [ 8.853573 ]\n [ 7.3169603]\n [ 6.2682934]\n [ 4.6207514]\n [ 6.448698 ]\n [ 5.7274795]\n [ 6.8658166]\n [ 7.753627 ]\n [ 6.5639157]\n [ 7.1091604]\n [ 4.9788013]\n [ 8.938311 ]\n [ 4.543302 ]\n [ 6.937185 ]\n [ 4.498145 ]\n [ 6.7026043]\n [ 6.3970203]\n [ 7.923537 ]\n [ 6.0254917]\n [ 6.448096 ]\n [ 6.4413877]\n [ 8.253274 ]\n [ 6.563882 ]\n [ 5.356101 ]\n [ 6.5743356]\n [ 7.573473 ]\n [ 5.5582223]\n [ 6.5782895]\n [ 5.8473506]\n [ 7.692916 ]\n [ 7.8612785]\n [ 7.278079 ]\n [ 6.5374923]\n [ 6.795162 ]\n [ 7.931876 ]\n [ 5.1171436]\n [ 6.808472 ]\n [ 8.215135 ]\n [ 7.479066 ]\n [ 6.3242803]\n [ 6.4399405]\n [ 5.8964047]\n [ 7.1333947]\n [ 6.4642153]\n [ 7.112414 ]\n [ 8.741119 ]\n [ 5.973453 ]\n [ 6.7898474]\n [ 7.6604304]\n [ 6.2723136]\n [ 5.3328223]\n [ 6.450289 ]\n [ 8.153385 ]\n [ 7.284831 ]\n [ 5.4238443]\n [ 5.788635 ]\n [ 7.6606746]\n [ 6.842133 ]\n [ 6.6296782]\n [ 5.498847 ]\n [ 9.240282 ]\n [ 6.317626 ]\n [ 5.2282495]\n [ 7.789218 ]\n [ 6.276103 ]\n [ 6.8711915]\n [ 6.406598 ]\n [ 7.3938217]\n [ 6.980268 ]\n [ 6.8485537]\n [ 6.113397 ]\n [ 5.3291535]\n [ 4.821697 ]\n [ 7.543131 ]\n [ 5.749691 ]\n [ 7.4695473]\n [ 6.448674 ]\n [ 7.6913157]\n [ 6.0566435]\n [ 7.304752 ]\n [ 4.8935423]\n [ 7.9620028]\n [ 3.9557633]\n [ 7.604338 ]\n [ 6.0491023]\n [ 7.109016 ]\n [ 6.9724946]\n [ 8.22139  ]\n [ 7.5605383]\n [ 6.579627 ]\n [ 8.160171 ]\n [ 7.7622333]\n [ 7.3352275]\n [ 6.417697 ]\n [ 7.005409 ]\n [ 7.532043 ]\n [ 7.516572 ]\n [ 7.8600893]\n [ 6.086522 ]\n [ 7.5101995]\n [ 7.0296516]\n [ 5.8513575]\n [ 6.8469086]\n [ 7.995807 ]\n [ 6.9427543]\n [ 6.8730793]\n [ 7.107653 ]\n [ 8.273667 ]\n [ 7.3882537]\n [ 7.885408 ]\n [ 8.514408 ]\n [ 6.91275  ]\n [ 6.791131 ]\n [ 6.388431 ]\n [ 7.3955607]\n [ 5.7377295]\n [ 5.3604083]\n [ 4.86893  ]\n [ 8.194683 ]\n [ 6.829945 ]\n [ 6.2723784]\n [ 6.896144 ]\n [ 5.5352335]\n [ 6.9669085]\n [ 6.602372 ]\n [ 5.6833973]\n [ 5.3508162]\n [ 7.1554084]\n [ 8.030773 ]\n [ 5.412171 ]\n [ 4.575143 ]\n [ 7.8838053]\n [ 7.64885  ]\n [ 8.95581  ]\n [ 7.4951077]\n [ 6.685517 ]\n [ 6.1289487]\n [ 7.5580387]\n [ 5.8085337]\n [ 6.540604 ]\n [ 5.6460648]\n [ 8.1537695]\n [ 5.566255 ]\n [ 7.4425116]\n [ 6.975315 ]\n [ 7.1047688]\n [ 6.2245636]\n [ 5.1624637]\n [ 6.995179 ]\n [ 5.511699 ]\n [ 6.6752386]\n [ 7.7544622]\n [ 8.541645 ]\n [ 7.590118 ]\n [ 6.4737754]\n [ 5.9875507]\n [ 8.885714 ]\n [ 5.494734 ]\n [ 7.1590333]\n [ 6.808472 ]\n [ 8.266021 ]\n [ 5.950111 ]\n [ 7.1506567]\n [ 8.300691 ]\n [ 7.6312046]\n [ 6.5431213]\n [ 7.0177827]\n [ 6.749807 ]\n [ 6.418311 ]\n [ 6.0023336]]", "q_t_selected": "[ 6.021546   7.1459694  7.6303363  7.3503175  5.7582374  5.806253\n  6.238563   7.755293   6.852913   6.465022   6.795162   6.8268533\n  4.074785   7.2881703 10.514497   9.0550375  7.7612276  5.09708\n  6.835952   6.5443816  6.625332   4.83611    5.4368258  7.3344274\n  5.200565   7.744951   5.2185025  5.589958   7.722257   8.110455\n  6.766716   6.102967   5.937665   6.192793   5.35445    8.000986\n  7.0157094  8.424996   6.410319   5.487419   7.066672   7.74119\n  6.915974   7.2804418  7.38452    6.808472   6.090673   5.9530144\n  8.4236965  7.788331   6.408653   7.0648174  8.25159    7.13865\n  7.6212153  6.8440433  6.828658   6.8663774  7.2536283  6.8344593\n  7.5962973  7.529781   8.257955   5.8244677  7.267512   7.4136662\n  7.679941   5.973453   7.3280473  6.026004   5.3407845  7.5165644\n  7.644201  10.024547   6.5027246  6.009627   6.0968137  6.565275\n  6.469416   6.296116   7.851736   5.13476    7.488879   6.422452\n  6.7614875  5.448886   8.853573   7.3169603  6.2682934  4.6207514\n  6.448698   5.7274795  6.8658166  7.753627   6.5639157  7.1091604\n  4.9788013  8.938311   4.543302   6.937185   4.498145   6.7026043\n  6.3970203  7.923537   6.0254917  6.448096   6.4413877  8.253274\n  6.563882   5.356101   6.5743356  7.573473   5.5582223  6.5782895\n  5.8473506  7.692916   7.8612785  7.278079   6.5374923  6.795162\n  7.931876   5.1171436  6.808472   8.215135   7.479066   6.3242803\n  6.4399405  5.8964047  7.1333947  6.4642153  7.112414   8.741119\n  5.973453   6.7898474  7.6604304  6.2723136  5.3328223  6.450289\n  8.153385   7.284831   5.4238443  5.788635   7.6606746  6.842133\n  6.6296782  5.498847   9.240282   6.317626   5.2282495  7.789218\n  6.276103   6.8711915  6.406598   7.3938217  6.980268   6.8485537\n  6.113397   5.3291535  4.821697   7.543131   5.749691   7.4695473\n  6.448674   7.6913157  6.0566435  7.304752   4.8935423  7.9620028\n  3.9557633  7.604338   6.0491023  7.109016   6.9724946  8.22139\n  7.5605383  6.579627   8.160171   7.7622333  7.3352275  6.417697\n  7.005409   7.532043   7.516572   7.8600893  6.086522   7.5101995\n  7.0296516  5.8513575  6.8469086  7.995807   6.9427543  6.8730793\n  7.107653   8.273667   7.3882537  7.885408   8.514408   6.91275\n  6.791131   6.388431   7.3955607  5.7377295  5.3604083  4.86893\n  8.194683   6.829945   6.2723784  6.896144   5.5352335  6.9669085\n  6.602372   5.6833973  5.3508162  7.1554084  8.030773   5.412171\n  4.575143   7.8838053  7.64885    8.95581    7.4951077  6.685517\n  6.1289487  7.5580387  5.8085337  6.540604   5.6460648  8.1537695\n  5.566255   7.4425116  6.975315   7.1047688  6.2245636  5.1624637\n  6.995179   5.511699   6.6752386  7.7544622  8.541645   7.590118\n  6.4737754  5.9875507  8.885714   5.494734   7.1590333  6.808472\n  8.266021   5.950111   7.1506567  8.300691   7.6312046  6.5431213\n  7.0177827  6.749807   6.418311   6.0023336]", "twin_q_t_selected": "[ 6.026275   7.3712764  7.630508   7.657321   5.1594925  5.5415883\n  5.953911   7.885636   7.000927   6.798709   6.861648   6.7825694\n  3.981143   7.6577864 10.722677   8.675987   7.267118   4.360764\n  6.2977004  6.6447396  6.2151046  5.2884383  5.9928217  7.6508985\n  5.456509   7.8565364  5.621362   5.7194176  7.6859674  7.9984417\n  7.6042786  6.071985   5.7759967  6.3492904  5.572352   7.5857534\n  7.1234183  7.98102    6.839182   5.654857   7.1676583  7.86902\n  6.3897214  6.655854   7.8037844  6.6055155  6.415418   6.383702\n  8.202463   8.076888   6.3588285  6.938146   8.3110075  7.2995143\n  7.1315265  6.69609    6.8158884  6.3875513  7.2092385  6.1198554\n  7.5464954  7.4281454  8.730892   6.4230137  6.8845415  7.3626924\n  7.463303   6.1745157  6.805404   6.1582174  4.95936    7.37218\n  7.420344  10.4513445  6.693659   5.867033   6.245352   5.693449\n  6.385498   6.4114494  7.16267    5.2257395  7.4648056  5.63567\n  6.716941   5.892948   8.130885   7.3020973  6.8159432  4.8129826\n  6.558813   5.718742   6.933067   7.8027425  6.5197377  7.425378\n  5.0939817  8.588591   4.575717   7.055736   4.1903563  6.669853\n  6.5978975  8.429079   5.3347507  6.640026   6.488457   8.144762\n  6.2016783  5.820743   5.9395585  7.502213   6.0127807  6.007489\n  5.530889   7.8525586  7.731873   6.4087896  6.9461865  6.861648\n  7.5695596  4.986663   6.6055155  7.986616   7.7994914  6.1090703\n  6.4167347  6.003568   6.5957513  5.7339153  7.841593   8.259579\n  6.1745157  5.8416386  7.44427    6.420536   5.65654    6.3509736\n  7.8471     7.352982   5.34596    5.782956   6.8714294  6.601978\n  6.482133   5.478816   9.085591   6.0532355  4.6588387  7.4989047\n  6.3040037  6.688675   6.410313   7.251179   7.242998   6.800006\n  7.4220624  5.1531324  4.7095704  7.768338   6.3515105  7.252328\n  6.5657907  7.3691096  6.5773373  7.555302   4.672338   8.935829\n  4.3268533  7.1583366  6.6619587  6.8747683  5.7937737  8.177508\n  7.741571   6.48657    8.2473755  7.8607097  7.3352146  6.7678914\n  7.2050247  7.3825173  7.763333   7.8102264  6.0810184  7.712348\n  6.9481053  5.7142377  6.990446   7.9797826  7.043231   6.4027953\n  7.2208967  7.928449   7.3273697  7.5678678  8.452919   7.1515036\n  6.6985908  6.245889   6.3947115  5.3972425  5.582117   5.0000095\n  7.5490737  6.386388   6.512378   6.6541886  5.807802   7.17363\n  6.134802   5.4217014  5.6199913  7.2082143  8.272391   5.563622\n  4.785275   8.307046   7.7390566  8.875832   6.868529   7.0962586\n  5.1690855  7.1528473  5.064886   6.0297966  5.6346164  8.189239\n  5.9576583  7.709242   6.539411   7.676454   6.7786293  5.2652764\n  6.4222684  5.8984857  6.0800014  7.1818805  8.102286   7.7063026\n  6.2176332  6.2233014  9.217761   5.280306   7.16038    6.6055155\n  8.652205   5.9283533  7.4242473  7.194064   7.1757035  6.839939\n  6.3190827  6.251905   6.205492   5.4129443]", "q_t_selected_target": "[ 5.3172975  5.750498   5.686363   8.216518   4.920046   4.5470896\n  5.4511027  7.0585175  6.703173   5.9154305  6.225239   7.483572\n  5.4294853  9.327708   7.91206    8.739328   7.453542   2.5801933\n  7.3785906  8.276461   6.2814245  7.25349    4.249917   7.8129354\n  5.222748   6.5927906  4.4067035  4.5399137  7.468304   8.6162405\n  7.543642   5.4931846  5.3235927  5.2368755  5.6490016  6.6013083\n  7.838019   7.3582716  6.2934103  5.9669623  7.625527   8.511432\n  7.3168144  7.209285   7.663731   6.8779716  4.1237555  6.3887005\n  6.3162727  8.622672   6.0535374  5.0439925  6.6196566  7.5535526\n  9.04909    6.0277762  5.032734   5.578776   6.041511   6.5173855\n  7.6690235  7.7660823 10.0300045  4.4558554  5.901021   4.348995\n  8.757671   5.824424   7.050926   5.518383   4.7451973  6.4718637\n  7.18903   12.065209   6.364861   5.454362   6.898585   4.7105794\n  5.0990753  7.204791   7.1585546  4.480999   6.0404263  6.9388657\n  7.2117825  4.920105   9.947721   8.2070675  6.2453213  4.4444666\n  6.766845   5.8472037  6.7081676  7.5809584  6.464447   6.2213883\n  5.1278286  6.689513   4.5961175  7.0523305  5.6479335  6.373692\n  7.8813357  9.115656   4.803408   5.376099   7.0552053  9.386882\n  7.7085705  5.865077   6.735157   8.701427   3.9451706  7.319107\n  5.753504   7.717128   7.280893   9.063548   6.1130233  5.9077816\n  8.944379   4.268693   6.1705127  8.5637045  8.343975   7.772521\n  7.975325   6.210286   6.6826744  4.9179068  9.04625    7.6984754\n  6.3657265  5.610069   8.809345   5.732261   7.0386667  6.152199\n  7.7767825  7.8331175  5.6477833  7.1726484  5.7665715  6.524928\n  4.948107   5.1333327  9.428321   6.5182185  3.7073631  6.965633\n  4.757791   7.5228577  7.7504616  6.3778167  5.0825305  9.845072\n  5.8805375  4.523487   4.698022   7.4057274  6.2536044  6.426849\n  6.345581   7.1193333  8.147745   7.5814843  4.546156   9.406357\n  3.4703732  8.217992   5.770279   6.5457277  5.2508936  6.8582325\n  7.7551727  5.376589   8.947947   8.222188   6.789713   7.678094\n  8.269842   7.1860847  6.271302   7.563001   5.7704535  8.634255\n  7.9975777  4.505229   6.6347938  9.3601     4.974822   6.9057417\n  9.424144   9.168257   7.5560822  7.1280417 10.026871   5.4766207\n  7.9795246  5.0648346  7.3925824  5.139637   3.8595467  5.0245543\n  7.0562234  6.681158   5.7506804  6.5698113  5.9871073  6.446275\n  6.7053823  5.4547453  4.32882    6.035344   6.8261414  5.61904\n  4.0230026  8.563886   7.074995   9.803484   8.985836   6.889032\n  4.7496843  7.9172235  4.939858   6.978401   4.9811907  8.836535\n  6.0076523  8.753255   8.959253   6.79603    7.134325   5.829783\n  7.403404   9.260827   6.062408   9.378385   6.6649456  6.8405967\n  6.400313   5.3727684  6.903417   5.6917577  5.8069696  6.2779765\n  7.375553   5.795847   7.079766   7.7827783  6.531328   6.184472\n  6.5909233  5.6527033  5.328989   4.6316633]", "q_tp1_best_masked": "[6.5087295 6.845335  5.7029686 7.1998596 6.0716276 5.837184  6.315155\n 6.6918507 6.88432   6.201241  6.8892803 7.4142995 6.7680144 8.30493\n 7.1353655 7.94363   7.975762  5.154568  8.536435  8.481723  6.770748\n 7.9998374 6.297399  7.8722525 6.2049055 6.2720604 5.6774483 5.7920947\n 7.357433  8.558796  7.3214245 6.5631714 6.295906  5.9286346 6.538894\n 6.5391407 7.987177  7.312498  6.894599  7.30612   7.008753  7.8188496\n 7.560032  6.7930117 7.6573715 6.984628  5.96085   7.275316  7.3764505\n 8.084714  6.686505  5.900843  7.1271667 7.5232754 8.434342  7.0721407\n 5.996609  6.0649357 6.820055  7.951558  7.6242056 8.282028  9.045751\n 5.72178   6.653286  6.349822  8.248636  7.027834  7.138318  7.3223386\n 5.250651  6.945948  7.1373005 9.667219  6.6116877 6.6435614 7.827592\n 6.2393074 6.4979253 7.5576334 7.229313  5.95905   6.5731716 7.293298\n 7.4633074 6.3072443 8.313889  7.67542   6.8084984 6.2927084 7.6701307\n 6.2974024 7.052248  7.962289  6.6332254 6.654853  6.8650923 6.9021683\n 5.607829  7.3303137 7.4589863 6.940093  8.257585  8.63218   6.4135857\n 6.5695    8.008629  8.351274  7.419565  7.1418085 7.2071285 7.9559975\n 5.202321  7.8504176 6.05962   7.5982203 8.173918  8.349098  6.5401597\n 6.5686164 8.764729  5.0833793 6.2700233 7.477017  8.828003  7.5716496\n 8.228821  7.1929545 7.2622194 6.470121  8.802185  7.6320925 7.574605\n 6.861313  7.8971505 6.3457994 7.739854  7.4267783 7.8943024 7.8351507\n 7.44057   7.4334536 6.759941  7.461116  5.78907   7.2402263 8.176884\n 7.71006   5.670089  6.999834  5.9229026 7.9929047 7.4827952 6.611828\n 5.7814817 9.811931  5.957506  6.641032  6.4072495 7.6272497 7.3082027\n 6.7517953 7.3100243 6.9762373 8.314741  7.7326603 6.5122805 9.060863\n 5.3473034 8.1959095 6.6271667 7.154339  6.381405  7.008272  7.5316973\n 6.6911106 8.130811  8.285681  7.1248417 7.7591476 7.7911615 8.138101\n 6.5236797 7.2094593 6.5131207 8.577057  7.909612  5.587041  7.349476\n 8.194021  6.080124  7.637306  9.075165  8.412166  8.359083  6.945956\n 8.809007  6.2457733 7.5415483 5.9829016 7.8346863 6.0995646 5.6769295\n 6.242114  7.505868  6.557205  6.681471  7.32695   7.189498  6.912462\n 7.82654   6.564561  5.876481  6.7272673 6.8580055 6.3655653 6.193433\n 7.961623  6.8309402 9.037374  8.240639  7.4115734 5.611764  7.1455736\n 5.9670973 8.3050375 6.175482  8.158245  6.5247154 7.7719135 8.329747\n 6.909614  7.4572105 6.297803  7.907356  8.958807  6.8136244 8.062291\n 7.4538937 7.4790177 6.3293586 7.0720325 6.521311  6.8840075 6.805132\n 6.3785725 7.797186  7.031008  6.9033866 8.024733  7.051892  6.2479496\n 6.956709  6.9308653 6.5436897 5.589657 ]", "policy_t": "[[ 0.18150234  0.05831289 -0.22324073  0.9346404  -0.31645024  0.8790133 ]\n [-0.8103504   0.20554209 -0.02036405  0.6006603   0.53330255  0.51771593]\n [ 0.7455628   0.0230155  -0.06017452 -0.68996185  0.42081702  0.8394666 ]\n ...\n [-0.03810704 -0.6421508   0.3413911   0.06520522  0.92553926  0.8065033 ]\n [ 0.6455629   0.41705656 -0.5666789   0.73323154 -0.2663144  -0.0324586 ]\n [-0.8920329  -0.6433137   0.6201694   0.36400366 -0.6892716   0.19187653]]", "td_error": "[0.70661306 1.5081251  1.9440589  0.7126992  0.5388191  1.126831\n 0.6451342  0.7619469  0.22374678 0.71643496 0.60316634 0.67886066\n 1.4015212  1.8547299  2.7065272  0.18952513 0.24705482 2.1487288\n 0.8117645  1.6819     0.20511365 2.1912158  1.4649067  0.32027245\n 0.12797213 1.207953   1.0132289  1.1147742  0.23580813 0.5617924\n 0.41878128 0.5942912  0.5332382  1.0341661  0.18560052 1.1920614\n 0.768455   0.8447366  0.33134007 0.3958242  0.5083618  0.7063267\n 0.66396666 0.31229377 0.20963216 0.17097783 2.12929    0.2203424\n 1.9968071  0.6900625  0.3302033  1.9574893  1.6616421  0.3344705\n 1.6727195  0.7422905  1.7895393  1.0481884  1.1899223  0.35730195\n 0.09762716 0.28711915 1.5355811  1.6678853  1.1750057  3.039184\n 1.1860492  0.2495606  0.26132154 0.5737276  0.40487504 0.97250843\n 0.34324217 1.8272638  0.23333073 0.48396802 0.7275021  1.4187827\n 1.3283818  0.8510084  0.34864855 0.6992507  1.4364161  0.9098046\n 0.47256827 0.75081205 1.4554915  0.89753866 0.29679704 0.27240038\n 0.26308966 0.12409306 0.19127417 0.19722629 0.0773797  1.0458808\n 0.0914371  2.0739374  0.03660798 0.05927563 1.3036828  0.31253672\n 1.3838768  0.939348   0.87671304 1.1679618  0.5902829  1.1878638\n 1.3257904  0.27665496 0.47820997 1.1635845  1.8403308  1.0262177\n 0.15823078 0.07982135 0.51568294 2.2201138  0.6288161  0.92062354\n 1.193661   0.7832103  0.53648114 0.4628291  0.7046964  1.5558457\n 1.5469875  0.26029968 0.26882172 1.1811585  1.569247   0.8018737\n 0.2917421  0.7056742  1.256995   0.61416364 1.5439856  0.2484324\n 0.22345996 0.51421094 0.26288104 1.386853   1.4994805  0.19712734\n 1.6077988  0.3554988  0.2653842  0.33278775 1.236181   0.6784284\n 1.5322623  0.74292445 1.342006   0.9446838  2.0291026  3.020792\n 0.88719225 0.7176559  0.06761193 0.25000715 0.30090976 0.9340887\n 0.16165137 0.41087937 1.8307548  0.15145731 0.23678422 0.95744085\n 0.67093515 0.8366544  0.58525157 0.44616437 1.1322405  1.3412166\n 0.10411811 1.1565096  0.7441735  0.41071653 0.54550815 1.0852997\n 1.1646254  0.2711954  1.3686502  0.27215672 0.31331682 1.0229816\n 1.0086992  1.2775686  0.28388357 1.3723049  2.0181706  0.26780438\n 2.2598689  1.0671985  0.19827056 0.5985961  1.5432072  1.555506\n 1.2346637  1.2523255  0.5004246  0.42784905 1.611716   0.09008455\n 0.815655   0.22177863 0.6416979  0.20535493 0.31558943 0.6239941\n 0.33679533 0.13084793 1.1565835  1.1464672  1.3254409  0.13114357\n 0.6572063  0.46846008 0.61895823 0.88766336 1.8040178  0.2053709\n 0.89933276 0.56178045 0.49685192 0.6932008  0.6591499  0.66503143\n 0.24569559 1.1773782  2.2018902  0.59458137 0.6327286  0.6159129\n 0.69468045 3.5557346  0.315212   1.9102132  1.6570201  0.8076136\n 0.12807107 0.7326577  2.1483202  0.30423784 1.352737   0.4290173\n 1.08356    0.14338517 0.20768619 0.55331326 0.87212586 0.50705814\n 0.34934998 0.84815264 0.98291254 1.0759757 ]", "mean_td_error": 0.8509436845779419, "actor_loss": -9.484451293945312, "critic_loss": 0.571267306804657, "alpha_loss": -3.0191609859466553, "alpha_value": 0.7372692227363586, "target_entropy": -6, "mean_q": 6.805295944213867, "max_q": 10.514496803283691, "min_q": 3.955763339996338, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 11000, "episodes_total": 11, "training_iteration": 11, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_14-42-45", "timestamp": 1587048165, "time_this_iter_s": 86.08346390724182, "time_total_s": 115.61449360847473, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 115.61449360847473, "timesteps_since_restore": 11000, "iterations_since_restore": 11, "perf": {"cpu_util_percent": 92.15804195804195, "ram_util_percent": 11.028671328671328}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -187.65127353230946, "episode_reward_min": -436.5397860518293, "episode_reward_mean": -285.08492166717684, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-302.07353943096945, -294.9502097668528, -313.66542549337373, -189.05649998844297, -222.60249767655455, -436.5397860518293, -370.3986784890048, -187.65127353230946, -237.407484362094, -296.50382188033706], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2602639011341408, "mean_processing_ms": 0.11974671048661409, "mean_inference_ms": 1.1695258609880168}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -262.5981647835083, "episode_reward_min": -452.38971113478243, "episode_reward_mean": -371.89335126966097, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-262.5981647835083, -442.1964360316331, -395.7194478314839, -452.38971113478243, -306.56299656689714], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2884686203247403, "mean_processing_ms": 0.40899755145008687, "mean_inference_ms": 1.2768735091221353}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 6000, "num_steps_trained": 510720, "num_steps_sampled": 12000, "sample_time_ms": 2.894, "replay_time_ms": 22.506, "grad_time_ms": 52.647, "update_time_ms": 0.004, "opt_peak_throughput": 4862.62, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[12.068711 ]\n [ 8.785666 ]\n [ 9.817709 ]\n [ 7.966838 ]\n [ 9.382977 ]\n [ 8.788259 ]\n [ 9.294754 ]\n [10.417205 ]\n [11.8995495]\n [ 8.747249 ]\n [ 8.810862 ]\n [10.731521 ]\n [ 9.478642 ]\n [11.545477 ]\n [ 7.9718957]\n [10.459086 ]\n [10.249502 ]\n [ 9.962874 ]\n [10.642349 ]\n [ 7.296705 ]\n [11.093567 ]\n [10.728889 ]\n [10.887254 ]\n [11.810396 ]\n [11.69296  ]\n [10.618545 ]\n [11.125606 ]\n [12.920767 ]\n [ 9.839022 ]\n [11.259601 ]\n [10.385474 ]\n [12.453195 ]\n [11.317421 ]\n [ 9.1671295]\n [10.54749  ]\n [11.036472 ]\n [10.180951 ]\n [10.497708 ]\n [12.023237 ]\n [ 9.831864 ]\n [ 9.170196 ]\n [ 9.79205  ]\n [ 8.550819 ]\n [ 8.065985 ]\n [10.358654 ]\n [ 9.458212 ]\n [ 6.7449284]\n [11.900675 ]\n [ 9.951204 ]\n [ 9.187049 ]\n [10.190598 ]\n [10.357733 ]\n [11.165185 ]\n [11.077447 ]\n [ 9.120718 ]\n [11.082864 ]\n [10.749624 ]\n [ 8.099407 ]\n [10.568906 ]\n [10.198232 ]\n [10.76113  ]\n [11.072869 ]\n [ 9.460046 ]\n [ 9.104269 ]\n [ 8.350021 ]\n [11.463636 ]\n [ 9.175624 ]\n [ 8.389937 ]\n [11.763633 ]\n [10.251779 ]\n [10.866725 ]\n [ 9.408628 ]\n [ 8.88     ]\n [ 9.033863 ]\n [ 9.159729 ]\n [10.825102 ]\n [11.361003 ]\n [ 9.534001 ]\n [11.746618 ]\n [ 7.8456373]\n [10.687685 ]\n [10.420653 ]\n [ 8.9366455]\n [11.344731 ]\n [ 6.3995514]\n [11.596344 ]\n [ 9.38239  ]\n [ 9.388261 ]\n [12.065949 ]\n [10.251604 ]\n [10.135672 ]\n [10.041461 ]\n [10.093639 ]\n [ 9.4559555]\n [13.975449 ]\n [11.899935 ]\n [11.317561 ]\n [12.422922 ]\n [ 8.295162 ]\n [11.4049015]\n [ 9.351968 ]\n [ 9.331765 ]\n [ 8.354999 ]\n [10.409271 ]\n [ 9.617544 ]\n [ 9.829658 ]\n [10.704799 ]\n [12.872815 ]\n [10.429341 ]\n [ 7.1268706]\n [ 9.958819 ]\n [ 9.353333 ]\n [10.702433 ]\n [ 9.482922 ]\n [ 9.355594 ]\n [10.626431 ]\n [ 8.996467 ]\n [10.738899 ]\n [11.302773 ]\n [10.521179 ]\n [11.420045 ]\n [ 9.366746 ]\n [ 8.545579 ]\n [10.293447 ]\n [11.955177 ]\n [ 8.457578 ]\n [ 7.7111955]\n [10.967212 ]\n [ 9.546279 ]\n [ 9.846427 ]\n [10.069436 ]\n [10.302376 ]\n [10.938302 ]\n [ 5.783131 ]\n [ 9.158137 ]\n [ 8.127439 ]\n [ 9.86615  ]\n [ 9.67692  ]\n [ 9.871082 ]\n [10.068654 ]\n [10.962175 ]\n [11.003564 ]\n [14.150234 ]\n [ 9.659588 ]\n [11.196179 ]\n [13.217093 ]\n [13.687152 ]\n [11.678746 ]\n [ 9.980361 ]\n [ 6.759934 ]\n [ 8.60229  ]\n [ 9.511578 ]\n [10.102483 ]\n [10.541802 ]\n [11.331221 ]\n [ 9.878815 ]\n [10.774072 ]\n [ 9.445267 ]\n [12.360402 ]\n [13.437357 ]\n [ 9.536256 ]\n [ 8.933466 ]\n [ 9.027052 ]\n [10.132069 ]\n [10.842926 ]\n [ 8.672291 ]\n [11.006717 ]\n [11.868721 ]\n [ 9.876712 ]\n [12.193631 ]\n [12.242975 ]\n [10.174781 ]\n [ 7.0036974]\n [ 9.929782 ]\n [ 9.756857 ]\n [12.509848 ]\n [11.015025 ]\n [14.718646 ]\n [10.951526 ]\n [ 6.8175325]\n [ 8.898698 ]\n [11.124395 ]\n [10.182597 ]\n [ 9.948885 ]\n [ 8.287783 ]\n [ 9.095382 ]\n [ 7.148779 ]\n [11.08426  ]\n [ 9.474185 ]\n [10.486628 ]\n [11.915403 ]\n [ 9.989425 ]\n [ 8.232444 ]\n [10.164314 ]\n [11.0236435]\n [10.086593 ]\n [10.465519 ]\n [ 9.592429 ]\n [11.685734 ]\n [11.239559 ]\n [12.275071 ]\n [ 9.925614 ]\n [ 9.424674 ]\n [ 9.225149 ]\n [ 9.79611  ]\n [10.646457 ]\n [10.83674  ]\n [11.437667 ]\n [10.226466 ]\n [12.308684 ]\n [10.926325 ]\n [11.055122 ]\n [10.467144 ]\n [10.11667  ]\n [12.305143 ]\n [11.420336 ]\n [ 9.709283 ]\n [11.759233 ]\n [ 8.767613 ]\n [11.711341 ]\n [ 9.42342  ]\n [ 9.74746  ]\n [11.083627 ]\n [ 9.07071  ]\n [10.252624 ]\n [ 9.476607 ]\n [ 7.381669 ]\n [12.13816  ]\n [11.830698 ]\n [11.3698015]\n [10.075932 ]\n [16.356384 ]\n [12.306295 ]\n [12.221689 ]\n [ 8.686553 ]\n [ 8.647807 ]\n [11.591788 ]\n [11.715276 ]\n [ 8.937188 ]\n [12.420953 ]\n [13.217093 ]\n [ 8.530523 ]\n [ 9.818524 ]\n [ 8.490022 ]\n [10.190514 ]\n [ 9.020978 ]\n [11.056245 ]\n [ 9.632162 ]\n [10.69241  ]\n [10.774928 ]\n [ 8.913572 ]\n [10.972443 ]\n [ 9.600349 ]\n [11.000499 ]\n [ 9.615569 ]\n [10.174815 ]]", "q_t_selected": "[12.068711   8.785666   9.817709   7.966838   9.382977   8.788259\n  9.294754  10.417205  11.8995495  8.747249   8.810862  10.731521\n  9.478642  11.545477   7.9718957 10.459086  10.249502   9.962874\n 10.642349   7.296705  11.093567  10.728889  10.887254  11.810396\n 11.69296   10.618545  11.125606  12.920767   9.839022  11.259601\n 10.385474  12.453195  11.317421   9.1671295 10.54749   11.036472\n 10.180951  10.497708  12.023237   9.831864   9.170196   9.79205\n  8.550819   8.065985  10.358654   9.458212   6.7449284 11.900675\n  9.951204   9.187049  10.190598  10.357733  11.165185  11.077447\n  9.120718  11.082864  10.749624   8.099407  10.568906  10.198232\n 10.76113   11.072869   9.460046   9.104269   8.350021  11.463636\n  9.175624   8.389937  11.763633  10.251779  10.866725   9.408628\n  8.88       9.033863   9.159729  10.825102  11.361003   9.534001\n 11.746618   7.8456373 10.687685  10.420653   8.9366455 11.344731\n  6.3995514 11.596344   9.38239    9.388261  12.065949  10.251604\n 10.135672  10.041461  10.093639   9.4559555 13.975449  11.899935\n 11.317561  12.422922   8.295162  11.4049015  9.351968   9.331765\n  8.354999  10.409271   9.617544   9.829658  10.704799  12.872815\n 10.429341   7.1268706  9.958819   9.353333  10.702433   9.482922\n  9.355594  10.626431   8.996467  10.738899  11.302773  10.521179\n 11.420045   9.366746   8.545579  10.293447  11.955177   8.457578\n  7.7111955 10.967212   9.546279   9.846427  10.069436  10.302376\n 10.938302   5.783131   9.158137   8.127439   9.86615    9.67692\n  9.871082  10.068654  10.962175  11.003564  14.150234   9.659588\n 11.196179  13.217093  13.687152  11.678746   9.980361   6.759934\n  8.60229    9.511578  10.102483  10.541802  11.331221   9.878815\n 10.774072   9.445267  12.360402  13.437357   9.536256   8.933466\n  9.027052  10.132069  10.842926   8.672291  11.006717  11.868721\n  9.876712  12.193631  12.242975  10.174781   7.0036974  9.929782\n  9.756857  12.509848  11.015025  14.718646  10.951526   6.8175325\n  8.898698  11.124395  10.182597   9.948885   8.287783   9.095382\n  7.148779  11.08426    9.474185  10.486628  11.915403   9.989425\n  8.232444  10.164314  11.0236435 10.086593  10.465519   9.592429\n 11.685734  11.239559  12.275071   9.925614   9.424674   9.225149\n  9.79611   10.646457  10.83674   11.437667  10.226466  12.308684\n 10.926325  11.055122  10.467144  10.11667   12.305143  11.420336\n  9.709283  11.759233   8.767613  11.711341   9.42342    9.74746\n 11.083627   9.07071   10.252624   9.476607   7.381669  12.13816\n 11.830698  11.3698015 10.075932  16.356384  12.306295  12.221689\n  8.686553   8.647807  11.591788  11.715276   8.937188  12.420953\n 13.217093   8.530523   9.818524   8.490022  10.190514   9.020978\n 11.056245   9.632162  10.69241   10.774928   8.913572  10.972443\n  9.600349  11.000499   9.615569  10.174815 ]", "twin_q_t_selected": "[11.84732    9.132682  10.909693   9.047062   9.318032   9.075689\n  9.845409  10.687638  11.481625   8.973166  10.114053  10.827876\n  9.766536  11.551934   7.737059  10.232517  10.1427145  9.451546\n 10.48236    7.2072954 11.912768   9.766662  11.323957  11.570017\n 11.210628  10.430371  11.1438265 12.4071045  9.60585   10.815313\n 11.2477665 11.91229   10.997842   9.04883   10.332492  11.213455\n 10.596003  10.842324  11.301288   9.752875   9.056163   9.571765\n  8.84227    9.432374  10.321247   9.685351   7.2141256 12.419791\n  9.773942   9.190456   9.347614   9.878717  10.954861  11.20024\n  8.971636  11.473076  10.84057    8.286918  10.735041  10.8230715\n 10.986989  11.801982   9.121907  10.323268   8.371881  11.494473\n  8.760614   8.312955  12.114802  10.34738   11.331233   9.016311\n  9.907124   8.97032    9.358651  10.719186  11.096176   9.707539\n 11.753741   7.999832  10.9841385 10.428314   8.915066  11.262264\n  6.103245  12.095264   9.855991   9.130495  11.79272   10.516147\n  9.376061  10.256837   9.836962   9.58552   13.660524  12.241342\n 11.766867  12.255708   9.0703535 11.360874   9.781467   9.358713\n  8.754363  10.517073   9.036398  10.448586  10.227363  12.9415\n  9.435823   8.363567  10.656783   9.253756  10.841887   9.534449\n  8.721679  10.512167   8.491042  10.829132  10.83547   10.036363\n 11.227582   8.890102   8.37366   10.626445  11.7709255  9.0669\n  7.654263  11.224604   9.785954  10.415833  10.134807  10.310704\n 10.803205   5.577302   8.674158   7.954222  10.290751   9.781212\n  9.729394  10.057389  11.030639  10.905941  13.825561  10.163668\n 11.351311  12.799166  13.395815  11.3638935  9.841138   6.399051\n  8.026686   8.39812   10.163124   9.856738  11.022145  10.124049\n 10.250363  10.0330305 12.069246  12.730718   9.69932    9.082126\n  9.022704   9.721139  10.405199   8.247379  11.232598  11.550686\n 10.507262  12.525541  12.453852  10.038465   7.5437603  9.531238\n 10.366981  12.120109  11.1363535 14.01386   10.828291   7.479657\n  8.39072   10.685942   9.816121  10.197581   8.9814005  9.963619\n  8.054209  10.700097  10.35366   10.170981  11.762463   9.879514\n  8.197966   9.726673  11.33344   10.227555  10.823176   9.65175\n 11.928807  11.40354   12.511667  10.073805   9.875456   9.790147\n  9.587797  10.406515  11.125722  10.669257  10.153333  12.663133\n 11.264546  10.792127  11.006342  10.447267  12.518922  11.122511\n 10.125946  11.159724   7.9451995 11.25284    9.91785    9.792977\n 10.644509   9.39655    9.922601   9.035706   7.9465117 12.4736595\n 12.028733  10.773338   9.533959  15.481162  11.712972  12.107108\n  8.496113   9.446539  11.083233  11.852284   9.283022  12.023514\n 12.799166   8.764335  10.749669   8.431002   9.770789   9.207673\n 11.635747   9.495464  10.477584  10.98465    9.48603   10.775532\n  9.171418  10.627869   9.423517  10.4463215]", "q_t_selected_target": "[10.892708   8.824358  10.057154   6.108873   8.814783   7.8695374\n  9.100885   9.218456  10.326573   8.627643   9.545876  10.940762\n  8.256445  11.865267   6.832107  11.172361   9.280664   9.903768\n 10.014695   5.9170885 11.401905   9.945625  11.424203  10.3674755\n 11.753944  11.486926  11.453394  11.653068   9.5143585 10.70161\n  9.516213  10.981699  11.95107    7.827429  10.301437  10.722142\n 10.578575  10.664031  10.073424  10.329071   9.577699   8.57672\n  9.258246   8.140396   9.761239  10.936557   7.581275  10.856887\n  9.795248   7.9610662  9.785017  11.041314  10.856886  11.727398\n  7.7830405 11.313149  11.85322    7.5438223 10.936889  11.755821\n 11.696243  13.4001875 10.18612   10.465501   8.5207205 10.240887\n  8.345233   8.95655   14.486871  10.172838  12.66219    8.415426\n  7.912493   8.752519   7.910095  11.349447  11.562384   8.39617\n 10.633594   7.8051243 10.885788  10.037982   8.647297  10.989604\n  4.5517387 11.597054   8.307585   9.129549  13.39568   10.694704\n 10.064542   9.925478  10.143421  10.389554  13.583415  13.217169\n 10.974819  13.925083   8.05008   11.461418   8.864167   9.937255\n  8.390233   9.801293   9.312646  10.959516  10.766056  13.674448\n  9.365967   7.283371  10.399233  10.415874  11.082247  10.219949\n  9.850739  11.24352    7.445858  11.202442  10.036277   9.185386\n 10.692086   8.6476145  8.419119   9.8216505 12.172225   9.573091\n  6.079419  10.400264  11.132178   8.055769  11.1328745 11.76069\n 10.21749    5.0599627  8.929909   8.634625   7.188421   9.904126\n 10.223665   9.128786  11.530442  12.366332  15.868595   7.711401\n 10.4184265 11.828438  13.269283  12.880283   9.4120035  5.6745963\n  6.9002376  8.511316   9.288356  11.923447  11.243297   9.31942\n  8.298552   8.886642  12.375532  15.86656    7.96634    8.731476\n  7.761102   9.589969  10.7408085  8.9471655 12.05187   11.726598\n 10.768186  12.309538  13.636829  10.076103   8.661067  11.578623\n  8.507692  13.578378  12.106555  15.62628   11.070491   7.2287207\n  7.262409  10.185587   8.600504   9.0067625  7.500241   8.058276\n  7.1040053  9.879969  10.795297  12.204916  14.344608  10.711629\n  7.0052943 10.323273   9.9145155  8.540362  10.335524   8.891391\n 10.831763  12.381031  13.066001  10.086741  10.693645   8.958285\n  8.917088  10.3448925  9.835402  12.294552   9.754959  12.321855\n 12.135566  12.653214   9.727497  11.407496   9.386376  11.673859\n  9.464887  11.218987   7.6206326 12.5377245 11.663797   8.41279\n  9.992878   8.657262   9.136362   8.083564   6.765797  12.758665\n 11.556158  11.606909   7.5039043 15.360292  13.101832  10.507251\n  8.552107   9.005661  12.621811  10.498057  10.95338   11.782902\n 12.272284   8.318986  10.472239   7.3280873 10.529776   8.427967\n  9.814662   8.538502  10.6810665 10.126542   8.572425  12.057089\n 10.147876   9.215135   8.934468   8.881479 ]", "q_tp1_best_masked": "[10.950241  10.380892  10.540876   8.439355  10.264697   9.536839\n  9.490009   9.900742  10.880558   9.043254  11.539688  10.683396\n  9.749115  11.210005   8.688786  11.417692   9.634432  10.149777\n 10.681868   8.027873  11.293051  10.944351  10.992371  10.409717\n 11.248885  11.600906  11.26012   11.667416  10.123369  10.3179245\n  9.811019  10.960935  11.777727   8.921724  10.579609  11.247807\n 10.619219  11.239685  10.013325  11.045528  10.794924  10.05628\n 10.952875   9.925616  10.095878  12.160425   9.707482  10.538126\n 10.693549   9.643175  10.17638   10.853101  10.552004  11.745029\n  9.715331  10.941368  12.071984   8.913281  11.328581  11.872267\n 12.094454  12.936969  10.645048  11.377031  10.234345  10.289665\n  9.826389   9.594549  13.675045  11.722221  12.542226   9.441314\n  9.265461   9.184768   8.650694  10.68045   11.244535   9.281716\n 10.527682   8.829216  10.789892  10.716467   9.90803   11.206266\n  6.478361  10.855999   8.68218   10.031989  12.351634  11.475805\n 10.258323  10.256664  10.874032  10.516029  12.700744  13.027935\n 11.4790535 12.807891   9.145895  11.631615   9.925326  10.891756\n  9.784436  10.037155   9.108122  11.361712  12.169971  12.863157\n 10.092909   9.282269  10.662014  11.710885  11.8175125 10.276891\n 10.503905  10.689262   9.688818  11.424612  10.173289   9.818759\n 10.381485   9.310667   9.560668  11.048393  11.609669  10.299438\n  8.155026  10.061005  11.875557   9.07325   11.103919  11.7942915\n 10.1666565  7.2502804  9.327545  10.494756   8.400128   9.765738\n 10.097375   9.598753  12.052594  12.193597  14.652847   8.678747\n 10.702892  11.193357  12.784794  12.407287   9.405233   8.457258\n  9.719207  10.3136015  9.8307905 12.290314  11.438321   9.5865965\n  8.581711  10.105263  11.29665   15.373989   9.294857  10.691233\n  8.865349  10.076117  11.152039  10.454692  11.297655  11.631173\n 10.431765  11.526187  13.288561  10.774563   9.419429  11.5153\n 10.006237  13.610441  12.042385  14.049808  11.747323   9.06403\n  8.769359  10.239262   9.755628   9.517295   9.332162   9.889158\n  8.638531  10.021569  11.196583  13.070885  13.320533  10.4758835\n  8.497673  10.685621  10.122503   8.54105   10.7938     9.768301\n 10.793553  12.595427  13.191041  10.306114  10.5886545 10.27424\n  9.583532  11.105123   9.832286  12.126479   9.908593  11.546478\n 12.072474  13.137227   9.87145   11.167397   9.71129   11.5036335\n 10.179069  10.736011   9.344105  12.073177  11.492149   9.6022625\n 10.399363   9.480083  10.134221   8.674006   9.011472  11.56636\n 11.804562  12.173952   8.85622   13.115161  12.766086  11.2073765\n  9.778566   9.504464  11.904314  10.399297  11.792301  11.552347\n 11.641686   8.845343  10.165475   8.211302  10.273947  10.041082\n  9.81204    8.899372  10.334754  10.761427   9.270999  11.723759\n 10.286443   9.746343  10.177339   9.4204445]", "policy_t": "[[-0.5799674   0.6521143  -0.2006405   0.48934364 -0.0514583  -0.02379602]\n [ 0.24251378 -0.6294287   0.6562464   0.29933858  0.503227   -0.33638215]\n [-0.90463364  0.42113733 -0.3782127   0.46918046 -0.28465438  0.5251322 ]\n ...\n [-0.48526287  0.4070902   0.34691656  0.67964303  0.10930204  0.70001626]\n [ 0.80580235 -0.91337174 -0.51003957  0.9764509  -0.7913226  -0.84582716]\n [ 0.68457043  0.38375616 -0.41606808  0.2791984   0.35106814 -0.43428397]]", "td_error": "[1.0653076  0.17350769 0.5459919  2.398077   0.5357213  1.0624366\n 0.46919632 1.3339653  1.3640137  0.23256493 0.6515956  0.1610632\n 1.3661442  0.31656122 1.0223703  0.82655954 0.9154439  0.25566435\n 0.5476594  1.3349116  0.40960073 0.4811139  0.31859732 1.322731\n 0.30215073 0.96246815 0.3186779  1.0108681  0.20807743 0.33584738\n 1.3004069  1.2010431  0.79343843 1.280551   0.13855362 0.40282154\n 0.20752573 0.17230797 1.5888381  0.5367012  0.4645195  1.1051874\n 0.5617018  0.68319464 0.5787115  1.3647752  0.601748   1.3033462\n 0.08863115 1.2276864  0.42149162 0.923089   0.20313692 0.5885544\n 1.2631364  0.19510603 1.0581226  0.64934015 0.28491545 1.2451696\n 0.8221836  1.9627619  0.8951435  0.75173235 0.15976954 1.2381682\n 0.6228862  0.6051035  2.5476532  0.12674093 1.5632114  0.7970433\n 1.4810686  0.24957275 1.3490949  0.5773034  0.33379412 1.2246003\n 1.1165862  0.11761045 0.14822674 0.3865018  0.27855873 0.3138938\n 1.6996593  0.24946022 1.3116059  0.12982893 1.4663458  0.3108287\n 0.3798051  0.22367096 0.17812061 0.8688164  0.23457146 1.1465306\n 0.56739473 1.5857682  0.63267756 0.07853031 0.7025504  0.59201574\n 0.19968224 0.6618786  0.29057312 0.82039356 0.2999754  0.7672906\n 0.5666156  0.61834836 0.34898186 1.112329   0.31008673 0.71126366\n 0.8121023  0.67422056 1.2978964  0.4184265  1.032845   1.0933852\n 0.6317272  0.4808097  0.08595943 0.6382952  0.30917358 0.8108516\n 1.6033101  0.6956439  1.4660616  2.0753613  1.0307531  1.4541497\n 0.6532631  0.6202538  0.24198961 0.59379506 2.89003    0.17506027\n 0.4234271  0.9342356  0.5340352  1.4115796  1.8806977  2.2002268\n 0.85531855 1.1796918  0.2722001  1.3589635  0.49874592 0.90489626\n 1.4142504  0.55672884 0.8444476  1.7241764  0.15453768 0.6820121\n 2.213666   0.85250616 0.16070795 2.7825227  1.6514478  0.27631998\n 1.2637758  0.3366351  0.21886349 0.48733044 0.9322128  0.15901756\n 0.5761986  0.16595507 1.2884159  0.06815815 1.3873382  1.8481131\n 1.5542264  1.2633996  1.0308657  1.2600269  0.18058252 0.33106232\n 1.3822999  0.7195816  1.3988552  1.0664706  1.1343508  1.4712243\n 0.4974885  1.0122099  0.88137436 1.8761115  2.5056753  0.7771597\n 1.2099104  0.377779   1.2640262  1.6167116  0.30882406 0.7306986\n 0.97550726 1.0594816  0.67263174 0.08703184 1.0435796  0.54936266\n 0.7748661  0.18159342 1.1458292  1.2410898  0.43494034 0.17722416\n 1.0401301  1.7295899  1.0092459  1.1255283  3.0256562  0.4024353\n 0.4527278  0.29975462 0.7357738  1.055634   1.9931626  1.3574286\n 0.8711901  0.57636833 0.9512501  1.1725926  0.89829326 0.45275545\n 0.37355757 0.5353389  2.3010411  0.55848074 1.0921988  1.6571479\n 0.09522009 0.3993659  1.2843003  1.2857227  1.8432746  0.43933153\n 0.73584604 0.32844305 0.46557236 1.1324244  0.54912424 0.68635845\n 1.5313339  1.0253115  0.10741329 0.7532468  0.6273761  1.1831017\n 0.761992   1.5990491  0.5850749  1.4290891 ]", "mean_td_error": 0.853169858455658, "actor_loss": -12.307074546813965, "critic_loss": 0.5486111640930176, "alpha_loss": -6.050596237182617, "alpha_value": 0.5457835793495178, "target_entropy": -6, "mean_q": 10.253630638122559, "max_q": 16.35638427734375, "min_q": 5.783131122589111, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 12000, "episodes_total": 12, "training_iteration": 12, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_14-44-28", "timestamp": 1587048268, "time_this_iter_s": 87.159592628479, "time_total_s": 202.77408623695374, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 202.77408623695374, "timesteps_since_restore": 12000, "iterations_since_restore": 12, "perf": {"cpu_util_percent": 92.20620689655173, "ram_util_percent": 11.200000000000001}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -143.45266638968494, "episode_reward_min": -288.7428453463488, "episode_reward_mean": -221.96746052454802, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-270.7486336801875, -154.16026066590703, -281.9317350044394, -211.19983004893191, -223.14750383527593, -143.45266638968494, -173.7785907721119, -226.51892830709542, -245.99361119549712, -288.7428453463488], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26009840780140514, "mean_processing_ms": 0.11947666581182219, "mean_inference_ms": 1.1702029105290799}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -94.96187381235399, "episode_reward_min": -452.38971113478243, "episode_reward_mean": -302.4464388258051, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-94.96187381235399, -395.7194478314839, -452.38971113478243, -306.56299656689714, -262.5981647835083], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2931831995297922, "mean_processing_ms": 0.4144067996642494, "mean_inference_ms": 1.3124758616926764}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 6500, "num_steps_trained": 766720, "num_steps_sampled": 13000, "sample_time_ms": 3.093, "replay_time_ms": 23.718, "grad_time_ms": 57.17, "update_time_ms": 0.004, "opt_peak_throughput": 4477.898, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[10.570323 ]\n [15.993519 ]\n [10.653598 ]\n [ 7.535949 ]\n [13.809938 ]\n [13.44222  ]\n [12.589233 ]\n [ 8.100152 ]\n [10.469182 ]\n [12.361974 ]\n [11.643417 ]\n [13.594369 ]\n [13.055126 ]\n [12.938529 ]\n [14.426334 ]\n [11.061038 ]\n [13.169502 ]\n [12.338733 ]\n [11.350566 ]\n [11.916171 ]\n [12.744077 ]\n [12.242089 ]\n [ 7.7402673]\n [12.60589  ]\n [ 8.976524 ]\n [12.996339 ]\n [13.888832 ]\n [ 9.37709  ]\n [13.789434 ]\n [ 8.22896  ]\n [14.304162 ]\n [12.701037 ]\n [12.446135 ]\n [10.14396  ]\n [13.433375 ]\n [10.782345 ]\n [10.2791   ]\n [ 8.74806  ]\n [14.271079 ]\n [12.338417 ]\n [ 8.622116 ]\n [10.601066 ]\n [10.646992 ]\n [ 8.849817 ]\n [13.382602 ]\n [ 8.453606 ]\n [ 9.663503 ]\n [10.113207 ]\n [13.469569 ]\n [13.371596 ]\n [11.930755 ]\n [15.206573 ]\n [13.243959 ]\n [11.325918 ]\n [14.72445  ]\n [ 8.55023  ]\n [11.993455 ]\n [12.887551 ]\n [11.097535 ]\n [14.380945 ]\n [12.2654295]\n [11.165216 ]\n [11.02053  ]\n [12.665771 ]\n [11.365607 ]\n [16.988997 ]\n [12.036656 ]\n [11.423485 ]\n [ 4.7807827]\n [12.403615 ]\n [10.42522  ]\n [12.900342 ]\n [12.33997  ]\n [15.952707 ]\n [12.750688 ]\n [11.70847  ]\n [14.061507 ]\n [10.75647  ]\n [13.530285 ]\n [13.879729 ]\n [12.522043 ]\n [13.727406 ]\n [11.181845 ]\n [13.593079 ]\n [13.8511305]\n [11.709382 ]\n [11.017983 ]\n [15.208078 ]\n [10.350396 ]\n [13.2634325]\n [14.562375 ]\n [ 9.770178 ]\n [14.121962 ]\n [12.418626 ]\n [13.259784 ]\n [11.794683 ]\n [11.170548 ]\n [13.283288 ]\n [11.448993 ]\n [14.403319 ]\n [16.558287 ]\n [10.805865 ]\n [13.501084 ]\n [13.474369 ]\n [10.569609 ]\n [10.824344 ]\n [13.015641 ]\n [11.408042 ]\n [13.495725 ]\n [17.24804  ]\n [13.275862 ]\n [11.578711 ]\n [12.610634 ]\n [13.6986885]\n [14.658372 ]\n [12.379248 ]\n [10.188031 ]\n [13.926614 ]\n [10.014311 ]\n [12.839631 ]\n [11.428409 ]\n [11.714896 ]\n [13.089116 ]\n [11.593287 ]\n [ 9.596807 ]\n [10.90996  ]\n [14.1218605]\n [11.866347 ]\n [12.91797  ]\n [11.829102 ]\n [11.694913 ]\n [11.362667 ]\n [13.500277 ]\n [12.790938 ]\n [12.653007 ]\n [12.97974  ]\n [12.400332 ]\n [12.493458 ]\n [13.382828 ]\n [18.165794 ]\n [12.239187 ]\n [ 9.018802 ]\n [13.28876  ]\n [10.847466 ]\n [13.459094 ]\n [12.668935 ]\n [15.179763 ]\n [11.978925 ]\n [13.255186 ]\n [12.129148 ]\n [12.007032 ]\n [13.089116 ]\n [13.184484 ]\n [11.233026 ]\n [13.937311 ]\n [13.609317 ]\n [12.156846 ]\n [14.32175  ]\n [13.955922 ]\n [10.932082 ]\n [14.214077 ]\n [16.995794 ]\n [10.371619 ]\n [16.159853 ]\n [16.204378 ]\n [11.497657 ]\n [ 9.124433 ]\n [11.138875 ]\n [11.200183 ]\n [14.873867 ]\n [13.446055 ]\n [12.435168 ]\n [12.1791315]\n [13.792719 ]\n [11.47199  ]\n [11.552999 ]\n [12.964169 ]\n [13.549511 ]\n [12.115026 ]\n [13.657212 ]\n [13.461075 ]\n [17.415976 ]\n [10.573748 ]\n [11.639301 ]\n [13.637691 ]\n [12.324252 ]\n [12.930347 ]\n [12.554237 ]\n [10.88889  ]\n [13.668166 ]\n [14.904557 ]\n [12.545248 ]\n [15.589057 ]\n [14.05176  ]\n [12.170818 ]\n [ 7.6559668]\n [11.934428 ]\n [12.874689 ]\n [12.062868 ]\n [12.699587 ]\n [15.149074 ]\n [11.943823 ]\n [11.494739 ]\n [13.703737 ]\n [14.206711 ]\n [10.747876 ]\n [14.114269 ]\n [13.32265  ]\n [11.005869 ]\n [16.7944   ]\n [12.330653 ]\n [14.627262 ]\n [12.127572 ]\n [ 7.3709254]\n [14.335424 ]\n [12.990191 ]\n [11.474165 ]\n [10.873131 ]\n [ 9.623118 ]\n [12.195538 ]\n [12.502164 ]\n [12.389506 ]\n [ 9.68148  ]\n [12.425985 ]\n [ 8.676026 ]\n [12.053444 ]\n [17.12552  ]\n [12.37192  ]\n [13.007574 ]\n [15.10748  ]\n [12.144579 ]\n [15.535501 ]\n [12.676764 ]\n [14.32506  ]\n [10.463801 ]\n [13.790094 ]\n [10.755669 ]\n [13.878731 ]\n [ 4.7807827]\n [11.2642565]\n [11.511208 ]\n [12.868558 ]\n [10.572817 ]\n [10.124598 ]\n [10.708489 ]\n [12.808973 ]\n [15.677162 ]\n [11.44262  ]\n [11.277196 ]\n [10.845272 ]\n [13.600929 ]\n [13.60739  ]\n [10.880511 ]\n [ 8.044257 ]\n [11.879117 ]\n [17.794947 ]]", "q_t_selected": "[10.570323  15.993519  10.653598   7.535949  13.809938  13.44222\n 12.589233   8.100152  10.469182  12.361974  11.643417  13.594369\n 13.055126  12.938529  14.426334  11.061038  13.169502  12.338733\n 11.350566  11.916171  12.744077  12.242089   7.7402673 12.60589\n  8.976524  12.996339  13.888832   9.37709   13.789434   8.22896\n 14.304162  12.701037  12.446135  10.14396   13.433375  10.782345\n 10.2791     8.74806   14.271079  12.338417   8.622116  10.601066\n 10.646992   8.849817  13.382602   8.453606   9.663503  10.113207\n 13.469569  13.371596  11.930755  15.206573  13.243959  11.325918\n 14.72445    8.55023   11.993455  12.887551  11.097535  14.380945\n 12.2654295 11.165216  11.02053   12.665771  11.365607  16.988997\n 12.036656  11.423485   4.7807827 12.403615  10.42522   12.900342\n 12.33997   15.952707  12.750688  11.70847   14.061507  10.75647\n 13.530285  13.879729  12.522043  13.727406  11.181845  13.593079\n 13.8511305 11.709382  11.017983  15.208078  10.350396  13.2634325\n 14.562375   9.770178  14.121962  12.418626  13.259784  11.794683\n 11.170548  13.283288  11.448993  14.403319  16.558287  10.805865\n 13.501084  13.474369  10.569609  10.824344  13.015641  11.408042\n 13.495725  17.24804   13.275862  11.578711  12.610634  13.6986885\n 14.658372  12.379248  10.188031  13.926614  10.014311  12.839631\n 11.428409  11.714896  13.089116  11.593287   9.596807  10.90996\n 14.1218605 11.866347  12.91797   11.829102  11.694913  11.362667\n 13.500277  12.790938  12.653007  12.97974   12.400332  12.493458\n 13.382828  18.165794  12.239187   9.018802  13.28876   10.847466\n 13.459094  12.668935  15.179763  11.978925  13.255186  12.129148\n 12.007032  13.089116  13.184484  11.233026  13.937311  13.609317\n 12.156846  14.32175   13.955922  10.932082  14.214077  16.995794\n 10.371619  16.159853  16.204378  11.497657   9.124433  11.138875\n 11.200183  14.873867  13.446055  12.435168  12.1791315 13.792719\n 11.47199   11.552999  12.964169  13.549511  12.115026  13.657212\n 13.461075  17.415976  10.573748  11.639301  13.637691  12.324252\n 12.930347  12.554237  10.88889   13.668166  14.904557  12.545248\n 15.589057  14.05176   12.170818   7.6559668 11.934428  12.874689\n 12.062868  12.699587  15.149074  11.943823  11.494739  13.703737\n 14.206711  10.747876  14.114269  13.32265   11.005869  16.7944\n 12.330653  14.627262  12.127572   7.3709254 14.335424  12.990191\n 11.474165  10.873131   9.623118  12.195538  12.502164  12.389506\n  9.68148   12.425985   8.676026  12.053444  17.12552   12.37192\n 13.007574  15.10748   12.144579  15.535501  12.676764  14.32506\n 10.463801  13.790094  10.755669  13.878731   4.7807827 11.2642565\n 11.511208  12.868558  10.572817  10.124598  10.708489  12.808973\n 15.677162  11.44262   11.277196  10.845272  13.600929  13.60739\n 10.880511   8.044257  11.879117  17.794947 ]", "twin_q_t_selected": "[10.991927  16.682686  10.422666   7.9641886 13.102788  14.260714\n 12.767301   8.227626  10.7634535 11.56963   12.35292   14.381634\n 13.585381  13.20932   13.504632  11.757606  12.810035  12.674661\n 11.831494  11.637912  12.675355  11.969569   9.0345    12.40098\n  9.275287  12.889164  13.522048   9.526258  13.410604   8.054415\n 13.587971  12.852288  12.475139  10.08389   12.944527  10.727825\n  9.949704   8.816305  13.806643  12.092205   9.489722  11.116038\n 10.247612   8.663062  13.7329235  8.830054   9.828471  10.183776\n 13.424962  13.842865  12.120984  15.539299  14.243368  10.956642\n 14.709465   8.488906  12.553769  12.577194  12.003877  15.050975\n 11.835527  11.137168  10.715141  11.755531  11.46717   17.116652\n 11.704526  11.377153   5.170309  12.035749   9.766333  13.468342\n 12.781293  15.679628  12.534234  12.149248  13.938868  10.240805\n 13.384314  13.921122  12.443017  13.277453  10.055046  13.382733\n 13.434261  11.510395  10.627605  14.992306  11.120659  13.398069\n 14.079059   9.459395  14.6682825 12.469168  12.997086  11.852043\n 11.277251  13.27952   11.277478  13.95413   15.363217  10.853443\n 13.363132  12.599238  10.905273  10.926586  13.600722  12.120606\n 13.646356  17.12755   13.180417  12.369906  12.088295  13.940923\n 14.173093  12.235492   9.971191  14.1466675 10.411013  12.241872\n 11.124695  12.300403  13.29877   11.941707   9.994231  11.651528\n 14.45996   11.515997  12.312602  10.247342  11.593481  11.139404\n 13.793862  12.5878105 12.725772  12.248036  11.595048  12.587812\n 12.558266  17.668167  12.911382   9.802878  12.992152  11.184544\n 12.753352  12.649089  15.278351  11.66882   13.175493  11.663447\n 11.468403  13.29877   12.704537  12.348018  13.806831  13.202877\n 12.125915  14.279837  14.710411  10.875467  13.990603  16.982655\n 11.222626  16.055237  15.6722765 12.651537   8.695631  11.502246\n 11.150782  15.245973  13.1849785 12.81427   12.274281  13.0409355\n 12.524826  11.411954  13.081784  12.983176  11.705562  14.14553\n 13.608208  16.818922  11.039209  11.599756  13.287874  12.395339\n 12.151926  12.287539  11.133557  13.224079  14.271184  12.546149\n 15.478858  14.154409  11.988687   7.604801  11.364091  13.255485\n 13.07345   12.998802  15.680938  11.958958  11.299314  14.318514\n 13.890133   9.525211  14.9884405 13.554701  10.917723  17.074331\n 12.713461  15.536168  11.775406   7.2367864 14.468197  12.934215\n 11.594178  11.377801   9.335515  12.846736  12.9254465 12.861393\n  9.417292  12.503163   8.282848  12.351554  15.940682  11.775131\n 13.04082   15.705669  12.317532  15.861605  12.531083  14.151996\n 10.21079   13.091646  11.403609  13.675403   5.170309  11.498823\n 11.549918  12.5194    10.965338   9.577704  10.018609  12.861896\n 15.741085  11.765455  11.634884  10.522281  13.651355  13.892575\n 10.690367   7.3405805 12.651899  18.12765  ]", "q_t_selected_target": "[10.427921  17.85733   10.42312    6.8065434 14.508013  15.150061\n 14.024941   6.605779  11.434147  12.957116  12.899065  15.207094\n 12.925024  14.988321  14.386304  11.384477  12.862034  12.327658\n 11.611483  12.633668  11.461399  12.852055   7.8405533 13.117283\n  9.422616  13.875343  12.200845   8.026829  13.56457    6.762362\n 13.85066   12.866896  11.51088    9.761601  12.299468   9.402931\n  9.831363   8.097706  14.504587  11.561632   7.4654775 11.521334\n 11.184688   7.7906904 14.973299   9.192562   9.8684225 10.66889\n 12.58379   13.926431  11.989611  17.053843  13.859245  11.742036\n 16.256916   8.682419  12.656695  13.417091  11.943222  14.385693\n 12.231629  11.814378  11.085586  11.126803  10.196394  17.577179\n 13.576027  11.649217   4.7928    11.920277  10.103674  12.950989\n 12.837821  15.278403  14.114041  13.820478  15.650719   9.991187\n 13.060473  12.366755  11.987499  13.605949   9.452105  12.891143\n 14.146314  13.253044   9.256562  15.542953  11.6575775 13.595408\n 13.131979   7.8512897 15.174491  11.875331  12.588119  13.445481\n 10.916406  12.076332  12.486607  14.178308  15.635079  11.059364\n 14.548395  13.299755  11.264889   9.765611  12.226604  11.897453\n 13.169943  17.99609   13.073178  11.800363  13.202961  13.101737\n 13.789546  11.52317   10.49804   14.01369   11.285304  13.648098\n 12.084376  13.344443  14.5013075 11.000255   9.813841  11.469854\n 13.087683  12.933833  12.158704  12.450002  10.927315  11.719181\n 13.838918  12.205759  12.875485  11.907219  12.013058  12.278224\n 13.232139  19.90231   13.759084   8.293484  12.146154  10.694069\n 13.656605  12.412232  15.146707  11.719545  14.516901  12.373804\n 11.049603  14.677051  13.077737  12.525944  15.407498  12.159633\n 11.850605  16.257261  14.016332  11.491882  13.016183  16.402771\n 10.648703  14.425155  16.109684  12.634527   9.302434  11.133071\n 10.347157  14.89408   14.9920225 12.0246935 11.59158   14.876935\n 11.974063  11.787772  11.124506  12.823706  12.3029585 13.891312\n 12.643984  18.716806  10.477757  12.26305   12.799349  10.833865\n 12.954596  13.750622  10.534258  14.047818  15.88862   11.168763\n 14.803229  12.000501  13.159774   7.4081254 11.54838   13.751033\n 11.439549  12.307923  14.302267  10.432988  10.753122  13.919053\n 15.244823  10.887526  13.728774  14.493458  12.72193   16.149218\n 11.98379   14.881801  12.322343   5.1673374 14.184432  11.0863695\n 13.01529   11.741141   9.349791  11.931531  11.590916  11.857442\n  9.947313  13.387508   8.311885  10.536727  15.181148  11.716433\n 13.974633  16.627987  12.076963  14.637335  12.1500635 13.889188\n  9.567436  11.897037   9.927898  13.010395   4.5743012 11.398395\n 10.633451  14.309674  11.482761   9.6404505  9.008356  13.042223\n 14.799556  13.049885  11.260373  10.668039  11.892049  14.340529\n 11.942916   9.184099  10.538024  19.39672  ]", "q_tp1_best_masked": "[11.309895  17.325426  12.779481   9.972239  14.379837  15.751214\n 14.292199   8.594726  12.79715   13.580229  13.204051  14.56305\n 13.170311  14.94638   14.182159  12.225706  13.309471  12.379101\n 12.633755  13.617637  11.963771  13.391368  10.354567  13.11967\n 10.884514  13.467376  12.109077   9.451634  13.997773   8.523019\n 13.870018  12.878357  12.395655  11.042492  12.037258  11.295584\n 10.5650425 10.093351  14.600101  11.237442   8.864817  12.852253\n 12.725711   9.107597  14.460122  11.182098  10.374226  11.9727\n 12.985735  13.883772  13.357685  16.971903  13.000998  12.869413\n 16.02067    9.598526  13.123342  13.363699  13.141818  13.806257\n 12.599289  11.786341  12.510942  12.074129  11.188841  16.553682\n 13.6788845 12.7211075  7.009611  12.419695  10.95498   13.6549635\n 13.398997  15.436092  14.773411  14.4501295 15.539459  10.874639\n 13.214896  12.746025  12.901833  14.325345  12.296851  12.714614\n 14.747011  12.971195   9.675628  15.962957  12.667276  14.22849\n 13.119221   9.227555  15.092918  11.626754  12.828709  13.882186\n 13.365704  12.610523  12.9961    15.470835  15.243693  12.675458\n 14.878314  13.756399  12.042265  10.3693495 12.877451  12.629344\n 13.931063  16.156538  13.059812  12.395276  13.759932  12.889603\n 13.26356   12.339588  11.591665  13.502537  11.916272  14.703196\n 12.562479  13.332789  14.387957  10.826556  10.310886  12.78331\n 12.644974  13.728516  13.004519  14.1454315 11.413246  12.89746\n 13.398093  12.148416  13.60905   13.133663  12.745636  12.457638\n 12.204681  18.737139  13.871772   9.940254  12.68708   10.752307\n 13.27059   13.149494  14.806449  12.01037   14.197336  12.570808\n 11.574903  14.5654745 14.006369  13.057607  14.535577  13.066584\n 12.979423  16.04047   14.279042  12.564134  13.664122  15.5402355\n 11.14998   13.963633  15.480399  12.74066   10.702673  11.306503\n 10.356952  14.15149   15.116945  12.749448  11.710822  15.400249\n 12.231058  12.28581   12.043716  13.429453  12.643701  14.615251\n 12.438777  18.471846  11.89893   12.324274  12.481925  11.222757\n 13.259566  13.99256   10.850902  14.020966  15.149887  11.60429\n 13.276963  12.096188  14.307271   8.649442  12.445274  14.5144615\n 12.700693  12.362518  13.632985  11.618776  11.652722  14.495041\n 14.923897  12.293431  13.081072  14.678494  13.761722  15.037884\n 12.334641  14.118509  12.618582   7.408102  13.741367  10.638368\n 12.533024  12.195903  11.151019  12.193797  11.440859  12.157353\n 10.950822  13.574467   9.558596  10.50266   15.193085  12.594786\n 14.478372  16.686892  13.358299  13.779685  12.5599985 13.987401\n 11.09203   11.959562  10.781077  13.385342   6.7889056 12.805463\n 11.419358  13.874172  13.009958  10.155466  10.241833  13.799219\n 13.953686  13.169866  12.225887  11.774359  12.512576  14.22857\n 13.78051   11.68036   11.606039  18.026306 ]", "policy_t": "[[-0.1928764   0.9017055  -0.848037    0.24386883  0.4474392   0.44665945]\n [ 0.0627538  -0.8356868   0.1241864  -0.7182543   0.82929015  0.07952106]\n [-0.07310063 -0.6480763  -0.9077122   0.89539194  0.8320725   0.70377946]\n ...\n [-0.9342163  -0.2768349  -0.78216785 -0.41459608  0.2833588   0.85667694]\n [ 0.07959545  0.27970827  0.25334132 -0.8975708   0.5992615   0.1333561 ]\n [-0.20679116 -0.5485829  -0.35112536 -0.84582865  0.5879359  -0.7811432 ]]", "td_error": "[0.35320377 1.519228   0.11546612 0.94352555 1.0516496  1.298594\n 1.3466744  1.5581098  0.81782913 0.9913144  0.90089655 1.2190928\n 0.39522934 1.9143968  0.4608512  0.34828377 0.17973375 0.179039\n 0.24046421 0.8566265  1.2483168  0.74622536 0.6471164  0.61384773\n 0.2967105  0.9325919  1.5045953  1.4248457  0.18941545 1.3793254\n 0.35809565 0.09023285 0.94975615 0.35232353 0.889483   1.3521538\n 0.28303957 0.68447685 0.46572638 0.6536789  1.5904417  0.6627817\n 0.73738575 0.96574926 1.4155364  0.55073214 0.12243557 0.5203986\n 0.8634758  0.31920004 0.09511471 1.6809068  0.49970436 0.6007557\n 1.5399585  0.16285086 0.38308334 0.6847186  0.45317078 0.33501482\n 0.21495104 0.6631856  0.21775007 1.0838475  1.2199945  0.52435493\n 1.7054358  0.24889755 0.19476318 0.29940557 0.32944345 0.28399992\n 0.27718973 0.53776455 1.4715805  1.8916192  1.6505313  0.5074501\n 0.3968258  1.5336709  0.49503088 0.22497606 1.1663408  0.59676313\n 0.50361776 1.6431556  1.5662322  0.44276047 0.92205    0.2646575\n 1.1887379  1.7634969  0.7793689  0.56856585 0.5403161  1.622118\n 0.30749416 1.2050719  1.1233711  0.2245946  0.59753466 0.2297101\n 1.1162868  0.43756533 0.5274477  1.1098542  1.0815773  0.35628223\n 0.4010973  0.80829525 0.15496111 0.39559793 0.85349655 0.7180686\n 0.6261864  0.78419924 0.4184289  0.11002684 1.0726423  1.1073465\n 0.8078246  1.3367939  1.3073645  0.76724243 0.19871235 0.37078428\n 1.2032275  1.242661   0.45658207 1.4117799  0.7168822  0.46814537\n 0.19184828 0.4836154  0.18609619 0.70666933 0.40264225 0.26241112\n 0.41228104 1.9853287  1.1837993  1.1173563  0.9943018  0.32193613\n 0.55038166 0.24677944 0.08235025 0.15505219 1.3015614  0.47750664\n 0.68811417 1.4831076  0.23997307 0.73542213 1.5354271  1.2464643\n 0.2907753  1.9564681  0.37724447 0.5881076  1.0861573  0.58645344\n 0.42550325 1.6823902  0.26605082 0.57694006 0.39240217 0.18748951\n 0.82832575 0.1860528  1.6765056  0.60002565 0.63512564 1.4601078\n 0.5264182  0.30529594 1.8984704  0.44263792 0.39266443 0.24415874\n 0.8906574  1.5993576  0.32872105 0.6435213  0.66343355 1.5259304\n 0.41345882 1.3297338  0.4769659  0.60169554 1.3007498  1.3769355\n 0.73072815 2.102584   1.0800214  0.22225857 0.28516865 0.685946\n 1.1286097  0.5412712  1.1127386  1.5184021  0.64390373 0.3073883\n 1.1964016  0.7509818  0.8225808  1.0547824  1.7601337  0.78514767\n 0.53826666 0.454453   0.3708539  2.1365185  0.21737862 1.8758335\n 1.4811187  0.61567545 0.14380169 0.5896058  1.1228895  0.76800776\n 0.39792728 0.92293406 0.196589   1.665772   1.351954   0.35709286\n 0.9504361  1.2214122  0.15409184 1.0612178  0.4538598  0.34933996\n 0.7698593  1.5438337  1.1517406  0.76667166 0.40124464 0.11728334\n 0.8971114  1.6156955  0.7136841  0.27344656 1.3551931  0.20678854\n 0.90956783 1.445847   0.19566679 0.16149569 1.7340932  0.5905466\n 1.1574769  1.4916804  1.7274842  1.4354229 ]", "mean_td_error": 0.7971594929695129, "actor_loss": -13.930516242980957, "critic_loss": 0.4696289300918579, "alpha_loss": -9.017951965332031, "alpha_value": 0.4044926166534424, "target_entropy": -6, "mean_q": 12.364320755004883, "max_q": 18.165794372558594, "min_q": 4.780782699584961, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 13000, "episodes_total": 13, "training_iteration": 13, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_14-46-09", "timestamp": 1587048369, "time_this_iter_s": 85.07604050636292, "time_total_s": 287.85012674331665, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 287.85012674331665, "timesteps_since_restore": 13000, "iterations_since_restore": 13, "perf": {"cpu_util_percent": 92.28251748251746, "ram_util_percent": 11.200699300699302}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -145.76190572774163, "episode_reward_min": -318.1547145511193, "episode_reward_mean": -225.66229057129704, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-222.8254272825948, -235.97479997272143, -145.76190572774163, -239.3141467675306, -318.1547145511193, -222.81991431782797, -197.45645414928455, -196.76186904300238, -233.01905410974092, -244.53461979140704], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2602259902460542, "mean_processing_ms": 0.11977878279204042, "mean_inference_ms": 1.1723635283817941}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -94.96187381235399, "episode_reward_min": -452.38971113478243, "episode_reward_mean": -250.62565661579674, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-136.6155367814418, -452.38971113478243, -306.56299656689714, -262.5981647835083, -94.96187381235399], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.29912377381337385, "mean_processing_ms": 0.4215988922466197, "mean_inference_ms": 1.3572520233039815}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 7000, "num_steps_trained": 1022720, "num_steps_sampled": 14000, "sample_time_ms": 2.984, "replay_time_ms": 20.212, "grad_time_ms": 51.038, "update_time_ms": 0.004, "opt_peak_throughput": 5015.915, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[12.943509 ]\n [21.35447  ]\n [13.143145 ]\n [13.312378 ]\n [11.354406 ]\n [ 7.0305347]\n [16.23798  ]\n [16.114594 ]\n [13.126923 ]\n [12.342951 ]\n [14.926931 ]\n [11.229014 ]\n [12.855781 ]\n [16.275633 ]\n [12.985301 ]\n [12.114595 ]\n [16.135242 ]\n [13.13722  ]\n [14.431715 ]\n [13.191846 ]\n [13.351438 ]\n [13.521589 ]\n [14.990288 ]\n [12.941327 ]\n [13.805411 ]\n [15.156637 ]\n [13.440551 ]\n [13.402143 ]\n [14.378883 ]\n [14.763368 ]\n [16.116121 ]\n [16.991077 ]\n [16.21866  ]\n [14.710827 ]\n [14.404875 ]\n [13.867271 ]\n [14.4866495]\n [11.628213 ]\n [16.032213 ]\n [15.71551  ]\n [17.40998  ]\n [12.3466835]\n [16.131952 ]\n [18.528908 ]\n [14.975551 ]\n [14.127801 ]\n [13.96181  ]\n [12.453702 ]\n [14.286154 ]\n [17.458847 ]\n [13.658911 ]\n [13.382701 ]\n [10.547787 ]\n [16.492014 ]\n [13.244623 ]\n [14.369008 ]\n [14.94877  ]\n [15.488331 ]\n [14.964789 ]\n [13.289874 ]\n [16.245417 ]\n [11.331593 ]\n [12.143556 ]\n [12.784482 ]\n [17.080135 ]\n [ 8.941975 ]\n [13.323104 ]\n [14.994817 ]\n [13.959568 ]\n [12.451076 ]\n [13.03918  ]\n [16.420784 ]\n [16.664585 ]\n [13.649098 ]\n [14.103758 ]\n [13.908088 ]\n [16.752483 ]\n [13.8843975]\n [12.086504 ]\n [11.256163 ]\n [16.615007 ]\n [13.384368 ]\n [11.867251 ]\n [ 9.832679 ]\n [14.958484 ]\n [11.88023  ]\n [14.134    ]\n [15.898349 ]\n [14.868248 ]\n [13.485869 ]\n [18.023441 ]\n [13.262826 ]\n [12.608695 ]\n [21.001923 ]\n [16.89358  ]\n [17.712856 ]\n [13.1742525]\n [14.071323 ]\n [14.240146 ]\n [13.603451 ]\n [16.303995 ]\n [14.507604 ]\n [13.556123 ]\n [14.657844 ]\n [15.267664 ]\n [13.949718 ]\n [15.724931 ]\n [15.7247   ]\n [ 9.197373 ]\n [13.162499 ]\n [15.789796 ]\n [13.533357 ]\n [14.037597 ]\n [13.225967 ]\n [14.566724 ]\n [14.630524 ]\n [16.202738 ]\n [16.439905 ]\n [13.296638 ]\n [13.690188 ]\n [15.090475 ]\n [14.675297 ]\n [15.596957 ]\n [16.930855 ]\n [14.325371 ]\n [13.366553 ]\n [14.265587 ]\n [14.393348 ]\n [11.729925 ]\n [14.469565 ]\n [11.322239 ]\n [14.72429  ]\n [13.232167 ]\n [12.562703 ]\n [14.5658455]\n [18.011469 ]\n [16.194958 ]\n [15.776176 ]\n [12.129652 ]\n [13.212789 ]\n [13.694575 ]\n [14.378883 ]\n [13.666672 ]\n [11.646882 ]\n [17.678396 ]\n [15.743342 ]\n [11.517617 ]\n [10.13517  ]\n [11.879427 ]\n [14.59514  ]\n [15.828282 ]\n [18.099014 ]\n [12.840478 ]\n [14.979764 ]\n [11.82459  ]\n [18.776907 ]\n [16.846033 ]\n [14.249531 ]\n [13.79395  ]\n [ 7.0305347]\n [15.456069 ]\n [12.649491 ]\n [17.099874 ]\n [16.081148 ]\n [15.095818 ]\n [15.524711 ]\n [15.255722 ]\n [ 9.886485 ]\n [12.014374 ]\n [14.002818 ]\n [12.425849 ]\n [10.174555 ]\n [17.489893 ]\n [13.141122 ]\n [15.669976 ]\n [14.345945 ]\n [16.734468 ]\n [12.0332   ]\n [16.714622 ]\n [12.925306 ]\n [21.972656 ]\n [12.471611 ]\n [15.495508 ]\n [16.207813 ]\n [16.804382 ]\n [15.268195 ]\n [16.43684  ]\n [10.046993 ]\n [13.407481 ]\n [16.752483 ]\n [16.264399 ]\n [15.535851 ]\n [11.295437 ]\n [15.142626 ]\n [15.606548 ]\n [15.890112 ]\n [10.936295 ]\n [17.962484 ]\n [16.936502 ]\n [16.216486 ]\n [14.784401 ]\n [13.124825 ]\n [15.473015 ]\n [11.257675 ]\n [15.644724 ]\n [12.816245 ]\n [17.001774 ]\n [13.660676 ]\n [14.287798 ]\n [14.132577 ]\n [10.475491 ]\n [13.238355 ]\n [13.971307 ]\n [13.97105  ]\n [13.709632 ]\n [19.081526 ]\n [15.137314 ]\n [17.73207  ]\n [12.083811 ]\n [18.558622 ]\n [14.848495 ]\n [15.325954 ]\n [13.738331 ]\n [15.087895 ]\n [11.804383 ]\n [14.62039  ]\n [12.9873295]\n [16.043175 ]\n [15.014361 ]\n [13.591056 ]\n [11.710275 ]\n [10.439169 ]\n [12.816245 ]\n [18.142984 ]\n [14.443832 ]\n [13.693745 ]\n [13.758059 ]\n [14.852305 ]\n [13.808538 ]\n [13.534388 ]\n [14.53707  ]\n [15.955869 ]\n [14.503783 ]\n [15.394336 ]\n [14.4042845]\n [15.274074 ]\n [14.950915 ]\n [13.416966 ]\n [12.43908  ]\n [14.529984 ]\n [14.036628 ]\n [12.752329 ]\n [13.856999 ]\n [13.45418  ]\n [15.1674795]\n [11.850748 ]]", "q_t_selected": "[12.943509  21.35447   13.143145  13.312378  11.354406   7.0305347\n 16.23798   16.114594  13.126923  12.342951  14.926931  11.229014\n 12.855781  16.275633  12.985301  12.114595  16.135242  13.13722\n 14.431715  13.191846  13.351438  13.521589  14.990288  12.941327\n 13.805411  15.156637  13.440551  13.402143  14.378883  14.763368\n 16.116121  16.991077  16.21866   14.710827  14.404875  13.867271\n 14.4866495 11.628213  16.032213  15.71551   17.40998   12.3466835\n 16.131952  18.528908  14.975551  14.127801  13.96181   12.453702\n 14.286154  17.458847  13.658911  13.382701  10.547787  16.492014\n 13.244623  14.369008  14.94877   15.488331  14.964789  13.289874\n 16.245417  11.331593  12.143556  12.784482  17.080135   8.941975\n 13.323104  14.994817  13.959568  12.451076  13.03918   16.420784\n 16.664585  13.649098  14.103758  13.908088  16.752483  13.8843975\n 12.086504  11.256163  16.615007  13.384368  11.867251   9.832679\n 14.958484  11.88023   14.134     15.898349  14.868248  13.485869\n 18.023441  13.262826  12.608695  21.001923  16.89358   17.712856\n 13.1742525 14.071323  14.240146  13.603451  16.303995  14.507604\n 13.556123  14.657844  15.267664  13.949718  15.724931  15.7247\n  9.197373  13.162499  15.789796  13.533357  14.037597  13.225967\n 14.566724  14.630524  16.202738  16.439905  13.296638  13.690188\n 15.090475  14.675297  15.596957  16.930855  14.325371  13.366553\n 14.265587  14.393348  11.729925  14.469565  11.322239  14.72429\n 13.232167  12.562703  14.5658455 18.011469  16.194958  15.776176\n 12.129652  13.212789  13.694575  14.378883  13.666672  11.646882\n 17.678396  15.743342  11.517617  10.13517   11.879427  14.59514\n 15.828282  18.099014  12.840478  14.979764  11.82459   18.776907\n 16.846033  14.249531  13.79395    7.0305347 15.456069  12.649491\n 17.099874  16.081148  15.095818  15.524711  15.255722   9.886485\n 12.014374  14.002818  12.425849  10.174555  17.489893  13.141122\n 15.669976  14.345945  16.734468  12.0332    16.714622  12.925306\n 21.972656  12.471611  15.495508  16.207813  16.804382  15.268195\n 16.43684   10.046993  13.407481  16.752483  16.264399  15.535851\n 11.295437  15.142626  15.606548  15.890112  10.936295  17.962484\n 16.936502  16.216486  14.784401  13.124825  15.473015  11.257675\n 15.644724  12.816245  17.001774  13.660676  14.287798  14.132577\n 10.475491  13.238355  13.971307  13.97105   13.709632  19.081526\n 15.137314  17.73207   12.083811  18.558622  14.848495  15.325954\n 13.738331  15.087895  11.804383  14.62039   12.9873295 16.043175\n 15.014361  13.591056  11.710275  10.439169  12.816245  18.142984\n 14.443832  13.693745  13.758059  14.852305  13.808538  13.534388\n 14.53707   15.955869  14.503783  15.394336  14.4042845 15.274074\n 14.950915  13.416966  12.43908   14.529984  14.036628  12.752329\n 13.856999  13.45418   15.1674795 11.850748 ]", "twin_q_t_selected": "[12.725834  20.298542  13.539444  13.9348955 11.278326   6.981083\n 15.672204  15.663366  13.67039   12.708514  15.172618  11.447633\n 12.454133  15.631975  14.360071  12.082643  16.354559  12.81131\n 14.588796  13.03513   13.521863  13.972486  14.660596  13.392326\n 14.108002  15.469854  13.768538  12.778205  14.602407  14.517006\n 16.307     16.539509  16.223738  14.238262  15.838917  13.74929\n 14.748022  12.290257  15.861054  16.09664   18.047739  12.692747\n 16.475252  18.430965  14.881239  14.420431  14.223472  13.071499\n 14.9313    17.89766   14.021949  12.755857  10.573987  16.429924\n 13.757232  13.619602  14.625368  15.279091  14.221158  12.10132\n 16.334103  10.245731  11.593022  13.053072  16.728184   9.115455\n 14.357432  14.523029  14.433086  12.268265  13.215118  16.478354\n 16.491066  13.847664  15.022784  14.154957  16.44753   13.639854\n 11.97756   11.287559  16.507685  14.315564  11.785121   9.286881\n 15.046344  11.033242  14.504743  16.08954   14.753299  14.021735\n 17.196064  13.430707  11.992918  20.97031   16.431103  16.610277\n 13.9858055 14.180823  14.866102  13.384414  15.942321  14.432057\n 12.716487  14.822451  15.402225  13.719681  16.16311   15.710668\n  9.25811   13.237905  15.201617  13.538391  14.695143  12.891575\n 14.507376  14.744002  16.539413  16.200981  13.238245  12.734861\n 14.849806  14.425296  15.411459  17.167597  14.724017  12.494034\n 14.371984  14.067004  12.105405  13.549295  11.281882  14.620501\n 14.387378  12.179596  14.784101  16.797653  16.885557  15.345075\n 11.77101   12.892242  14.084918  14.602407  13.2111635 11.755328\n 17.191532  15.938409  11.222379  10.165088  13.247791  14.826914\n 15.55722   18.77081   12.537636  14.567879  11.848398  18.93452\n 16.220066  15.299811  14.155824   6.981083  15.878913  12.575268\n 16.419847  16.295784  14.564525  16.504684  15.381874  10.400537\n 12.472345  13.565201  11.168744   9.968264  17.587423  12.783947\n 15.434688  14.4156275 16.35342   13.133473  16.427048  13.343414\n 22.07252   12.766199  15.396254  16.26376   16.937708  14.945293\n 16.820745  10.483565  13.30619   16.44753   17.203241  16.115118\n 12.494732  15.383806  16.271152  15.304919  10.42528   17.130798\n 17.314823  16.180151  14.772232  14.4528    14.95023   11.962988\n 15.193574  12.933317  16.823795  13.578404  14.595268  14.213606\n  9.826747  13.221836  15.133647  14.442262  13.65576   19.356897\n 14.268456  17.086796  12.950169  19.558182  15.188969  15.445007\n 12.526863  14.900681  12.088761  14.063296  13.571067  16.753807\n 14.856841  14.529288  11.221351  10.641809  12.933317  17.867764\n 14.726749  13.473513  14.093866  14.64763   13.609241  13.521295\n 14.738088  15.389661  14.5400095 15.74593   15.466302  15.228295\n 14.437369  13.3438225 12.283887  14.303892  14.0475235 13.051226\n 13.497546  13.590947  14.739936  11.546678 ]", "q_t_selected_target": "[11.538535  20.478668  14.227278  14.111664  12.643789   7.370948\n 16.629723  14.866263  11.530174  10.445096  16.479136  11.706691\n 13.005416  16.960999  13.625891  11.89422   16.968344  12.636988\n 14.00041   14.472938  13.259703  13.401182  16.144716  13.597516\n 12.936462  15.444651  13.816415  13.371227  13.956239  15.81913\n 15.850462  15.726344  15.178093  13.740629  16.501255  14.646202\n 14.683262  10.853609  17.825363  17.731571  17.201916  11.62649\n 15.765133  18.397482  15.435543  14.695906  12.385487  12.71762\n 14.343726  19.655624  14.295889  14.210138  11.755546  14.845433\n 12.79171   13.657493  14.7158785 16.099312  13.8214245 12.639254\n 15.588959  10.230605  11.194986  12.926544  16.608023   7.8595366\n 13.319864  13.77552   13.13574   10.216251  14.320294  15.059005\n 17.550636  12.05667   14.1728115 13.61661   16.650309  13.781471\n 11.534057  10.756813  17.493326  14.032104  11.010396   9.108468\n 14.52165   11.459555  14.620738  15.723017  13.802853  14.215056\n 19.353611  14.279808  11.86072   19.970102  17.105185  16.83854\n 14.7135    13.968957  15.488614  14.036302  16.648558  15.433356\n 14.2283125 14.389984  15.10645   14.140276  14.8631525 15.559799\n  8.254114  14.118552  17.125881  14.08175   13.807748  12.686949\n 13.416023  15.507002  15.388491  14.171538  12.209695  11.847589\n 15.58217   13.383381  17.136402  15.49831   14.741929  12.24608\n 15.6258135 14.629156  12.66942   15.412417  11.841456  15.104963\n 14.072832  13.356146  13.67813   15.0520115 16.738306  14.380493\n 10.772009  13.794597  14.256544  13.438833  13.686602  10.856193\n 17.629124  15.8928795 12.34472    9.158312  13.50233   15.448832\n 16.247908  17.08274   11.286421  13.518203  13.973732  17.214901\n 17.06562   14.428172  13.32615    8.014788  15.938558  13.159478\n 16.4126    17.770252  12.962456  14.867035  15.1778965  9.876559\n 13.449951  13.883659  12.157549  11.016883  16.651472  11.689416\n 16.64983   13.727047  18.0114    14.3160515 17.034384  13.334032\n 22.07113   12.264709  16.059114  15.173257  15.858013  13.89938\n 16.213196   9.4997425 14.532482  16.521626  17.853737  16.13823\n 12.815971  14.671965  15.651092  16.092287  10.221564  17.071218\n 18.407606  14.47119   15.078133  14.123528  16.077333  11.308173\n 16.516197  11.856929  16.564972  14.105019  13.9932575 13.400777\n 10.009958  13.393719  14.072764  13.994662  14.386477  19.18044\n 14.250809  17.300955  13.426775  19.57182   14.740048  16.369772\n 11.840319  15.796142  12.893077  15.8073635 14.435716  16.740948\n 14.059766  13.641753  11.638336  11.723449  12.072427  17.011902\n 13.74514   14.241266  14.500732  13.468981  12.525294  13.737685\n 14.530645  15.919614  14.72886   15.284248  14.156884  15.8710575\n 15.825299  13.91409   13.9625225 16.167751  14.309367  12.218934\n 15.72542   12.327475  13.513113  13.14625  ]", "q_tp1_best_masked": "[12.157334  19.477058  14.682122  15.319504  13.328863   9.575195\n 16.962252  15.256194  12.373158  11.583949  16.267685  12.601581\n 13.446953  16.8576    13.555063  12.692581  16.961348  13.024156\n 15.025334  14.652392  13.680778  14.1077175 16.210838  13.778149\n 14.073034  14.674912  14.539351  13.7014675 14.101059  16.081963\n 16.516819  15.784018  15.083515  13.711462  16.455763  15.205291\n 15.962332  12.878122  18.329967  17.641329  17.142508  12.166568\n 15.382205  18.95051   15.1471405 15.764287  13.956569  13.401184\n 14.680659  18.292515  14.876358  15.41238   13.174121  14.603987\n 13.638165  13.3911085 16.129128  16.341917  14.284866  12.880095\n 15.726568  11.459403  12.137004  14.008229  16.28217    9.360544\n 13.833438  15.554272  13.406253  11.247066  15.02711   15.312247\n 17.140993  13.258859  14.427539  13.718201  17.365265  14.493144\n 12.606189  11.771463  16.949013  14.393353  11.981467  10.913273\n 14.352678  12.832939  15.147605  15.195103  14.499283  14.533336\n 19.157927  14.883469  12.167413  19.392725  16.09753   16.528065\n 15.381897  13.93295   15.575802  14.795059  16.87117   15.709589\n 15.010561  14.963802  14.546923  14.788778  14.668473  16.21403\n  9.866205  14.839919  17.959064  14.37681   13.852737  12.805278\n 14.449803  16.21736   14.621498  14.60424   12.57365   12.719932\n 14.668632  13.880045  16.85341   15.470979  14.681678  14.176053\n 16.626425  15.641228  13.730368  16.322542  13.608788  15.620119\n 14.952826  13.998185  13.95161   14.861812  17.244715  15.171134\n 12.099307  15.186372  15.034941  13.578427  13.885102  12.226308\n 17.763573  15.9469595 14.037935  11.135409  13.930522  15.202094\n 16.585217  16.889944  11.394696  14.536367  15.141528  16.429848\n 16.714045  15.3635025 13.385316  10.225538  15.445432  14.374589\n 16.08021   17.303051  14.294435  14.925307  15.042027  10.918112\n 14.365007  13.937587  13.782088  11.833537  15.649399  12.619393\n 17.695263  13.969224  18.133589  14.558492  16.438412  13.340906\n 20.22329   14.087048  15.926412  15.398781  15.42951   14.533074\n 15.279369  10.799857  16.331081  17.235281  18.03825   16.374592\n 12.917429  14.679538  17.265875  16.590832  11.171503  17.482239\n 18.438961  15.393509  14.877704  14.178827  16.515245  11.38836\n 16.973936  12.563978  16.185171  15.027389  14.349808  14.193636\n 10.876942  14.3715315 14.623626  14.760439  14.558828  18.699413\n 14.333176  17.111528  13.689802  18.312637  14.604366  15.22103\n 11.967585  15.5229225 13.13619   15.93276   14.344726  17.845272\n 15.432518  14.174583  13.08875   12.785292  12.781652  16.058146\n 13.923166  14.895594  15.595734  15.011877  13.258747  13.519978\n 14.70657   15.568647  15.461021  13.800832  15.402668  15.704269\n 15.730998  13.551376  15.15491   15.94785   14.40628   13.0561905\n 15.309122  13.68927   13.783912  13.789646 ]", "policy_t": "[[-0.5874824  -0.26108575  0.8785846   0.10730088  0.16636825  0.58445895]\n [ 0.4871118  -0.41900307 -0.22931194  0.19819868 -0.37890047 -0.7322755 ]\n [-0.7499405  -0.5439224   0.6794708   0.7308179   0.03104806  0.87900746]\n ...\n [-0.614877   -0.12254792 -0.46911287  0.32567668  0.07751989 -0.6000081 ]\n [ 0.7104473  -0.68727547 -0.43423045  0.25053203 -0.5651314  -0.29425275]\n [ 0.96824527  0.6480756  -0.91078144  0.4620309   0.8918803  -0.562757  ]]", "td_error": "[1.2961364  0.52796364 0.88598347 0.4880271  1.3274231  0.365139\n 0.67463064 1.0227165  1.8684821  2.0806365  1.4293609  0.3683672\n 0.3504591  1.0071945  0.6873851  0.20439863 0.72344303 0.3372774\n 0.50984526 1.3594499  0.1769476  0.34585524 1.3192744  0.43068933\n 1.0202441  0.15660858 0.2118702  0.3119688  0.53440666 1.1789432\n 0.3610983  1.038949   1.0431061  0.7339153  1.3793592  0.83792114\n 0.13068628 1.1056261  1.8787293  1.8254962  0.52694416 0.89322567\n 0.5384693  0.08245468 0.50714827 0.42178965 1.7071543  0.30889845\n 0.32257318 1.9773712  0.45545912 1.1408591  1.1946588  1.6155357\n 0.70921755 0.37470293 0.16170073 0.71560097 0.7715492  0.5942769\n 0.7008009  0.55805683 0.67330265 0.13429499 0.29613686 1.169178\n 0.52040386 0.9834027  1.0605869  2.1434188  1.1931453  1.390564\n 0.97281075 1.691711   0.4595132  0.4149127  0.15247631 0.12227154\n 0.49797535 0.51504755 0.93198013 0.4655981  0.8157902  0.45131207\n 0.48076344 0.42349386 0.3013668  0.2709279  1.0079207  0.46125412\n 1.7438583  0.9330416  0.44008684 1.0160141  0.44284344 0.55128956\n 1.133471   0.15711641 0.93549013 0.54236937 0.5253997  0.9635258\n 1.0920076  0.35016298 0.22849417 0.30557632 1.0808682  0.1578846\n 0.97362757 0.9183502  1.6301746  0.545876   0.5586219  0.37182236\n 1.1210265  0.81973886 0.98258495 2.1489048  1.0577464  1.3649364\n 0.6120291  1.1669154  1.632194   1.5509157  0.21723509 0.68421316\n 1.3070283  0.39898014 0.75175524 1.402987   0.5393958  0.43256807\n 0.57760525 0.9849963  0.99684286 2.3525496  0.34529972 1.1801324\n 1.1783223  0.74208117 0.36679745 1.0518122  0.247684   0.8449125\n 0.24343204 0.09753323 0.9747219  0.991817   0.9387207  0.7378044\n 0.55515623 1.352171   1.402636   1.2556186  2.137238   1.6408129\n 0.53257084 0.5251403  0.64873695 1.0089788  0.27106667 0.54709864\n 0.34726143 1.5817862  1.8677154  1.1476626  0.14090157 0.26695156\n 1.2065916  0.21880865 0.62855244 0.9454737  0.88718605 1.2731185\n 1.0974979  0.65373945 1.4674559  1.7327147  0.46354866 0.209054\n 0.04993153 0.3541956  0.61323357 1.0625296  1.013032   1.2073646\n 0.41559696 0.7655368  1.1756468  0.15247631 1.1199169  0.3127451\n 0.920887   0.5912514  0.3323021  0.49477148 0.4592228  0.47542286\n 1.2819433  1.727128   0.29981613 0.66398764 0.8657112  0.35265636\n 1.0970483  1.0178523  0.34781265 0.4854784  0.44827557 0.77231455\n 0.3243718  0.16362333 0.5811701  0.23560572 0.7037816  0.13768578\n 0.45207644 0.32263756 0.9097853  0.51341724 0.2786832  0.9842911\n 1.2922783  0.80185366 0.9465046  1.4655204  1.1565175  0.35531616\n 0.8758354  0.4691162  0.24446201 1.1829596  0.80235434 0.9934721\n 0.84015083 0.6576376  0.57477    1.2809868  1.1835952  0.20984411\n 0.10693359 0.28310394 0.20696354 0.28588438 0.778409   0.61987305\n 1.1311569  0.5336957  1.6010389  1.750813   0.26729155 0.6828432\n 2.0481472  1.1950889  1.4405947  1.447537  ]", "mean_td_error": 0.7994813919067383, "actor_loss": -15.533794403076172, "critic_loss": 0.4663424491882324, "alpha_loss": -11.831668853759766, "alpha_value": 0.30007466673851013, "target_entropy": -6, "mean_q": 14.313754081726074, "max_q": 21.97265625, "min_q": 7.030534744262695, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 14000, "episodes_total": 14, "training_iteration": 14, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_14-47-51", "timestamp": 1587048471, "time_this_iter_s": 86.1155514717102, "time_total_s": 373.96567821502686, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 373.96567821502686, "timesteps_since_restore": 14000, "iterations_since_restore": 14, "perf": {"cpu_util_percent": 92.24137931034483, "ram_util_percent": 11.299999999999999}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -92.78918199586307, "episode_reward_min": -376.9012552628892, "episode_reward_mean": -233.17167161416623, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-236.00786208961455, -222.45098351702745, -207.52004343509552, -267.9109252725903, -291.19934557149713, -248.99304526412683, -159.09220466730252, -376.9012552628892, -228.85186906565562, -92.78918199586307], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2602836664935121, "mean_processing_ms": 0.1199817227639135, "mean_inference_ms": 1.1731776004799737}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -94.96187381235399, "episode_reward_min": -306.56299656689714, "episode_reward_mean": -217.35093683341566, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-286.016112222877, -306.56299656689714, -262.5981647835083, -94.96187381235399, -136.6155367814418], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.305871618247357, "mean_processing_ms": 0.4300547845595591, "mean_inference_ms": 1.41034210065528}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 7500, "num_steps_trained": 1278720, "num_steps_sampled": 15000, "sample_time_ms": 2.982, "replay_time_ms": 23.423, "grad_time_ms": 54.833, "update_time_ms": 0.005, "opt_peak_throughput": 4668.755, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[19.224943 ]\n [15.352029 ]\n [10.969076 ]\n [18.086832 ]\n [16.588903 ]\n [15.494471 ]\n [17.73032  ]\n [14.570581 ]\n [17.563395 ]\n [14.0169935]\n [15.200042 ]\n [ 9.016294 ]\n [15.149313 ]\n [13.692454 ]\n [18.39928  ]\n [16.540857 ]\n [11.6627   ]\n [12.441807 ]\n [16.79384  ]\n [17.48535  ]\n [14.393315 ]\n [15.279173 ]\n [16.274363 ]\n [19.106636 ]\n [13.00763  ]\n [16.823015 ]\n [15.364713 ]\n [14.269105 ]\n [20.622705 ]\n [16.073215 ]\n [15.988788 ]\n [ 8.903126 ]\n [17.01799  ]\n [15.697548 ]\n [17.81529  ]\n [18.99756  ]\n [18.232182 ]\n [11.000349 ]\n [15.924095 ]\n [15.234996 ]\n [14.445297 ]\n [15.718418 ]\n [13.0138445]\n [13.252261 ]\n [16.259314 ]\n [16.321373 ]\n [14.643411 ]\n [14.762127 ]\n [12.660982 ]\n [19.111652 ]\n [16.497759 ]\n [ 9.850971 ]\n [13.286201 ]\n [15.487716 ]\n [14.282619 ]\n [18.629902 ]\n [13.903224 ]\n [17.740572 ]\n [19.188309 ]\n [17.852516 ]\n [15.584716 ]\n [12.25336  ]\n [20.054426 ]\n [12.116014 ]\n [15.109626 ]\n [14.799555 ]\n [12.955405 ]\n [17.221703 ]\n [11.669217 ]\n [13.812672 ]\n [15.991492 ]\n [16.527336 ]\n [12.79577  ]\n [12.589162 ]\n [15.404121 ]\n [12.974539 ]\n [14.155083 ]\n [13.35274  ]\n [14.16696  ]\n [12.155273 ]\n [12.9334345]\n [14.54377  ]\n [18.25862  ]\n [17.13805  ]\n [17.08796  ]\n [ 8.919154 ]\n [19.131208 ]\n [10.014904 ]\n [16.425999 ]\n [16.407074 ]\n [16.773987 ]\n [14.845202 ]\n [13.971474 ]\n [15.491349 ]\n [15.264426 ]\n [13.696588 ]\n [16.041224 ]\n [13.492258 ]\n [17.016317 ]\n [20.428694 ]\n [14.423816 ]\n [12.611416 ]\n [15.031518 ]\n [16.255524 ]\n [15.853943 ]\n [14.752184 ]\n [15.300204 ]\n [12.835008 ]\n [21.002827 ]\n [17.770296 ]\n [22.46361  ]\n [13.975208 ]\n [16.521011 ]\n [12.1827545]\n [13.385506 ]\n [13.59381  ]\n [16.340698 ]\n [16.456453 ]\n [15.440261 ]\n [12.821093 ]\n [11.22186  ]\n [16.87679  ]\n [15.515623 ]\n [15.769094 ]\n [13.634699 ]\n [12.453086 ]\n [19.184135 ]\n [14.347162 ]\n [14.42913  ]\n [24.01688  ]\n [15.8930855]\n [13.579137 ]\n [16.572222 ]\n [22.665886 ]\n [15.738183 ]\n [13.843484 ]\n [17.052204 ]\n [14.0487585]\n [14.400743 ]\n [16.51607  ]\n [15.4388485]\n [13.631467 ]\n [14.006446 ]\n [15.630903 ]\n [15.329845 ]\n [16.515062 ]\n [14.16466  ]\n [14.288051 ]\n [14.212088 ]\n [14.698262 ]\n [15.755673 ]\n [14.506017 ]\n [14.414892 ]\n [15.857956 ]\n [14.9537945]\n [17.049408 ]\n [13.045154 ]\n [17.466787 ]\n [13.800521 ]\n [15.129015 ]\n [15.090746 ]\n [21.780485 ]\n [17.085243 ]\n [23.326015 ]\n [12.976786 ]\n [14.756094 ]\n [14.494125 ]\n [10.503406 ]\n [16.30685  ]\n [16.147396 ]\n [18.284788 ]\n [26.252693 ]\n [18.417763 ]\n [14.327479 ]\n [16.839773 ]\n [19.577473 ]\n [14.902976 ]\n [12.207971 ]\n [15.011021 ]\n [ 9.848884 ]\n [12.571013 ]\n [17.764814 ]\n [12.363463 ]\n [16.933084 ]\n [16.838655 ]\n [12.977013 ]\n [11.972831 ]\n [16.11094  ]\n [14.216006 ]\n [17.685686 ]\n [21.643448 ]\n [13.509418 ]\n [14.297809 ]\n [15.293125 ]\n [14.371295 ]\n [14.774619 ]\n [14.749418 ]\n [13.70667  ]\n [15.006309 ]\n [20.76439  ]\n [17.555801 ]\n [14.881367 ]\n [20.524315 ]\n [12.536312 ]\n [13.535467 ]\n [12.466771 ]\n [20.458784 ]\n [14.347615 ]\n [12.617904 ]\n [18.100273 ]\n [15.439631 ]\n [13.7396345]\n [16.034367 ]\n [15.00094  ]\n [ 9.933135 ]\n [12.058371 ]\n [13.670345 ]\n [12.897896 ]\n [14.619762 ]\n [14.826274 ]\n [15.331633 ]\n [16.452898 ]\n [14.937357 ]\n [18.445522 ]\n [19.188309 ]\n [15.058166 ]\n [12.092803 ]\n [16.259668 ]\n [15.372318 ]\n [17.470686 ]\n [14.612106 ]\n [14.65253  ]\n [12.836277 ]\n [15.77505  ]\n [14.523037 ]\n [14.176027 ]\n [15.610301 ]\n [16.407074 ]\n [14.400638 ]\n [14.077836 ]\n [14.669087 ]\n [15.466586 ]\n [12.821673 ]\n [15.174904 ]\n [ 8.965671 ]\n [15.777982 ]\n [18.27621  ]\n [12.09378  ]\n [13.753151 ]\n [16.687748 ]\n [ 9.187586 ]\n [18.79255  ]\n [15.654339 ]\n [15.525447 ]\n [13.25789  ]\n [14.723796 ]]", "q_t_selected": "[19.224943  15.352029  10.969076  18.086832  16.588903  15.494471\n 17.73032   14.570581  17.563395  14.0169935 15.200042   9.016294\n 15.149313  13.692454  18.39928   16.540857  11.6627    12.441807\n 16.79384   17.48535   14.393315  15.279173  16.274363  19.106636\n 13.00763   16.823015  15.364713  14.269105  20.622705  16.073215\n 15.988788   8.903126  17.01799   15.697548  17.81529   18.99756\n 18.232182  11.000349  15.924095  15.234996  14.445297  15.718418\n 13.0138445 13.252261  16.259314  16.321373  14.643411  14.762127\n 12.660982  19.111652  16.497759   9.850971  13.286201  15.487716\n 14.282619  18.629902  13.903224  17.740572  19.188309  17.852516\n 15.584716  12.25336   20.054426  12.116014  15.109626  14.799555\n 12.955405  17.221703  11.669217  13.812672  15.991492  16.527336\n 12.79577   12.589162  15.404121  12.974539  14.155083  13.35274\n 14.16696   12.155273  12.9334345 14.54377   18.25862   17.13805\n 17.08796    8.919154  19.131208  10.014904  16.425999  16.407074\n 16.773987  14.845202  13.971474  15.491349  15.264426  13.696588\n 16.041224  13.492258  17.016317  20.428694  14.423816  12.611416\n 15.031518  16.255524  15.853943  14.752184  15.300204  12.835008\n 21.002827  17.770296  22.46361   13.975208  16.521011  12.1827545\n 13.385506  13.59381   16.340698  16.456453  15.440261  12.821093\n 11.22186   16.87679   15.515623  15.769094  13.634699  12.453086\n 19.184135  14.347162  14.42913   24.01688   15.8930855 13.579137\n 16.572222  22.665886  15.738183  13.843484  17.052204  14.0487585\n 14.400743  16.51607   15.4388485 13.631467  14.006446  15.630903\n 15.329845  16.515062  14.16466   14.288051  14.212088  14.698262\n 15.755673  14.506017  14.414892  15.857956  14.9537945 17.049408\n 13.045154  17.466787  13.800521  15.129015  15.090746  21.780485\n 17.085243  23.326015  12.976786  14.756094  14.494125  10.503406\n 16.30685   16.147396  18.284788  26.252693  18.417763  14.327479\n 16.839773  19.577473  14.902976  12.207971  15.011021   9.848884\n 12.571013  17.764814  12.363463  16.933084  16.838655  12.977013\n 11.972831  16.11094   14.216006  17.685686  21.643448  13.509418\n 14.297809  15.293125  14.371295  14.774619  14.749418  13.70667\n 15.006309  20.76439   17.555801  14.881367  20.524315  12.536312\n 13.535467  12.466771  20.458784  14.347615  12.617904  18.100273\n 15.439631  13.7396345 16.034367  15.00094    9.933135  12.058371\n 13.670345  12.897896  14.619762  14.826274  15.331633  16.452898\n 14.937357  18.445522  19.188309  15.058166  12.092803  16.259668\n 15.372318  17.470686  14.612106  14.65253   12.836277  15.77505\n 14.523037  14.176027  15.610301  16.407074  14.400638  14.077836\n 14.669087  15.466586  12.821673  15.174904   8.965671  15.777982\n 18.27621   12.09378   13.753151  16.687748   9.187586  18.79255\n 15.654339  15.525447  13.25789   14.723796 ]", "twin_q_t_selected": "[18.398169  14.977751  10.748311  17.737875  17.018646  15.464881\n 18.496023  14.595728  16.819881  14.836688  15.329613   9.598591\n 15.789315  13.782836  18.410463  16.360003  12.710843  12.35721\n 16.231066  17.904215  13.852144  14.042608  17.126719  19.003016\n 12.999029  17.584026  15.642836  14.322016  20.93021   15.581925\n 15.199566   9.740028  17.305372  16.00016   17.624329  18.34991\n 18.137264  10.655063  15.634338  14.401187  14.681849  16.062029\n 12.169158  13.214334  15.534136  14.894638  14.252533  14.961835\n 12.026479  19.118711  15.9440365  9.338366  13.598646  16.641514\n 14.214366  18.506987  12.968958  18.228079  18.471806  17.207918\n 15.450542  12.392133  19.325762  13.505649  15.046857  14.402394\n 13.047001  17.127707  11.884223  13.271014  14.999541  16.216476\n 13.2133875 12.492385  15.306939  13.954726  14.439516  13.45527\n 14.3427305 11.40432   13.636609  14.797974  18.290333  16.295399\n 17.073643   9.889648  18.46969   10.21151   15.913669  17.376299\n 16.903664  14.487549  14.732191  16.10094   14.197661  12.902817\n 15.525275  14.403415  17.166286  20.280487  15.024462  12.801854\n 14.36019   16.472315  16.312393  15.143138  14.944857  13.020985\n 20.62178   16.527378  22.676214  13.505218  15.929793  13.082923\n 12.859228  14.351293  16.285093  16.689816  16.484728  11.815909\n 11.594292  16.483337  15.840837  14.871629  13.672028  13.185818\n 19.584831  13.722298  14.755496  23.819412  16.91361   13.815758\n 16.125645  23.040134  14.7567625 14.170363  16.574455  13.977546\n 14.472091  16.049696  15.395733  12.976267  14.848145  14.961581\n 16.387848  16.4391    13.705048  14.610548  14.319504  15.534257\n 15.819014  14.933351  14.59844   15.378473  14.110435  15.763435\n 13.279394  17.608595  13.866846  15.384066  14.792642  22.474487\n 16.438118  23.709599  13.099547  14.418644  15.103165  10.725251\n 15.887966  15.636663  18.512606  26.368517  17.31857   14.512034\n 16.646544  20.40873   15.053872  12.118485  14.927481  10.1386795\n 12.656942  17.387554  11.183967  16.92906   17.343609  12.988378\n 12.229304  16.24649   14.418238  17.47654   21.626226  13.489775\n 14.142962  15.754964  13.931098  14.335374  15.821482  14.12316\n 14.809458  22.36023   16.908772  15.164704  19.710634  12.4993305\n 14.250925  13.063407  19.681852  14.6634245 13.030187  18.049093\n 14.766742  14.39272   17.146936  14.665764   9.799573  12.660827\n 14.678121  12.886771  15.1729965 15.525853  14.403982  16.640354\n 15.102381  18.345264  18.471806  15.208771  13.060267  15.847127\n 14.618958  16.82041   14.327162  14.908098  12.649478  15.636771\n 14.713582  14.497551  15.596773  17.376299  14.38478   13.79888\n 16.0329    15.621349  12.722225  15.767331   8.649213  15.445726\n 18.848469  11.574623  13.584214  16.198654   9.991998  18.499105\n 15.799178  14.702626  13.297085  14.976855 ]", "q_t_selected_target": "[18.35664   15.818516  10.333669  18.331148  15.023819  14.2142315\n 16.456436  14.999489  18.033405  15.22785   16.714125   9.088079\n 14.863446  14.365245  16.713837  16.223812  12.818534  11.460797\n 17.802269  17.078949  13.403353  14.869788  17.534115  19.008053\n 11.317652  17.65982   15.000984  13.675258  21.945152  16.560768\n 15.857958   8.1026125 19.416174  15.552823  15.15867   18.71623\n 19.329182  10.70131   16.283003  15.193086  15.053252  15.813217\n 14.13549   13.810307  17.459854  16.15267   15.592429  14.848524\n 13.135039  22.63891   16.528008   8.503384  13.7133875 15.828878\n 14.560686  18.77948   12.785339  17.473469  18.16616   18.210041\n 15.917038  12.167267  18.252089  13.271084  14.993995  15.752497\n 12.942347  15.782619  12.152008  15.665244  15.138107  17.449831\n 12.350997  13.815564  15.116929  13.036601  14.148716  12.376251\n 13.873741  11.88211   14.019861  14.942661  19.330517  15.651334\n 18.64885    9.575652  18.29901    9.722244  15.894609  15.473634\n 16.209465  14.3718605 15.727168  14.339897  14.31477   14.501697\n 15.866381  15.269864  15.43041   21.785452  16.450829  14.595345\n 14.523278  14.554869  18.174553  13.840906  16.088331  14.279486\n 20.15782   18.070957  24.51107   13.360939  15.863132  13.540663\n 14.497609  15.689869  14.448342  15.60823   15.119481  12.513023\n 11.43207   16.16208   16.515991  16.520002  13.0222025 12.60191\n 18.50708   14.731554  13.603422  25.358477  15.790352  13.878594\n 16.03962   21.808561  16.219728  13.134803  18.279202  16.058273\n 13.941504  17.610798  14.350423  11.769037  15.563044  16.62671\n 16.504833  17.269053  13.189807  13.893803  14.766393  16.326485\n 15.213607  15.952741  15.843424  17.660156  14.148061  16.433903\n 12.511956  18.624928  13.250967  16.3406    14.993973  21.460756\n 16.12455   23.945606  13.175504  13.315044  15.378733   9.252437\n 17.084509  17.081436  19.647789  27.552748  20.134483  14.443991\n 16.368565  21.009104  16.105852  13.445878  14.575512  10.109681\n 13.471766  18.179667  12.887808  15.688566  17.807428  11.669901\n 13.658638  16.445168  13.807546  19.658005  21.774994  15.406664\n 14.075748  13.724444  15.547213  15.202345  16.098972  12.9301405\n 14.256734  21.934145  17.861956  16.125664  22.231524  12.748448\n 14.3347435 12.180193  18.693869  15.320178  12.737425  17.58854\n 14.64631   12.651831  15.859756  14.8128805  9.770598  13.973943\n 13.722938  12.36438   15.840395  14.361446  14.295965  15.765184\n 14.767332  19.531326  18.022242  15.195843  13.036466  15.027137\n 16.07968   15.992241  15.261697  15.141325  12.519911  14.469606\n 14.1851425 15.313189  15.101663  15.712743  13.712503  15.822573\n 14.496723  15.450218  11.951454  15.097302   8.196678  16.538649\n 17.793098  10.49666   14.742607  18.220331   8.662431  18.639576\n 16.329437  16.378084  14.170945  16.119165 ]", "q_tp1_best_masked": "[17.331877  16.556194  12.053381  18.47355   15.183433  14.394313\n 16.115793  15.849673  18.070864  15.525472  17.13109   10.101793\n 14.875258  15.181969  16.438421  16.463192  13.509621  12.5174465\n 17.807276  16.926373  14.610975  15.465288  17.16993   19.187681\n 12.046843  17.8665    15.320848  14.284036  21.911493  16.848612\n 15.086016   9.249034  20.100746  16.08186   14.61541   18.750427\n 18.848988  12.998299  16.090536  15.454417  15.967635  15.886283\n 13.979682  14.548558  18.48061   17.298395  16.472368  14.35195\n 14.121294  22.585928  16.42496   10.118932  13.31067   16.355394\n 15.318997  19.148796  14.280228  16.776657  18.072918  18.803349\n 16.268045  14.652501  18.15585   13.406559  15.687352  16.039753\n 13.907009  15.486366  13.076628  16.996716  14.608507  18.307545\n 13.763356  14.141198  16.757977  12.4969845 15.689609  12.992243\n 14.946997  13.635295  15.590447  15.676974  18.467773  16.095303\n 18.142584  11.216122  18.792389  10.452918  16.90553   15.33745\n 16.470274  15.365257  16.164211  14.66824   14.5302305 16.097383\n 16.108347  17.024574  15.45448   21.989     17.595242  15.431184\n 15.54887   15.286607  18.260164  13.502544  17.560427  15.8209915\n 19.235785  17.922016  23.560848  14.257054  15.87085   13.700588\n 15.207566  16.219261  15.529406  16.220516  15.216764  14.220524\n 12.053419  16.609085  17.335032  17.004242  15.139981  13.783572\n 17.68277   14.849902  13.669212  24.484604  15.513056  14.820519\n 16.336792  21.249754  16.967516  14.780756  18.003225  15.929941\n 16.130709  17.361929  14.429351  12.774805  15.839921  17.186174\n 17.01185   17.08816   14.010734  13.915031  16.146368  16.731785\n 15.567299  17.381367  16.359634  17.88054   14.999553  17.710804\n 12.98218   18.50484   14.120368  15.968441  16.027855  20.233713\n 15.812192  22.967136  15.100148  13.9978895 16.032675  10.529536\n 16.975786  18.448612  18.88754   26.261164  21.147633  14.45036\n 16.412663  20.129116  15.884655  13.863247  15.109468  12.147299\n 13.491816  18.26895   13.990795  15.207764  17.958963  13.006128\n 15.281943  17.167776  14.75569   19.615229  22.126968  15.514891\n 14.56775   14.866624  15.7457485 17.037512  16.852757  12.504863\n 14.47045   21.185434  17.146763  17.208502  21.192148  14.378992\n 15.450664  12.771581  18.741287  15.9718275 13.72923   16.859879\n 15.864746  13.187877  15.523091  16.227507  10.472687  14.718022\n 15.116626  13.511364  16.843184  14.407455  14.567862  16.18313\n 15.10113   19.237007  17.927546  15.413036  15.33482   16.440987\n 16.686193  16.374891  15.785529  16.660233  13.97787   15.116169\n 14.361969  15.549977  15.459368  15.578974  15.468529  16.281607\n 15.660629  16.447601  13.167147  16.036129  10.324909  18.146072\n 18.096283  12.409779  16.67876   18.948586  10.28549   18.770071\n 16.875147  17.21381   15.597772  16.223192 ]", "policy_t": "[[-0.0270732  -0.8227668  -0.6852669   0.64330435  0.82184005 -0.15992194]\n [ 0.79182315 -0.4396845   0.2377752  -0.457924   -0.70280635  0.23043811]\n [ 0.65754676  0.7303493  -0.698363    0.32761705  0.19011796 -0.35480082]\n ...\n [-0.67332023  0.3859625   0.4705168   0.8167777   0.06755602  0.40826082]\n [ 0.9251677   0.56351495  0.55202603  0.42520022  0.66065943 -0.5684569 ]\n [ 0.59970033 -0.21170038 -0.1797517   0.88936365  0.86429167  0.6694257 ]]", "td_error": "[0.454916   0.65362597 0.5250249  0.41879463 1.7799559  1.2654443\n 1.6567354  0.41633415 0.8417673  0.8010092  1.4492974  0.29114866\n 0.60586786 0.6275997  1.6910353  0.22661781 0.6317625  0.93871117\n 1.2898159  0.6158333  0.71937704 0.6182823  0.8335743  0.05181026\n 1.685678   0.45629978 0.50279    0.6203027  1.1686945  0.7331977\n 0.39461088 1.2189646  2.2544928  0.296031   2.561139   0.32382488\n 1.1444588  0.17264318 0.5037861  0.41690445 0.48967934 0.17180538\n 1.5439892  0.5770092  1.5631294  0.71336746 1.1444573  0.09985399\n 0.7913089  3.5237284  0.30710983 1.0912848  0.27096415 0.57689905\n 0.3121934  0.21103573 0.6507516  0.5108566  0.66389656 0.6798239\n 0.39940882 0.15547943 1.4380054  0.69481754 0.08424664 1.1515222\n 0.05885649 1.3920851  0.375288   2.1234012  0.4959755  1.0779247\n 0.6535816  1.2747908  0.23860121 0.4900937  0.14858341 1.0277538\n 0.381104   0.37547684 0.73483944 0.27178955 1.0560408  1.0653906\n 1.5680485  0.48524714 0.50144005 0.3909626  0.2752242  1.4180527\n 0.6293602  0.29451513 1.3753357  1.4562478  0.5333824  1.2019944\n 0.25797415 1.3220277  1.6608915  1.4308615  1.7266898  1.8887095\n 0.3356638  1.8090506  2.091385   1.1067548  0.96580076 1.3514895\n 0.6544838  0.9221201  1.9411583  0.3792739  0.36226988 0.90782404\n 1.3752422  1.7173176  1.8645535  0.9649048  0.8430133  0.5025916\n 0.18621588 0.5179825  0.8377614  1.1996408  0.63116074 0.3663659\n 0.87740326 0.6968241  0.98889065 1.4403305  0.6129961  0.1811471\n 0.30931377 1.0444489  0.9722557  0.87212086 1.4658718  2.0451212\n 0.4949131  1.3279152  1.0668678  1.5348296  1.1357484  1.3304687\n 0.64598656 0.7919712  0.7450471  0.5554967  0.500597   1.2102251\n 0.57373667 1.233057   1.3367577  2.0419416  0.42167997 0.6429863\n 0.65031767 1.0872364  0.58271646 1.0840588  0.14905214 0.6667299\n 0.63713074 0.42779922 0.13733721 1.2723246  0.58008766 1.3618917\n 0.9871006  1.1894064  1.2490921  1.2421427  2.2663174  0.09227753\n 0.37459373 1.0160027  1.127428   1.28265    0.39373875 0.14489794\n 0.85778856 0.60348225 1.1140928  1.242506   0.7162962  1.3127942\n 1.5575705  0.26645184 0.5095763  2.076892   0.14015675 1.9070678\n 0.14463711 1.7996001  1.3960161  0.6473484  0.81352234 0.9847746\n 0.6511493  0.7979193  0.6296692  1.1026282  2.114049   0.23062706\n 0.4415474  0.5848961  1.3764496  0.81465816 0.20614147 0.48614407\n 0.45687675 1.4143467  0.73089504 0.16758823 0.09575558 1.6143441\n 0.50388765 0.5279536  0.9440155  0.81461716 0.5718422  0.7814417\n 0.25253677 1.1359329  0.80781555 0.0753026  0.48373222 1.0262609\n 1.0840411  1.153307   0.79206276 0.36101103 0.22296667 1.2363043\n 0.43316698 0.9763994  0.50187445 1.1789436  0.68020535 1.8842149\n 0.85427046 0.09374952 0.8204951  0.37381506 0.61076355 0.9267945\n 0.76924133 1.3375411  1.0739245  1.7771301  0.927361   0.14672184\n 0.6026788  1.2640476  0.8934579  1.2688398 ]", "mean_td_error": 0.8689201474189758, "actor_loss": -16.121599197387695, "critic_loss": 0.5597748756408691, "alpha_loss": -14.305008888244629, "alpha_value": 0.22321817278862, "target_entropy": -6, "mean_q": 15.307018280029297, "max_q": 26.25269317626953, "min_q": 8.903125762939453, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 15000, "episodes_total": 15, "training_iteration": 15, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_14-49-32", "timestamp": 1587048572, "time_this_iter_s": 85.02665877342224, "time_total_s": 458.9923369884491, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 458.9923369884491, "timesteps_since_restore": 15000, "iterations_since_restore": 15, "perf": {"cpu_util_percent": 92.25034965034965, "ram_util_percent": 11.299999999999997}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -161.99868585284742, "episode_reward_min": -306.3627502160501, "episode_reward_mean": -224.236512194594, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-188.06583230153944, -306.3627502160501, -161.99868585284742, -221.14961959779822, -231.45892467436855, -253.2046864737556, -234.83389639927847, -207.7305695082432, -206.139462812874, -231.42069410918538], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2597315114451768, "mean_processing_ms": 0.1197205938584555, "mean_inference_ms": 1.1711324388417041}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -94.96187381235399, "episode_reward_min": -286.016112222877, "episode_reward_mean": -205.46633445747207, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-247.13998468717926, -262.5981647835083, -94.96187381235399, -136.6155367814418, -286.016112222877], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.31198814033114514, "mean_processing_ms": 0.43663591333714746, "mean_inference_ms": 1.4568359517288558}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 8000, "num_steps_trained": 1534720, "num_steps_sampled": 16000, "sample_time_ms": 3.58, "replay_time_ms": 22.05, "grad_time_ms": 56.199, "update_time_ms": 0.006, "opt_peak_throughput": 4555.26, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[15.976706 ]\n [14.122589 ]\n [13.510642 ]\n [14.31887  ]\n [15.209476 ]\n [14.535573 ]\n [16.268522 ]\n [17.866161 ]\n [18.455204 ]\n [16.502102 ]\n [17.47653  ]\n [14.802712 ]\n [11.175807 ]\n [15.197665 ]\n [15.431394 ]\n [16.72697  ]\n [14.357326 ]\n [ 8.927579 ]\n [16.317612 ]\n [12.4630785]\n [16.617813 ]\n [15.284989 ]\n [13.946318 ]\n [18.971046 ]\n [15.484132 ]\n [14.989581 ]\n [16.083767 ]\n [14.00028  ]\n [16.4494   ]\n [14.9447975]\n [13.296661 ]\n [13.157501 ]\n [12.2150755]\n [14.196212 ]\n [16.054697 ]\n [15.202248 ]\n [14.05492  ]\n [12.03333  ]\n [16.59288  ]\n [17.535707 ]\n [14.438353 ]\n [15.858311 ]\n [16.628992 ]\n [10.113539 ]\n [15.204182 ]\n [12.8786335]\n [15.675104 ]\n [11.439224 ]\n [17.795528 ]\n [20.786484 ]\n [12.466753 ]\n [16.590471 ]\n [16.235594 ]\n [14.887054 ]\n [12.010265 ]\n [14.587555 ]\n [15.1747265]\n [16.562637 ]\n [11.087185 ]\n [14.039182 ]\n [15.160258 ]\n [15.125455 ]\n [15.707015 ]\n [17.143108 ]\n [12.067396 ]\n [18.936243 ]\n [14.429933 ]\n [16.804556 ]\n [18.360935 ]\n [17.201735 ]\n [17.582232 ]\n [15.938852 ]\n [14.445496 ]\n [16.876217 ]\n [16.823172 ]\n [14.57862  ]\n [17.188257 ]\n [ 9.276357 ]\n [17.687157 ]\n [13.921633 ]\n [13.228804 ]\n [13.043175 ]\n [14.369722 ]\n [11.141053 ]\n [15.4883375]\n [12.212319 ]\n [18.335426 ]\n [25.300047 ]\n [12.93776  ]\n [14.643446 ]\n [17.81361  ]\n [12.978054 ]\n [15.056077 ]\n [15.633319 ]\n [18.49561  ]\n [14.837995 ]\n [16.279806 ]\n [13.697329 ]\n [14.55498  ]\n [14.880427 ]\n [16.42234  ]\n [14.094682 ]\n [13.862635 ]\n [23.205275 ]\n [14.312844 ]\n [16.138231 ]\n [10.6233015]\n [14.765598 ]\n [14.48111  ]\n [15.304401 ]\n [13.730637 ]\n [14.093502 ]\n [16.953201 ]\n [14.871608 ]\n [14.2035675]\n [13.256822 ]\n [17.677437 ]\n [16.52173  ]\n [15.55871  ]\n [14.453178 ]\n [19.322502 ]\n [13.263891 ]\n [14.917304 ]\n [20.032698 ]\n [21.900793 ]\n [14.350907 ]\n [16.325542 ]\n [19.065441 ]\n [15.313128 ]\n [16.023499 ]\n [17.25815  ]\n [14.61456  ]\n [14.689277 ]\n [ 8.29683  ]\n [16.051418 ]\n [15.06852  ]\n [11.718659 ]\n [16.012724 ]\n [12.917672 ]\n [15.411585 ]\n [17.863232 ]\n [12.460229 ]\n [14.041541 ]\n [20.223494 ]\n [17.457287 ]\n [18.486805 ]\n [14.236959 ]\n [17.449547 ]\n [17.132265 ]\n [14.737635 ]\n [16.474672 ]\n [14.108075 ]\n [ 9.31452  ]\n [18.847597 ]\n [15.360824 ]\n [14.75988  ]\n [14.501218 ]\n [14.273904 ]\n [17.572855 ]\n [20.457901 ]\n [17.929354 ]\n [14.033794 ]\n [10.174266 ]\n [15.259989 ]\n [15.034804 ]\n [16.427467 ]\n [14.608509 ]\n [12.65399  ]\n [11.92752  ]\n [15.747915 ]\n [16.594059 ]\n [15.746721 ]\n [14.349196 ]\n [16.154192 ]\n [21.719868 ]\n [12.639925 ]\n [14.729875 ]\n [16.439781 ]\n [ 9.1312065]\n [11.509676 ]\n [11.036587 ]\n [16.335125 ]\n [17.95884  ]\n [17.956823 ]\n [19.142511 ]\n [18.593733 ]\n [16.402029 ]\n [17.257902 ]\n [15.526812 ]\n [18.473207 ]\n [17.09956  ]\n [16.59842  ]\n [14.72783  ]\n [16.904028 ]\n [15.818414 ]\n [14.877258 ]\n [17.758913 ]\n [13.144154 ]\n [15.377132 ]\n [19.802025 ]\n [13.035582 ]\n [14.237627 ]\n [15.536492 ]\n [13.198352 ]\n [15.405535 ]\n [15.79947  ]\n [14.830499 ]\n [15.8098955]\n [17.461248 ]\n [11.509272 ]\n [15.350097 ]\n [19.074965 ]\n [17.318357 ]\n [19.15485  ]\n [17.199066 ]\n [13.888249 ]\n [15.276582 ]\n [17.226702 ]\n [13.854397 ]\n [14.41824  ]\n [14.410493 ]\n [12.295284 ]\n [16.291166 ]\n [12.882441 ]\n [10.423499 ]\n [11.729014 ]\n [16.729057 ]\n [13.237221 ]\n [16.0026   ]\n [16.702196 ]\n [14.510385 ]\n [12.996523 ]\n [17.27895  ]\n [16.400679 ]\n [19.173214 ]\n [14.357326 ]\n [19.596619 ]\n [16.777393 ]\n [22.02117  ]\n [17.962654 ]\n [14.774824 ]\n [15.536823 ]\n [10.194497 ]\n [14.410493 ]\n [14.91042  ]\n [25.341782 ]\n [16.128435 ]\n [17.331354 ]\n [15.138276 ]\n [12.3527775]\n [15.848183 ]\n [15.784027 ]\n [17.129404 ]\n [11.028568 ]\n [17.281122 ]\n [17.715609 ]]", "q_t_selected": "[15.976706  14.122589  13.510642  14.31887   15.209476  14.535573\n 16.268522  17.866161  18.455204  16.502102  17.47653   14.802712\n 11.175807  15.197665  15.431394  16.72697   14.357326   8.927579\n 16.317612  12.4630785 16.617813  15.284989  13.946318  18.971046\n 15.484132  14.989581  16.083767  14.00028   16.4494    14.9447975\n 13.296661  13.157501  12.2150755 14.196212  16.054697  15.202248\n 14.05492   12.03333   16.59288   17.535707  14.438353  15.858311\n 16.628992  10.113539  15.204182  12.8786335 15.675104  11.439224\n 17.795528  20.786484  12.466753  16.590471  16.235594  14.887054\n 12.010265  14.587555  15.1747265 16.562637  11.087185  14.039182\n 15.160258  15.125455  15.707015  17.143108  12.067396  18.936243\n 14.429933  16.804556  18.360935  17.201735  17.582232  15.938852\n 14.445496  16.876217  16.823172  14.57862   17.188257   9.276357\n 17.687157  13.921633  13.228804  13.043175  14.369722  11.141053\n 15.4883375 12.212319  18.335426  25.300047  12.93776   14.643446\n 17.81361   12.978054  15.056077  15.633319  18.49561   14.837995\n 16.279806  13.697329  14.55498   14.880427  16.42234   14.094682\n 13.862635  23.205275  14.312844  16.138231  10.6233015 14.765598\n 14.48111   15.304401  13.730637  14.093502  16.953201  14.871608\n 14.2035675 13.256822  17.677437  16.52173   15.55871   14.453178\n 19.322502  13.263891  14.917304  20.032698  21.900793  14.350907\n 16.325542  19.065441  15.313128  16.023499  17.25815   14.61456\n 14.689277   8.29683   16.051418  15.06852   11.718659  16.012724\n 12.917672  15.411585  17.863232  12.460229  14.041541  20.223494\n 17.457287  18.486805  14.236959  17.449547  17.132265  14.737635\n 16.474672  14.108075   9.31452   18.847597  15.360824  14.75988\n 14.501218  14.273904  17.572855  20.457901  17.929354  14.033794\n 10.174266  15.259989  15.034804  16.427467  14.608509  12.65399\n 11.92752   15.747915  16.594059  15.746721  14.349196  16.154192\n 21.719868  12.639925  14.729875  16.439781   9.1312065 11.509676\n 11.036587  16.335125  17.95884   17.956823  19.142511  18.593733\n 16.402029  17.257902  15.526812  18.473207  17.09956   16.59842\n 14.72783   16.904028  15.818414  14.877258  17.758913  13.144154\n 15.377132  19.802025  13.035582  14.237627  15.536492  13.198352\n 15.405535  15.79947   14.830499  15.8098955 17.461248  11.509272\n 15.350097  19.074965  17.318357  19.15485   17.199066  13.888249\n 15.276582  17.226702  13.854397  14.41824   14.410493  12.295284\n 16.291166  12.882441  10.423499  11.729014  16.729057  13.237221\n 16.0026    16.702196  14.510385  12.996523  17.27895   16.400679\n 19.173214  14.357326  19.596619  16.777393  22.02117   17.962654\n 14.774824  15.536823  10.194497  14.410493  14.91042   25.341782\n 16.128435  17.331354  15.138276  12.3527775 15.848183  15.784027\n 17.129404  11.028568  17.281122  17.715609 ]", "twin_q_t_selected": "[15.621656  13.7488985 13.928704  14.777815  15.89636   13.998201\n 17.162188  18.070454  17.763945  16.290081  17.565836  14.290402\n 11.230583  14.7198715 15.442162  16.702053  14.49556    9.530931\n 16.61876   12.974137  16.346035  16.565222  14.946394  18.602907\n 15.498958  13.884733  16.321575  14.130483  15.367982  13.733004\n 14.442534  13.178857  12.83198   14.320376  15.845753  15.4538355\n 13.59209   11.113144  16.239666  17.522371  13.987114  15.021442\n 16.691986  10.385671  14.855488  12.799879  15.613103  11.5386095\n 16.973518  19.280323  13.031768  17.183544  16.578207  14.036938\n 12.126977  14.126131  14.726709  17.001638  11.595543  13.636369\n 14.548622  15.583415  15.03989   18.131542  11.822327  19.017057\n 14.714562  17.14378   18.866728  16.349026  17.138979  16.615\n 15.864657  17.150114  16.979733  14.814981  17.706514   9.492001\n 17.52096   14.3882475 13.188326  12.977934  13.8336935 11.200634\n 15.048335  12.488421  18.771914  24.920471  13.394118  15.522848\n 17.737677  13.889193  15.470397  15.794368  18.132864  14.956329\n 17.170832  13.705867  14.854315  14.5326805 16.486235  14.280456\n 14.38349   24.079432  14.288747  15.834853  11.581791  14.881548\n 14.197321  15.508172  14.226783  14.057755  17.325825  14.4850645\n 14.33724   11.934431  17.464954  17.089144  16.101755  14.842007\n 20.203653  13.97851   15.420733  20.948977  21.900406  14.552088\n 15.888657  20.579643  14.398717  16.689182  16.829119  14.553088\n 15.548823   8.705255  15.521521  15.128912  11.542635  15.9344015\n 12.855629  15.201215  18.048336  12.264809  13.730917  20.898155\n 17.057543  18.460917  14.683021  17.205475  16.616251  14.749238\n 15.866606  13.744881   9.138508  18.419767  15.421168  15.039169\n 13.923074  13.91793   17.090973  21.362537  17.25531   13.966189\n  9.117462  14.789766  16.051327  16.406416  14.355861  12.88739\n 10.866589  16.209982  16.418554  15.914114  14.240545  15.527101\n 21.660778  12.319567  14.776974  16.4243     9.220985  12.404886\n 10.895468  16.641628  17.215752  17.406355  18.517973  16.948763\n 16.55075   16.739372  14.465677  18.123913  17.812353  17.40877\n 15.424145  16.571098  15.465588  14.218262  17.971941  13.122242\n 15.506126  20.439566  12.696717  14.191789  16.063221  12.185328\n 14.957284  14.89705   13.998239  16.345093  17.400717  11.544475\n 15.616034  18.725437  17.08299   18.336746  17.25376   13.826307\n 15.080851  17.358326  14.28746   14.660446  15.621532  12.422895\n 16.270113  13.394723  10.982317  11.812308  16.550251  13.920109\n 17.174139  17.000986  13.430876  13.22657   17.591991  17.057259\n 20.087734  14.49556   19.58858   17.342005  22.662306  17.66746\n 13.988537  14.964492   9.682522  15.621532  14.42554   24.72544\n 15.644847  17.215528  16.215796  12.948545  15.378046  15.491048\n 16.891275  11.396629  17.587273  17.616867 ]", "q_t_selected_target": "[15.270177  15.307044  13.823834  15.211329  15.572357  14.771414\n 18.024849  17.962013  17.401978  14.254406  18.491972  14.907924\n 10.602649  16.160095  15.365786  14.583351  15.746973   8.922293\n 17.367643  12.336905  17.247438  15.322363  14.885037  20.483341\n 14.926298  15.466069  15.403472  14.17697   15.970148  14.065421\n 11.640565  13.801089  11.379867  14.947502  16.834465  15.0926485\n 14.540314  10.323832  17.165375  15.280709  15.192629  15.574534\n 16.334145  10.000517  16.860107  12.987171  15.4323635 10.240639\n 18.250658  20.957907  12.269795  15.735315  18.004421  14.167106\n 11.0147505 15.699469  16.623268  17.284307  11.485047  11.936685\n 14.346946  13.927367  17.046982  17.241947  10.275772  18.221304\n 15.342408  18.380322  18.06563   18.45879   17.738403  17.218485\n 13.894475  18.446016  18.459957  15.437795  16.508652   8.30785\n 18.520443  13.814181  13.445769  11.7122965 14.37897   12.1014805\n 14.6862545 12.312951  17.058893  26.25793   14.441703  15.559246\n 18.266088  12.241262  15.387094  15.2680855 17.901337  13.478198\n 17.910986  12.224828  15.301252  15.913178  17.41217   12.142158\n 13.213377  23.21827   14.984005  17.252739  10.851077  15.870156\n 15.350122  16.010225  13.924565  14.063886  18.285233  15.636208\n 13.590387  12.353941  18.576643  17.252377  16.429893  14.136233\n 18.459768  13.779272  14.905133  20.993841  23.552824  15.015537\n 14.965668  20.760492  15.975521  16.693214  15.879886  14.3899975\n 17.140486   5.8228064 15.256223  13.920734  11.202786  16.493515\n 12.625036  16.386621  17.027342  10.464708  15.8018    20.33875\n 17.842905  18.6772    14.942803  17.791487  16.103283  15.017395\n 15.217299  13.416932   9.165703  18.699472  15.164442  16.300585\n 15.611793  14.001218  18.649603  21.112434  17.072681  13.376692\n  9.633491  16.177792  15.684256  16.267302  14.427519  12.353018\n 11.014559  16.353546  16.113844  16.667067  13.381475  15.940185\n 23.03561   12.321544  15.366267  15.8175535  8.348562  13.354072\n 11.013067  15.989834  19.500519  19.790956  18.40227   18.10577\n 18.000523  17.452932  14.7664995 17.15433   16.88626   16.590067\n 14.300135  15.807638  15.231667  13.384619  18.727371  13.553126\n 17.292181  22.375881  11.724667  13.634423  13.8127165 11.73419\n 15.452231  15.45754   14.582219  15.530589  18.031862  10.639795\n 15.189142  20.04043   18.188107  20.395107  16.094717  14.426773\n 15.629835  17.186466  13.511941  13.636414  15.518209  12.508441\n 16.327349  14.465215   9.862028  10.62059   17.309175  15.179365\n 15.586172  14.766898  15.029527  15.077515  17.089191  17.872942\n 20.869228  16.438744  20.473389  17.028593  21.954935  19.32187\n 14.863125  14.936593   8.441722  15.569435  15.738183  25.052248\n 15.101132  19.299812  15.383052  11.220604  15.848632  15.427246\n 17.907837  12.850682  18.777782  18.18233  ]", "q_tp1_best_masked": "[15.643902  16.809006  14.03642   16.631983  16.311682  16.307684\n 17.602999  19.28773   16.463161  15.27729   17.74348   16.008867\n 11.295275  17.065126  15.823433  15.310528  15.949345  10.222712\n 16.937275  12.821056  17.072765  16.294586  15.903932  19.722229\n 14.914374  15.769943  16.120817  14.333921  16.468943  14.422763\n 12.740349  14.139116  13.194063  15.437091  16.470598  15.572313\n 15.738949  11.8356905 16.570168  16.085564  15.899755  16.208918\n 17.169296  11.284811  18.14381   14.324114  15.900732  11.578776\n 17.784492  20.07065   14.089033  16.830166  18.91363   14.36191\n 11.824342  16.50256   17.428286  17.141241  11.551275  12.817194\n 14.885782  14.707851  17.270435  17.837063  12.142698  17.436338\n 15.3196745 18.002712  17.853432  18.286745  17.823925  16.896315\n 14.199248  18.380121  18.702663  16.356188  16.594992   9.107843\n 18.756334  14.464793  13.423791  13.763513  14.755208  13.71181\n 15.214727  13.065947  16.186922  25.469805  15.371869  17.5175\n 18.457706  12.813934  16.679073  15.961946  18.709385  15.31725\n 17.063675  12.923383  15.501438  16.462648  17.604301  12.297112\n 14.198452  22.6737    15.455046  18.45381   12.232926  16.168705\n 15.791333  15.6336    15.101427  15.009465  19.44569   16.250763\n 14.920909  13.60114   18.577396  17.67995   16.058638  14.6338215\n 18.815704  14.869571  15.625392  20.610058  23.736593  15.829237\n 16.192375  20.882057  17.528261  16.318382  16.021214  14.72445\n 17.63998    8.178671  15.61522   14.531992  11.771424  15.7517605\n 13.751122  16.820889  17.159065  10.828372  16.7824    20.41002\n 18.424406  18.682936  15.860226  17.509731  16.156294  15.894137\n 15.63582   13.684438  11.048032  18.962431  15.676229  16.314293\n 16.089462  14.42004   19.23766   20.513329  17.445667  13.932346\n 11.701036  17.27695   15.982259  15.456685  15.105009  13.203749\n 13.257214  16.325266  15.601397  16.65763   13.594971  15.548516\n 21.47321   13.354608  16.5719    16.560713   9.580134  15.105068\n 12.525096  16.061647  19.131578  19.224686  17.66842   17.669989\n 18.589003  18.767654  15.936098  17.73483   17.193142  16.853455\n 14.049016  15.91279   16.499903  14.064729  17.557339  15.140956\n 18.48784   22.205992  12.483195  15.066067  14.635032  12.681319\n 16.096062  16.221685  14.538177  15.869326  18.106255  10.89122\n 15.4427185 19.333973  18.921324  20.714098  16.142427  15.983706\n 16.833422  17.435717  13.892792  14.569471  16.785315  12.968254\n 16.76355   15.486618  11.553077  12.447049  17.836191  15.520122\n 15.145918  15.559571  16.970964  15.451832  16.970846  18.6968\n 20.072554  16.648104  20.041458  17.356098  20.675528  19.1528\n 15.985042  16.574997   9.570193  16.837059  16.92207   24.188292\n 15.188645  19.052185  16.186268  12.083137  17.396877  16.638634\n 18.38275   13.68624   18.589552  17.737804 ]", "policy_t": "[[ 0.79713535  0.97593236  0.8489554   0.74497986  0.8709266   0.978539  ]\n [ 0.48395193 -0.67808187 -0.944553   -0.02187204  0.8502661   0.4744445 ]\n [ 0.0979147   0.8901262   0.5800109  -0.7564575  -0.8127184  -0.9494704 ]\n ...\n [ 0.7949995  -0.8842779   0.7285497  -0.6455035  -0.07903439  0.25477004]\n [ 0.04572165  0.2867074   0.44202948  0.7240449   0.9461117  -0.05463594]\n [-0.23700422 -0.5846838  -0.6605554   0.9022043   0.42960882  0.3181628 ]]", "td_error": "[0.5290041  1.3713002  0.2090311  0.66298723 0.34344196 0.5045266\n 1.309494   0.10214615 0.7075968  2.1416855  0.97078896 0.36136627\n 0.60054636 1.2013268  0.07099199 2.1311607  1.3205304  0.3069625\n 0.89945793 0.38170338 0.7655144  0.6401162  0.50003815 1.6963644\n 0.5652466  1.0289121  0.7991991  0.111588   0.540709   0.60589695\n 2.229033   0.63291025 1.143661   0.68920803 0.88424015 0.23539305\n 0.7168088  1.2494054  0.74910164 2.24833    0.9798956  0.41843414\n 0.3263445  0.24908781 1.8302727  0.14791489 0.21174002 1.2482781\n 0.86613464 0.9245033  0.479465   1.1516924  1.5975208  0.42505836\n 1.0538707  1.3426256  1.6725502  0.5021696  0.254179   1.9010906\n 0.50749445 1.4270678  1.6735291  0.49421692 1.6690893  0.7553463\n 0.7701607  1.4061537  0.54820156 1.6834097  0.37779808 0.94155884\n 1.2606015  1.4328508  1.558505   0.740994   0.93873405 1.0763288\n 0.9163847  0.3407588  0.23720455 1.2982578  0.2772622  0.9306369\n 0.5820818  0.13805103 1.4947767  1.1476717  1.2757635  0.476099\n 0.49044514 1.1923609  0.20716    0.44575787 0.41289997 1.4189639\n 1.185667   1.4767699  0.5966048  1.2066245  0.9578829  2.045411\n 0.90968513 0.43707848 0.6832094  1.2661967  0.4792447  1.0465832\n 1.0109072  0.6039386  0.2480731  0.01787329 1.1457195  0.95787144\n 0.6800165  0.6611953  1.0054474  0.44693947 0.5996609  0.5113592\n 1.3033094  0.35730934 0.2638855  0.5030041  1.6522245  0.5640397\n 1.1414318  0.93795013 1.1195984  0.336874   1.1637487  0.19382668\n 2.0214357  2.678236   0.53024673 1.1779814  0.42786074 0.5199523\n 0.26161432 1.0802217  0.928442   1.8978105  1.9155707  0.33733082\n 0.5854902  0.20333958 0.48281336 0.4639759  0.7709751  0.27395868\n 0.9533396  0.5095458  0.08800602 0.21391487 0.22655392 1.4010601\n 1.3996468  0.1779871  1.317689   0.4523182  0.51965046 0.6233001\n 0.52840185 1.152914   0.5082612  0.14964008 0.12632418 0.41767216\n 0.5304656  0.37459755 0.39246273 0.83664894 0.9133954  0.3135457\n 1.3452873  0.16017914 0.61284304 0.6144867  0.8275337  1.3967905\n 0.0705595  0.4985428  1.9132233  2.1093674  0.4279728  0.82248497\n 1.5241327  0.45429516 0.53056717 1.1442308  0.5696974  0.4135275\n 0.7758527  0.92992496 0.4103341  1.1631413  0.8619442  0.41992855\n 1.8505516  2.255086   1.1414828  0.5802846  1.9871402  0.9576497\n 0.27082205 0.45121002 0.41613007 0.54690504 0.60087967 0.8870778\n 0.2939229  1.1402292  0.9874325  1.6493092  1.1316957  0.5694947\n 0.45111895 0.10604763 0.5589876  0.9029293  0.6055198  0.14935112\n 0.04670906 1.326633   0.8408799  1.1500711  0.66952133 1.6007004\n 1.0021973  2.084693   1.0588965  1.9659681  0.34627914 1.1439734\n 1.2387543  2.012301   0.88078976 0.28230572 0.38680267 1.506814\n 0.48144436 0.3140645  1.4967875  0.6055198  1.0702028  0.30817127\n 0.78550863 2.026371   0.5387597  1.4300575  0.2355175  0.21029139\n 0.8974972  1.6380835  1.343585   0.51609135]", "mean_td_error": 0.8539735674858093, "actor_loss": -16.150253295898438, "critic_loss": 0.5422941446304321, "alpha_loss": -16.11745834350586, "alpha_value": 0.16699405014514923, "target_entropy": -6, "mean_q": 15.422325134277344, "max_q": 25.341781616210938, "min_q": 8.296830177307129, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 16000, "episodes_total": 16, "training_iteration": 16, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_14-51-14", "timestamp": 1587048674, "time_this_iter_s": 86.66365194320679, "time_total_s": 545.6559889316559, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 545.6559889316559, "timesteps_since_restore": 16000, "iterations_since_restore": 16, "perf": {"cpu_util_percent": 92.1558620689655, "ram_util_percent": 11.299999999999999}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -194.4629962130586, "episode_reward_min": -272.76327752657915, "episode_reward_mean": -234.40989966850947, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-208.60972463359315, -194.4629962130586, -232.23622316353746, -230.90127471038915, -272.76327752657915, -272.18405594291175, -214.88793551929808, -261.0529651250035, -248.65404285675066, -208.3465009939739], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.25969801358518424, "mean_processing_ms": 0.11961986761973913, "mean_inference_ms": 1.1712725972036429}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -94.96187381235399, "episode_reward_min": -286.016112222877, "episode_reward_mean": -197.2181454208939, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-221.35721960061747, -94.96187381235399, -136.6155367814418, -286.016112222877, -247.13998468717926], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.31722699120235853, "mean_processing_ms": 0.44292571266723535, "mean_inference_ms": 1.496090271802166}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 8500, "num_steps_trained": 1790720, "num_steps_sampled": 17000, "sample_time_ms": 3.228, "replay_time_ms": 22.396, "grad_time_ms": 59.025, "update_time_ms": 0.004, "opt_peak_throughput": 4337.133, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[15.694581 ]\n [15.99009  ]\n [14.314847 ]\n [15.762679 ]\n [14.198464 ]\n [22.462692 ]\n [14.196552 ]\n [20.689384 ]\n [16.595034 ]\n [13.841483 ]\n [12.971687 ]\n [13.215886 ]\n [14.690366 ]\n [20.61793  ]\n [13.457447 ]\n [13.794014 ]\n [14.833035 ]\n [18.381044 ]\n [16.349337 ]\n [15.715164 ]\n [14.104779 ]\n [20.260473 ]\n [16.728693 ]\n [14.720383 ]\n [20.388105 ]\n [20.103916 ]\n [14.742732 ]\n [17.456478 ]\n [13.572561 ]\n [14.358858 ]\n [14.364457 ]\n [18.161777 ]\n [14.943561 ]\n [15.857423 ]\n [14.58351  ]\n [15.857133 ]\n [20.248621 ]\n [15.025492 ]\n [10.306296 ]\n [14.387316 ]\n [18.146278 ]\n [15.587104 ]\n [16.312977 ]\n [12.762725 ]\n [14.929726 ]\n [14.347809 ]\n [12.324356 ]\n [13.723906 ]\n [15.933876 ]\n [14.250665 ]\n [13.358992 ]\n [12.308334 ]\n [14.08665  ]\n [18.61208  ]\n [23.137917 ]\n [17.7956   ]\n [11.977509 ]\n [14.841539 ]\n [15.590159 ]\n [19.983995 ]\n [20.183037 ]\n [16.401936 ]\n [15.9013195]\n [19.681168 ]\n [17.151628 ]\n [17.107893 ]\n [14.425603 ]\n [17.237185 ]\n [15.5498705]\n [15.513548 ]\n [14.385102 ]\n [15.770022 ]\n [15.590065 ]\n [18.574844 ]\n [17.445343 ]\n [15.04735  ]\n [13.175399 ]\n [16.325628 ]\n [15.963936 ]\n [15.472344 ]\n [16.926064 ]\n [16.330755 ]\n [15.722382 ]\n [18.731632 ]\n [20.337503 ]\n [17.645285 ]\n [18.965649 ]\n [14.548346 ]\n [15.68292  ]\n [14.483388 ]\n [18.917591 ]\n [19.58702  ]\n [17.292023 ]\n [14.607057 ]\n [17.414007 ]\n [15.757844 ]\n [15.4964075]\n [15.286757 ]\n [18.81415  ]\n [15.466165 ]\n [11.396301 ]\n [21.315578 ]\n [20.257734 ]\n [14.155696 ]\n [12.812343 ]\n [21.280481 ]\n [16.108978 ]\n [17.262342 ]\n [15.301691 ]\n [26.79604  ]\n [15.40565  ]\n [14.106922 ]\n [13.835367 ]\n [16.811111 ]\n [ 9.2797785]\n [21.07647  ]\n [15.165762 ]\n [21.235865 ]\n [ 8.291613 ]\n [17.92625  ]\n [15.084022 ]\n [19.638788 ]\n [17.751389 ]\n [15.313144 ]\n [14.839274 ]\n [14.661833 ]\n [14.306232 ]\n [14.020712 ]\n [16.444578 ]\n [16.859455 ]\n [17.009424 ]\n [15.114705 ]\n [15.438598 ]\n [18.901913 ]\n [15.945697 ]\n [19.18433  ]\n [13.358992 ]\n [21.966354 ]\n [14.799687 ]\n [17.59569  ]\n [15.653414 ]\n [12.372883 ]\n [13.764148 ]\n [17.899935 ]\n [15.36584  ]\n [12.160861 ]\n [16.860537 ]\n [14.830839 ]\n [15.10475  ]\n [18.192348 ]\n [16.187193 ]\n [16.458593 ]\n [16.172096 ]\n [14.903886 ]\n [15.2198105]\n [16.58568  ]\n [14.9056015]\n [17.08753  ]\n [16.311    ]\n [ 9.456581 ]\n [19.88848  ]\n [17.663593 ]\n [15.276437 ]\n [18.314    ]\n [17.222721 ]\n [14.331252 ]\n [15.112589 ]\n [16.79821  ]\n [12.945095 ]\n [ 8.084395 ]\n [11.778913 ]\n [17.255499 ]\n [14.973724 ]\n [17.104626 ]\n [19.948645 ]\n [15.324967 ]\n [16.593014 ]\n [14.99853  ]\n [15.6465845]\n [16.653387 ]\n [12.57009  ]\n [13.491657 ]\n [20.290533 ]\n [15.466259 ]\n [24.641699 ]\n [10.652177 ]\n [21.881752 ]\n [17.052187 ]\n [14.531106 ]\n [13.354094 ]\n [19.515408 ]\n [15.746128 ]\n [18.183342 ]\n [12.627769 ]\n [15.058118 ]\n [18.13625  ]\n [14.064744 ]\n [11.721208 ]\n [16.664843 ]\n [13.25009  ]\n [17.512203 ]\n [20.088764 ]\n [14.549892 ]\n [22.866714 ]\n [17.112713 ]\n [12.457412 ]\n [11.750789 ]\n [12.223252 ]\n [ 8.291613 ]\n [19.171745 ]\n [ 4.2047873]\n [15.134145 ]\n [15.961994 ]\n [12.85464  ]\n [13.824977 ]\n [14.2187195]\n [14.859408 ]\n [17.473965 ]\n [12.319645 ]\n [16.517311 ]\n [15.8289995]\n [13.797858 ]\n [17.192759 ]\n [14.214728 ]\n [15.693098 ]\n [18.813503 ]\n [26.79604  ]\n [17.1876   ]\n [15.51295  ]\n [14.951222 ]\n [18.604994 ]\n [18.379608 ]\n [23.763699 ]\n [15.442278 ]\n [11.0755625]\n [21.971943 ]\n [13.706767 ]\n [19.907276 ]\n [19.18433  ]\n [17.979084 ]\n [19.45342  ]\n [15.740204 ]\n [16.384045 ]\n [13.106012 ]\n [20.996294 ]\n [17.392813 ]\n [19.452995 ]\n [16.048403 ]\n [17.868713 ]\n [16.338001 ]\n [24.91461  ]\n [15.790762 ]\n [14.77279  ]\n [14.947709 ]\n [17.38019  ]\n [16.586193 ]]", "q_t_selected": "[15.694581  15.99009   14.314847  15.762679  14.198464  22.462692\n 14.196552  20.689384  16.595034  13.841483  12.971687  13.215886\n 14.690366  20.61793   13.457447  13.794014  14.833035  18.381044\n 16.349337  15.715164  14.104779  20.260473  16.728693  14.720383\n 20.388105  20.103916  14.742732  17.456478  13.572561  14.358858\n 14.364457  18.161777  14.943561  15.857423  14.58351   15.857133\n 20.248621  15.025492  10.306296  14.387316  18.146278  15.587104\n 16.312977  12.762725  14.929726  14.347809  12.324356  13.723906\n 15.933876  14.250665  13.358992  12.308334  14.08665   18.61208\n 23.137917  17.7956    11.977509  14.841539  15.590159  19.983995\n 20.183037  16.401936  15.9013195 19.681168  17.151628  17.107893\n 14.425603  17.237185  15.5498705 15.513548  14.385102  15.770022\n 15.590065  18.574844  17.445343  15.04735   13.175399  16.325628\n 15.963936  15.472344  16.926064  16.330755  15.722382  18.731632\n 20.337503  17.645285  18.965649  14.548346  15.68292   14.483388\n 18.917591  19.58702   17.292023  14.607057  17.414007  15.757844\n 15.4964075 15.286757  18.81415   15.466165  11.396301  21.315578\n 20.257734  14.155696  12.812343  21.280481  16.108978  17.262342\n 15.301691  26.79604   15.40565   14.106922  13.835367  16.811111\n  9.2797785 21.07647   15.165762  21.235865   8.291613  17.92625\n 15.084022  19.638788  17.751389  15.313144  14.839274  14.661833\n 14.306232  14.020712  16.444578  16.859455  17.009424  15.114705\n 15.438598  18.901913  15.945697  19.18433   13.358992  21.966354\n 14.799687  17.59569   15.653414  12.372883  13.764148  17.899935\n 15.36584   12.160861  16.860537  14.830839  15.10475   18.192348\n 16.187193  16.458593  16.172096  14.903886  15.2198105 16.58568\n 14.9056015 17.08753   16.311      9.456581  19.88848   17.663593\n 15.276437  18.314     17.222721  14.331252  15.112589  16.79821\n 12.945095   8.084395  11.778913  17.255499  14.973724  17.104626\n 19.948645  15.324967  16.593014  14.99853   15.6465845 16.653387\n 12.57009   13.491657  20.290533  15.466259  24.641699  10.652177\n 21.881752  17.052187  14.531106  13.354094  19.515408  15.746128\n 18.183342  12.627769  15.058118  18.13625   14.064744  11.721208\n 16.664843  13.25009   17.512203  20.088764  14.549892  22.866714\n 17.112713  12.457412  11.750789  12.223252   8.291613  19.171745\n  4.2047873 15.134145  15.961994  12.85464   13.824977  14.2187195\n 14.859408  17.473965  12.319645  16.517311  15.8289995 13.797858\n 17.192759  14.214728  15.693098  18.813503  26.79604   17.1876\n 15.51295   14.951222  18.604994  18.379608  23.763699  15.442278\n 11.0755625 21.971943  13.706767  19.907276  19.18433   17.979084\n 19.45342   15.740204  16.384045  13.106012  20.996294  17.392813\n 19.452995  16.048403  17.868713  16.338001  24.91461   15.790762\n 14.77279   14.947709  17.38019   16.586193 ]", "twin_q_t_selected": "[15.430297  16.654408  14.887541  14.958012  14.864458  22.253757\n 13.917436  20.303513  16.039524  13.31168   13.06988   13.820485\n 13.662554  19.605188  14.530589  13.29628   14.642376  17.640368\n 15.8221855 15.197248  15.255629  20.066273  17.197027  14.260579\n 19.413122  18.711363  14.7531185 18.04796   13.916597  13.631276\n 14.111644  16.378256  15.495303  15.848993  14.677999  16.253502\n 21.31029   14.901138  11.08398   15.380766  18.164324  15.953289\n 16.253914  12.392905  15.150519  14.426253  12.144276  14.177108\n 16.131895  14.83336   13.474803  12.05281   13.474677  18.62023\n 22.457176  18.696215  12.03801   14.65299   14.631389  20.284912\n 21.106476  16.029491  15.893509  19.47838   17.896297  16.424358\n 14.139824  16.993605  15.9075775 16.211332  14.379855  15.669351\n 15.751657  18.056667  17.628094  15.996003  13.239492  17.259237\n 15.277249  15.609811  17.34024   15.635006  15.348605  20.126095\n 19.57989   16.81262   19.520426  13.79202   16.317715  14.784124\n 19.368284  20.106441  16.881157  13.826061  17.970547  16.114447\n 15.88214   15.527804  18.553215  15.586793  12.280693  22.064438\n 19.909737  14.712303  12.526419  20.334118  15.99739   17.457075\n 15.174441  26.051388  15.869634  14.584651  13.238175  16.517656\n  9.283765  21.121132  15.326617  20.636806   7.869206  18.014145\n 15.047684  18.672766  16.690994  16.59157   15.356421  14.361546\n 15.056539  13.970904  16.71563   16.471605  17.557852  15.407597\n 15.980183  19.107576  15.156775  19.814548  13.474803  22.441416\n 14.192552  17.145752  14.202875  12.310875  13.478221  16.752296\n 15.920688  12.441541  16.507168  14.604459  15.15437   18.604494\n 16.718203  16.708576  16.505922  14.792349  15.664688  16.099663\n 15.1269245 17.892565  15.2210655  8.733558  21.494678  17.491198\n 14.967991  18.8488    17.384064  14.293183  15.326178  18.313799\n 13.952871   8.613128  11.813008  17.541578  15.778032  17.101357\n 19.117264  15.483952  17.06822   16.20337   16.453987  16.801804\n 13.371162  12.450014  21.130337  14.735663  23.975977  11.855781\n 21.037563  15.080388  14.974609  13.632603  19.422598  16.774822\n 17.850266  12.687505  14.819945  18.64507   14.666595  12.400171\n 17.198664  12.844569  16.2629    19.820696  15.429774  23.369106\n 17.032124  11.915788  12.800519  12.7413025  7.869206  17.902542\n  4.450483  15.369626  17.806087  13.3715105 13.110907  14.1776495\n 15.672059  17.724396  12.395315  16.739672  15.147773  13.574538\n 16.615196  15.249227  16.1829    19.84771   26.051388  17.644236\n 14.718685  15.142463  19.361338  18.752687  24.04513   14.622145\n 11.022566  22.161198  13.551979  20.292875  19.814548  17.388947\n 19.24129   15.434677  16.684898  13.399557  21.22342   17.493961\n 19.582438  16.49774   17.56299   16.030441  24.743752  15.778252\n 15.093634  14.4075165 17.135134  16.418425 ]", "q_t_selected_target": "[15.156282  15.729396  12.6025305 16.402822  16.451826  21.840849\n 14.121486  21.968851  17.196325  13.824726  13.267016  12.793928\n 13.311772  20.020758  14.049387  13.171792  13.5361805 18.303158\n 16.42483   15.301481  13.519906  18.987652  17.016739  13.87698\n 19.102953  19.550066  15.93708   18.255451  15.277719  14.480928\n 13.846489  17.293005  16.509123  15.677733  13.270885  15.97813\n 20.821415  16.066547   9.862226  15.47885   17.853245  15.438563\n 16.825186  13.357121  14.075333  14.337707  13.620155  14.741878\n 15.6004    15.168188  13.7796755 12.55203   14.31101   17.99593\n 23.80971   18.574757  11.109965  14.218018  15.818315  21.302397\n 22.525589  16.645864  15.374591  21.297283  18.178442  17.103628\n 13.804489  16.74483   15.341921  16.730001  15.614451  15.753267\n 15.065749  18.683989  18.161438  16.597986  13.475441  15.718155\n 14.437303  15.293638  17.956913  15.2423935 14.803316  18.219898\n 21.308737  17.478615  20.199528  13.14776   14.1347065 14.253015\n 16.445179  19.418713  16.614862  15.196876  17.629545  15.781788\n 16.110844  15.007656  17.401497  16.77983   13.149605  21.711647\n 22.372845  14.520669  13.601428  21.256813  13.883753  15.416337\n 14.692829  28.38583   14.534741  13.352276  13.051585  16.647837\n 10.056     23.217142  15.982897  16.680504   7.3604074 17.355328\n 13.996995  19.808685  17.382217  17.168905  15.063084  13.051047\n 14.386408  13.832125  15.542208  17.297054  17.324976  15.752776\n 14.9240885 16.988302  15.785122  21.364994  13.850333  23.088894\n 15.318629  17.75086   16.804787  11.814608  12.053798  16.61689\n 16.686378   9.080858  16.534002  14.76082   15.92006   16.919178\n 16.63058   16.107891  14.724761  15.729479  15.775131  15.363959\n 14.68587   15.462119  16.21056    8.92114   20.344334  17.03151\n 16.658825  17.482168  18.491678  15.34261   15.060478  18.321894\n 12.95744    8.563356  12.6024475 18.339905  14.82672   18.103888\n 21.235111  14.283269  17.67591   13.420778  16.702223  15.266542\n 10.944179  12.385686  19.328949  14.976658  24.432127   9.110568\n 22.926039  15.574291  14.542007  12.709406  21.426834  13.656449\n 17.759903  12.79224   15.403729  19.135324  14.848711  12.011239\n 15.888195  11.926872  17.308239  18.709038  14.843032  22.75392\n 18.08759   12.552562  13.4517    10.881837   7.3252196 17.689999\n  3.4223566 15.239271  16.511396  13.394932  15.179394  15.2007475\n 14.577014  17.808636  11.205615  17.988457  15.473503  14.510655\n 15.506036  13.809673  16.44385   19.321348  28.229649  18.44957\n 14.449136  13.974852  20.13716   19.039087  25.058208  13.700104\n 11.660699  22.05343   12.460409  20.363512  21.107977  18.426146\n 17.94666   15.477411  14.566511  12.370148  21.870886  17.74262\n 21.445368  14.415617  16.700722  18.209866  23.442976  14.551975\n 16.242107  13.000516  17.613625  16.504963 ]", "q_tp1_best_masked": "[15.583821  16.220259  13.408463  17.220882  17.380337  22.551819\n 14.995243  21.761892  16.734793  14.418755  14.730627  14.82703\n 14.276038  20.301968  15.07396   14.136956  15.268596  17.923977\n 16.166101  15.936201  13.502296  18.448162  17.921412  14.060881\n 18.786318  20.467182  16.792408  17.791397  16.034967  15.670396\n 14.044915  16.377895  16.678614  17.326809  14.621237  15.78914\n 20.507172  17.610151  10.592141  16.196203  17.552187  15.73498\n 17.617216  14.769138  15.410751  15.0642185 14.617848  16.02154\n 15.7162075 15.356628  15.249311  13.256546  15.213191  17.38733\n 23.24019   18.684168  11.876231  16.453398  16.690924  21.17162\n 22.949966  17.448795  16.892803  21.902231  18.196283  18.047606\n 14.849206  16.866432  15.534     17.111975  16.093699  15.316845\n 14.752009  18.697903  18.055706  16.710865  14.349842  16.561304\n 14.485216  15.646384  17.57608   15.685484  15.848834  18.36223\n 21.016716  18.43412   21.137335  14.855585  15.723457  14.825101\n 16.189808  20.083237  16.556107  16.442125  17.684208  16.511192\n 16.475273  14.826959  17.322184  18.358397  15.126031  21.728779\n 21.579697  14.95455   14.596814  20.411213  14.000733  15.784774\n 15.102023  28.316841  15.474788  13.743299  14.100812  16.43067\n 12.140989  22.215813  16.357437  16.521122   9.113302  16.84577\n 14.975064  20.840992  17.146006  18.429243  15.802469  13.880827\n 15.431584  14.878037  15.590347  17.479637  17.003881  16.133188\n 16.135078  17.2865    16.470713  20.894588  15.3206835 23.063164\n 15.202755  17.435919  18.306108  12.446452  12.807728  17.192074\n 18.302769  10.944502  16.39779   15.065268  15.63183   16.814634\n 17.44025   16.083662  14.7425785 15.803585  16.775795  15.961696\n 16.181124  16.86635   16.575258  10.712064  20.551474  17.213776\n 15.942458  17.799385  19.126165  15.991173  16.011215  18.50803\n 13.699551  10.90694   13.990679  18.729286  15.81846   19.45601\n 20.887543  16.328243  17.533928  13.775962  17.62979   14.914963\n 12.159849  13.466421  19.136866  15.323911  23.470451  10.249618\n 22.553604  15.719568  15.023044  14.16928   20.848639  13.978207\n 18.38385   13.43726   16.292173  18.700838  15.699945  12.806102\n 16.679451  12.770339  17.362555  19.081263  14.975127  22.312742\n 18.903194  14.052217  13.787254  11.737704   9.077759  19.365877\n  5.7811775 15.601563  16.783794  13.899244  15.422256  16.042702\n 15.132506  18.446033  13.251713  18.107899  15.547025  15.91942\n 16.061228  13.565089  17.122095  18.944054  28.159082  17.989954\n 15.081216  14.578011  20.418299  19.741379  24.293442  13.934457\n 12.674769  22.134104  13.693616  21.553928  20.634975  19.272537\n 18.642183  15.302173  15.447508  14.190901  21.433264  17.827488\n 20.479439  14.680508  16.322292  18.658264  22.08096   14.435051\n 17.185741  13.17309   18.827196  17.182076 ]", "policy_t": "[[ 0.8097255  -0.7789515   0.54379046  0.49403286 -0.22739863 -0.6658532 ]\n [ 0.91156554 -0.41094965  0.7718997   0.13456404  0.7898557   0.9373281 ]\n [-0.61645406 -0.9271112  -0.57800406  0.70287526 -0.2745223  -0.6334751 ]\n ...\n [-0.7847189  -0.77473485  0.9849112   0.84733856 -0.0902524   0.71468496]\n [ 0.6281327   0.11567259  0.40446866  0.9057727   0.892699    0.08397651]\n [ 0.32184923  0.5676843  -0.5209307  -0.5982124  -0.638488   -0.1476261 ]]", "td_error": "[0.40615654 0.59285307 1.9986634  1.0424771  1.9203649  0.51737595\n 0.13955832 1.4724026  0.87904644 0.26490164 0.24623299 0.72425747\n 0.86468744 0.50637054 0.536571   0.3733549  1.2015252  0.37033844\n 0.3390684  0.25895786 1.1602979  1.1757212  0.2341671  0.6135011\n 0.7976608  0.69627666 1.1891551  0.503232   1.5331392  0.4858613\n 0.3915615  0.8917608  1.289691   0.17547464 1.359869   0.19818449\n 0.5308342  1.1032324  0.83291245 0.59480953 0.3020563  0.3316331\n 0.5417404  0.7793064  0.96478987 0.04932451 1.3858395  0.79137087\n 0.43248558 0.6261759  0.3627782  0.37145758 0.5303469  0.62022495\n 1.0121632  0.4503069  0.89779377 0.5292473  0.7075405  1.167943\n 1.8808327  0.430151   0.52282333 1.7175093  0.654479   0.3417673\n 0.47822428 0.3705654  0.38680315 0.86756134 1.2319727  0.05033588\n 0.6051116  0.36823273 0.6247196  1.0763097  0.26799536 1.0742779\n 1.18329    0.24743938 0.823761   0.7404871  0.73217726 1.2089653\n 1.3500404  0.41633224 0.9564905  1.0224223  1.8656111  0.3807416\n 2.6977587  0.42801857 0.47172737 0.98031664 0.27826977 0.17830133\n 0.42156982 0.39962482 1.2821856  1.2533522  1.3111076  0.3744297\n 2.2891092  0.27830362 0.93204737 0.47318172 2.1694312  1.9433718\n 0.54523706 1.9621162  1.1029005  0.9935107  0.4851861  0.14672756\n 0.7742281  2.1183414  0.7367072  4.2558317  0.72000194 0.6148701\n 1.0688577  0.6529083  0.53019714 1.2165484  0.25857353 1.4606419\n 0.37515306 0.16368341 1.0378962  0.6315241  0.2742138  0.4916253\n 0.7853017  2.0164423  0.39446068 1.8655548  0.43343592 0.8850088\n 0.82250977 0.38013935 1.8766422  0.5272713  1.5673866  0.70922565\n 1.0431147  3.2203426  0.17668438 0.11319017 0.79050016 1.4792433\n 0.26550484 0.4756937  1.6142483  0.8813615  0.33288193 0.9787121\n 0.33039284 2.0279284  0.54496765 0.3615117  0.80309963 0.54588604\n 1.5366111  1.0992317  1.1882858  1.0303926  0.15890503 0.76588917\n 0.50388813 0.26436615 0.8064871  0.9413662  0.5491581  1.0008965\n 1.702157   1.1211905  0.845294   2.1801715  0.651937   1.4610529\n 2.0264478  0.58514977 1.3814859  0.3652978  0.33286095 2.1434107\n 1.4663811  0.98589945 0.22175169 0.7839422  1.9578314  2.6040258\n 0.2569008  0.13460302 0.46469784 0.74466515 0.4830413  0.33948183\n 1.0435581  1.1204572  0.6246519  1.2456923  0.43994093 0.36399078\n 1.015171   0.36596203 1.1760464  1.6004405  0.75518966 0.8471451\n 0.90527844 0.11774063 0.92204666 0.28185654 1.711452   1.002563\n 0.68871975 0.20945549 1.151865   1.3599653  0.34061337 0.82445717\n 1.3979416  0.92230415 0.5058508  0.5171032  1.8059349  1.0336523\n 0.66668177 1.071991   1.1539936  0.4729395  1.1537943  1.3321075\n 0.61163473 0.09462738 1.1689639  0.26343632 1.6085377  0.7421303\n 1.4006958  0.15276337 1.9679604  0.882637   0.76102924 0.29923344\n 1.9276514  1.8574543  1.01513    2.0256443  1.3862047  1.2325315\n 1.3088956  1.6770968  0.35596275 0.08388424]", "mean_td_error": 0.883712887763977, "actor_loss": -16.752107620239258, "critic_loss": 0.6091270446777344, "alpha_loss": -17.225326538085938, "alpha_value": 0.1257324069738388, "target_entropy": -6, "mean_q": 16.13987922668457, "max_q": 26.796039581298828, "min_q": 4.204787254333496, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 17000, "episodes_total": 17, "training_iteration": 17, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_14-52-56", "timestamp": 1587048776, "time_this_iter_s": 86.19494795799255, "time_total_s": 631.8509368896484, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 631.8509368896484, "timesteps_since_restore": 17000, "iterations_since_restore": 17, "perf": {"cpu_util_percent": 92.26736111111111, "ram_util_percent": 11.400000000000002}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -195.0815642160215, "episode_reward_min": -252.98440159051196, "episode_reward_mean": -224.3773708283956, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-245.26213466994045, -212.5851270165044, -199.24433554714335, -214.75535258727226, -235.16977585192478, -234.9815134941209, -195.0815642160215, -219.00237693827543, -234.70712637224125, -252.98440159051196], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.25960308263687815, "mean_processing_ms": 0.11952939125705626, "mean_inference_ms": 1.1714049167796752}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -136.6155367814418, "episode_reward_min": -286.016112222877, "episode_reward_mean": -218.46125132247084, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-201.17740332023865, -136.6155367814418, -286.016112222877, -247.13998468717926, -221.35721960061747], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.3218246036619998, "mean_processing_ms": 0.44862406821971235, "mean_inference_ms": 1.5307215213289003}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 9000, "num_steps_trained": 2046720, "num_steps_sampled": 18000, "sample_time_ms": 2.999, "replay_time_ms": 16.98, "grad_time_ms": 48.776, "update_time_ms": 0.005, "opt_peak_throughput": 5248.462, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[12.327037 ]\n [16.370043 ]\n [15.7468405]\n [13.277105 ]\n [17.230124 ]\n [14.131524 ]\n [16.109352 ]\n [18.395777 ]\n [16.660826 ]\n [13.980832 ]\n [16.763868 ]\n [12.237456 ]\n [14.856078 ]\n [18.694895 ]\n [14.581854 ]\n [15.133108 ]\n [15.18372  ]\n [15.277654 ]\n [14.856697 ]\n [16.220518 ]\n [15.544839 ]\n [10.148393 ]\n [19.249538 ]\n [ 9.8757105]\n [ 9.951762 ]\n [16.505001 ]\n [15.182751 ]\n [13.326414 ]\n [15.053211 ]\n [19.535625 ]\n [18.380827 ]\n [ 7.2186184]\n [22.56424  ]\n [17.317238 ]\n [11.486292 ]\n [15.318201 ]\n [14.158419 ]\n [17.05072  ]\n [18.607603 ]\n [16.006744 ]\n [15.332158 ]\n [25.83179  ]\n [14.76973  ]\n [19.188725 ]\n [17.920527 ]\n [14.377439 ]\n [12.852512 ]\n [15.795122 ]\n [14.870093 ]\n [12.794985 ]\n [20.035013 ]\n [16.380384 ]\n [18.242239 ]\n [11.753974 ]\n [15.747172 ]\n [11.852687 ]\n [21.044708 ]\n [16.9876   ]\n [16.30278  ]\n [20.552664 ]\n [19.764349 ]\n [20.52532  ]\n [20.57607  ]\n [15.042839 ]\n [20.974472 ]\n [15.548622 ]\n [15.771856 ]\n [17.898392 ]\n [17.34889  ]\n [14.539881 ]\n [ 3.5038807]\n [18.224743 ]\n [15.329173 ]\n [18.64925  ]\n [15.629779 ]\n [15.749934 ]\n [20.672443 ]\n [15.803276 ]\n [15.559597 ]\n [15.447638 ]\n [14.615166 ]\n [17.196875 ]\n [ 6.9373536]\n [ 9.294629 ]\n [15.057105 ]\n [13.739957 ]\n [18.855755 ]\n [14.540779 ]\n [14.902671 ]\n [16.008444 ]\n [12.229444 ]\n [20.915756 ]\n [13.072508 ]\n [14.460283 ]\n [12.995296 ]\n [16.938251 ]\n [13.820764 ]\n [13.1369295]\n [13.119979 ]\n [26.913471 ]\n [15.693513 ]\n [22.076885 ]\n [15.642721 ]\n [15.127559 ]\n [14.372541 ]\n [12.163977 ]\n [14.954344 ]\n [15.3991375]\n [20.927279 ]\n [17.92138  ]\n [16.10681  ]\n [11.021794 ]\n [15.629041 ]\n [ 9.212214 ]\n [14.6087   ]\n [14.348051 ]\n [16.528896 ]\n [23.061638 ]\n [19.113256 ]\n [14.990132 ]\n [15.178591 ]\n [13.105709 ]\n [10.708948 ]\n [16.597084 ]\n [12.897076 ]\n [12.900727 ]\n [13.021303 ]\n [15.266065 ]\n [ 7.4019523]\n [13.913005 ]\n [11.278393 ]\n [14.615273 ]\n [19.535625 ]\n [15.258438 ]\n [17.132463 ]\n [15.157276 ]\n [13.820742 ]\n [15.952146 ]\n [13.230454 ]\n [19.065578 ]\n [15.262977 ]\n [ 9.1916275]\n [14.255757 ]\n [17.168365 ]\n [15.9996805]\n [18.158611 ]\n [11.289374 ]\n [15.722043 ]\n [ 5.6159186]\n [14.734053 ]\n [13.087964 ]\n [23.07951  ]\n [16.030752 ]\n [20.38714  ]\n [15.871264 ]\n [11.025616 ]\n [15.876869 ]\n [17.008844 ]\n [14.505018 ]\n [14.988058 ]\n [16.903505 ]\n [22.154713 ]\n [13.363959 ]\n [16.063692 ]\n [16.19765  ]\n [15.604043 ]\n [16.95571  ]\n [14.473987 ]\n [13.722499 ]\n [24.112354 ]\n [13.381346 ]\n [17.373913 ]\n [14.841787 ]\n [14.04422  ]\n [22.891375 ]\n [14.628896 ]\n [15.535868 ]\n [11.031054 ]\n [15.017916 ]\n [12.353758 ]\n [14.648582 ]\n [15.305698 ]\n [15.497716 ]\n [14.546615 ]\n [17.645273 ]\n [13.344077 ]\n [15.818905 ]\n [14.631927 ]\n [19.010529 ]\n [ 8.513151 ]\n [14.061833 ]\n [15.022891 ]\n [15.762328 ]\n [15.281532 ]\n [15.432195 ]\n [13.717382 ]\n [14.550568 ]\n [18.315838 ]\n [17.990047 ]\n [15.887647 ]\n [27.483753 ]\n [12.666079 ]\n [14.832546 ]\n [17.851583 ]\n [17.34889  ]\n [14.655332 ]\n [12.678813 ]\n [13.835176 ]\n [18.747927 ]\n [16.563541 ]\n [16.58549  ]\n [17.994843 ]\n [20.431578 ]\n [17.688852 ]\n [12.91259  ]\n [14.572486 ]\n [20.296654 ]\n [18.324903 ]\n [23.493238 ]\n [18.097107 ]\n [17.890888 ]\n [15.965905 ]\n [11.324862 ]\n [17.3686   ]\n [15.514568 ]\n [16.929361 ]\n [13.586005 ]\n [19.006725 ]\n [18.303919 ]\n [15.26001  ]\n [17.203117 ]\n [14.329838 ]\n [15.782495 ]\n [18.38058  ]\n [14.766568 ]\n [17.08513  ]\n [20.854385 ]\n [18.047857 ]\n [22.306929 ]\n [18.05105  ]\n [13.242554 ]\n [18.113123 ]\n [12.872219 ]\n [12.601997 ]\n [ 9.930403 ]\n [15.615111 ]\n [29.623789 ]\n [20.540678 ]\n [21.095064 ]\n [12.87105  ]\n [22.891375 ]\n [21.2847   ]\n [22.891989 ]\n [13.28888  ]\n [19.5344   ]\n [13.560814 ]]", "q_t_selected": "[12.327037  16.370043  15.7468405 13.277105  17.230124  14.131524\n 16.109352  18.395777  16.660826  13.980832  16.763868  12.237456\n 14.856078  18.694895  14.581854  15.133108  15.18372   15.277654\n 14.856697  16.220518  15.544839  10.148393  19.249538   9.8757105\n  9.951762  16.505001  15.182751  13.326414  15.053211  19.535625\n 18.380827   7.2186184 22.56424   17.317238  11.486292  15.318201\n 14.158419  17.05072   18.607603  16.006744  15.332158  25.83179\n 14.76973   19.188725  17.920527  14.377439  12.852512  15.795122\n 14.870093  12.794985  20.035013  16.380384  18.242239  11.753974\n 15.747172  11.852687  21.044708  16.9876    16.30278   20.552664\n 19.764349  20.52532   20.57607   15.042839  20.974472  15.548622\n 15.771856  17.898392  17.34889   14.539881   3.5038807 18.224743\n 15.329173  18.64925   15.629779  15.749934  20.672443  15.803276\n 15.559597  15.447638  14.615166  17.196875   6.9373536  9.294629\n 15.057105  13.739957  18.855755  14.540779  14.902671  16.008444\n 12.229444  20.915756  13.072508  14.460283  12.995296  16.938251\n 13.820764  13.1369295 13.119979  26.913471  15.693513  22.076885\n 15.642721  15.127559  14.372541  12.163977  14.954344  15.3991375\n 20.927279  17.92138   16.10681   11.021794  15.629041   9.212214\n 14.6087    14.348051  16.528896  23.061638  19.113256  14.990132\n 15.178591  13.105709  10.708948  16.597084  12.897076  12.900727\n 13.021303  15.266065   7.4019523 13.913005  11.278393  14.615273\n 19.535625  15.258438  17.132463  15.157276  13.820742  15.952146\n 13.230454  19.065578  15.262977   9.1916275 14.255757  17.168365\n 15.9996805 18.158611  11.289374  15.722043   5.6159186 14.734053\n 13.087964  23.07951   16.030752  20.38714   15.871264  11.025616\n 15.876869  17.008844  14.505018  14.988058  16.903505  22.154713\n 13.363959  16.063692  16.19765   15.604043  16.95571   14.473987\n 13.722499  24.112354  13.381346  17.373913  14.841787  14.04422\n 22.891375  14.628896  15.535868  11.031054  15.017916  12.353758\n 14.648582  15.305698  15.497716  14.546615  17.645273  13.344077\n 15.818905  14.631927  19.010529   8.513151  14.061833  15.022891\n 15.762328  15.281532  15.432195  13.717382  14.550568  18.315838\n 17.990047  15.887647  27.483753  12.666079  14.832546  17.851583\n 17.34889   14.655332  12.678813  13.835176  18.747927  16.563541\n 16.58549   17.994843  20.431578  17.688852  12.91259   14.572486\n 20.296654  18.324903  23.493238  18.097107  17.890888  15.965905\n 11.324862  17.3686    15.514568  16.929361  13.586005  19.006725\n 18.303919  15.26001   17.203117  14.329838  15.782495  18.38058\n 14.766568  17.08513   20.854385  18.047857  22.306929  18.05105\n 13.242554  18.113123  12.872219  12.601997   9.930403  15.615111\n 29.623789  20.540678  21.095064  12.87105   22.891375  21.2847\n 22.891989  13.28888   19.5344    13.560814 ]", "twin_q_t_selected": "[13.083087  17.237589  14.6919775 14.134639  17.170023  13.759422\n 16.463696  19.439034  15.322022  14.57894   17.024567  12.200944\n 14.279785  19.913488  13.823738  16.847055  16.072355  14.344769\n 14.237715  17.030241  15.871214   8.691545  19.851492  11.18559\n  9.648938  15.540699  14.946384  13.959235  15.335398  19.223446\n 17.605982   7.074677  21.949368  17.144608  11.419351  15.735687\n 14.831248  16.923504  18.86892   15.784122  14.852023  25.898808\n 14.656944  20.757685  17.403168  14.853133  12.861758  15.367053\n 14.390515  12.920094  19.790672  16.252811  18.987608  11.2589245\n 16.35669   12.516376  20.887663  17.45626   16.815016  19.833448\n 19.793633  20.281263  20.393887  15.126496  20.641466  14.877387\n 14.9650135 17.985928  18.092817  14.341989   3.5075057 17.800907\n 14.827518  19.349867  14.724813  16.532778  20.708128  16.104174\n 17.050644  14.205476  13.868322  17.740088   6.9105344 10.358678\n 14.075893  14.105476  18.403606  13.892076  15.238333  16.717829\n 10.739502  22.334955  13.506012  14.592447  13.371755  15.885994\n 13.638123  13.168978  13.769192  26.89914   16.812351  22.849703\n 14.786483  15.610571  14.255449  12.571047  13.674503  15.142738\n 20.316177  17.56473   16.904768  10.734221  14.601809   9.165104\n 15.227321  13.936242  14.759695  22.242159  19.011477  14.6933155\n 15.311641  12.710856   9.818456  16.68505   13.126134  13.362255\n 13.647274  14.916878   8.074804  14.254689  10.517913  14.689936\n 19.223446  15.196237  16.980297  15.20761   13.705369  15.27928\n 14.650779  18.446136  15.353215   8.7617445 14.2422285 16.871374\n 17.09469   17.782759  11.099754  14.613986   5.3808956 14.431034\n 12.886407  23.859785  15.641508  21.280403  15.231138  11.079397\n 16.418089  17.373816  14.365168  15.521364  17.059624  21.676903\n 13.5137415 16.812489  15.25646   15.419885  16.52065   13.362582\n 14.091143  23.834967  14.626081  16.89082   14.362071  13.770139\n 23.279995  14.615772  16.251406  11.385487  13.947607  11.506223\n 14.744036  15.040336  15.355837  13.846193  17.903168  13.805511\n 16.868042  14.06684   18.928751   8.729288  13.855113  15.467028\n 17.013859  14.785662  15.0610285 13.567167  14.690319  19.042696\n 17.50927   16.038986  27.510912  14.036902  14.046331  17.618927\n 18.092817  14.692369  13.636307  13.582237  18.307924  16.608662\n 16.66768   18.072939  19.685545  17.230097  13.039725  14.632228\n 20.94043   17.738644  21.986883  18.409382  18.040043  16.741276\n 12.309139  18.2579    15.637572  16.664705  13.767134  19.525436\n 18.383558  14.423252  17.030268  13.808094  15.364871  18.130949\n 15.049216  17.00418   20.11333   18.173897  22.530848  17.489857\n 14.212738  18.372742  12.677002  13.821383   9.804876  14.892481\n 28.56027   21.099056  21.28685   12.431334  23.279995  22.51101\n 22.042467  14.62721   19.411236  12.35552  ]", "q_t_selected_target": "[13.610253  15.808333  14.616494  13.692618  19.043747  14.581336\n 19.329752  20.010887  15.957697  15.565063  15.73862   12.168974\n 14.243766  20.986666  14.852342  17.3469    14.790179  13.57549\n 14.086906  15.630847  15.35952    8.904926  20.082378   9.482194\n  9.474944  16.387173  15.508326  14.929882  14.787715  20.211523\n 13.331563   5.828123  21.851217  17.735666  12.983945  14.335446\n 13.233272  15.59111   20.898283  15.366913  17.385004  25.463156\n 15.992615  20.774073  18.973228  15.117741  13.411877  17.100718\n 13.928664  12.327052  21.358109  17.053143  19.55057   11.974268\n 15.039426  13.059774  22.821054  16.538443  15.429189  22.95471\n 18.598543  20.537422  20.995892  14.684095  19.123531  16.33693\n 15.7385235 19.591715  15.740855  15.050588   1.8167018 17.450428\n 16.800854  16.642138  14.347318  16.127275  19.934814  15.206729\n 17.33295   14.305111  13.623897  17.54349    7.3496013  7.4567776\n 14.321817  15.327041  17.249924  14.305996  13.486258  17.431131\n 11.451354  21.224596  13.361905  16.330988  13.159465  15.045752\n 12.857837  13.309492  14.785861  27.242487  14.902492  21.079002\n 13.631896  15.814582  14.189523  10.428324  12.991046  16.531578\n 19.549633  19.403461  15.441425  10.197005  14.960759   9.057493\n 14.901667  14.772314  15.21495   21.797285  19.684975  12.373878\n 16.70016   12.714317  10.463444  17.713316  13.706456  14.26487\n 11.651043  14.824075   6.465003  13.432941  10.372119  15.266645\n 20.509394  13.616426  17.126747  13.563204  10.054503  14.428554\n 12.206232  18.43196   14.504659   7.3112707 14.681311  15.336504\n 18.698332  18.112432   8.592175  15.777814   4.9985113 14.049566\n 11.316512  23.467968  15.752358  22.340416  13.902745  13.176966\n 15.12684   18.415249  14.055807  14.955263  16.825165  23.996416\n 12.308424  17.432205  16.4892    14.507261  16.16025   11.828135\n 13.701397  24.609161  14.488826  15.435173  14.588478  13.529118\n 24.83907   14.385287  16.147457  12.534602  14.908574  11.919246\n 14.766219  13.055933  15.928283  15.358619  18.859041  12.150207\n 16.587484  13.492053  18.137491   7.651343  13.385104  15.278881\n 15.373115  14.728936  14.689938  14.961137  13.981193  17.486902\n 19.038126  15.304242  28.912388  12.973549  14.549947  18.477808\n 16.005453  15.160361  14.624503  12.484528  16.719995  14.2005625\n 17.2097    18.455736  17.397535  16.541853  13.824796  14.845908\n 22.044481  17.170898  21.130096  19.02469   16.627443  17.115286\n  9.949891  16.880102  15.959441  17.207167  13.698009  17.700693\n 20.402834  18.034365  19.852043  14.770697  14.163069  15.930995\n 15.00638   16.55404   20.823072  17.514544  20.6533    14.948323\n 13.1046915 18.38243   11.584009  14.253965   8.645475  13.424631\n 27.31732   19.79341   19.860672  12.133748  24.60184   20.782791\n 22.197573  13.420186  21.70426   10.981604 ]", "q_tp1_best_masked": "[14.335931  16.240774  14.5537815 14.128123  19.300835  16.076239\n 20.25692   19.805124  17.026758  16.541096  16.67088   13.440187\n 14.140106  20.527914  16.233099  18.780785  15.104842  14.257528\n 14.979861  14.937522  15.714968   9.950986  20.833498  11.234219\n 11.191035  16.07888   15.396964  15.971043  15.135351  20.037775\n 13.431098   8.140475  21.643484  17.801613  14.563301  15.020665\n 14.279226  16.851381  21.291758  15.140356  18.609928  24.611671\n 16.553686  21.064674  19.213144  15.5803385 13.334478  17.754305\n 15.130056  13.836909  20.813492  16.688295  19.462576  13.396887\n 15.8619175 14.373589  23.836266  16.144243  16.036192  23.512568\n 17.419132  21.373943  22.055422  14.983599  19.499943  16.09761\n 15.537976  19.595354  15.476512  15.2208805  3.7603302 16.665424\n 15.434369  17.231422  15.618955  16.496408  19.848902  15.896304\n 16.94759   14.664874  13.417099  18.22565    9.818624   8.59101\n 15.562912  14.752663  18.414028  14.721476  13.229866  18.08635\n 11.972569  21.16516   13.422934  16.919691  15.025187  14.918162\n 12.556929  13.757905  15.364182  27.649689  15.156855  20.652029\n 14.595506  15.734764  15.048305  11.283116  15.061035  16.886368\n 18.764696  18.335436  15.74382   11.591431  15.39601    9.478053\n 15.541845  16.330563  15.458816  21.459337  19.193663  12.925876\n 17.219723  13.023747  11.303172  18.104267  15.745048  14.965293\n 12.399691  15.228058   8.648344  14.141566  10.525981  16.004847\n 20.338654  13.68794   17.922523  13.817872  11.363302  14.7201605\n 13.467346  18.687468  15.329816   8.951821  15.326478  15.690976\n 19.227505  18.266356   9.792199  16.553978   7.245864  14.666625\n 12.062995  22.29767   16.945402  22.291552  13.654608  14.69201\n 15.726393  18.708904  14.711325  16.00935   16.4682    24.13266\n 13.630363  18.51977   17.83051   15.769706  16.806402  12.585208\n 14.741394  24.765059  15.030212  15.551995  15.341178  14.663168\n 24.268885  14.083612  16.270813  13.922148  15.302459  13.180284\n 16.324377  13.289457  17.081003  15.635863  19.365688  13.786215\n 16.926653  14.072963  17.861467   9.831357  15.122155  15.432018\n 15.808058  14.693024  14.726396  15.352999  14.111079  17.19875\n 19.632547  15.547523  26.958132  14.816966  14.730813  19.206825\n 15.743781  16.847893  15.446278  13.3436    16.838348  14.4159565\n 17.122482  17.411882  18.093613  17.167912  14.966702  15.253869\n 22.472086  17.494383  20.719746  18.788704  16.367712  16.749193\n 11.998256  17.004848  15.517547  16.498802  15.196406  17.579992\n 19.953596  18.759575  19.439564  14.230355  14.502675  16.726027\n 15.252417  16.355835  20.60666   18.240065  20.279104  15.176987\n 13.60375   18.115658  12.631251  15.733034   9.70291   14.30496\n 26.476244  19.740847  18.996592  13.047583  24.029257  20.579721\n 21.166328  13.830441  20.656721  12.797046 ]", "policy_t": "[[-0.36805177 -0.7866663   0.33630848  0.50106287  0.9987719   0.7885114 ]\n [ 0.77557135 -0.33734977  0.07366896  0.89245856  0.73424673 -0.47155964]\n [ 0.17824078 -0.08205903  0.13638997  0.12999499  0.6693655   0.9324814 ]\n ...\n [-0.46701437  0.2461896   0.9920641  -0.50650114  0.16849089  0.00665557]\n [-0.7183938   0.4268576   0.00533617  0.9513508  -0.8842868  -0.8096634 ]\n [ 0.97240794  0.9233397   0.5557163   0.70822453  0.40549743  0.1854372 ]]", "td_error": "[0.9051914  0.99548244 0.6029148  0.42876673 1.8436737  0.6358628\n 3.0432281  1.093482   0.66940165 1.2851772  1.1555977  0.05022621\n 0.32416582 1.6824741  0.64954567 1.3568192  0.8378582  1.2357211\n 0.4602995  0.9945326  0.34850645 0.7284241  0.5318632  1.0484562\n 0.32540607 0.48215103 0.443758   1.2870574  0.4065895  0.8319874\n 4.6618414  1.3185246  0.40558624 0.5047436  1.5311236  1.1914978\n 1.2615619  1.3960018  2.1600218  0.5285206  2.2929134  0.40214348\n 1.2792778  0.80086803 1.3113813  0.50245476 0.5547414  1.51963\n 0.7016401  0.53048706 1.4452658  0.7365446  0.93564606 0.46781874\n 1.012505   0.8752432  1.8548689  0.6834879  1.1297092  2.761654\n 1.1804476  0.13413048 0.5109129  0.4005723  1.6844378  1.1239247\n 0.4034214  1.6495552  1.9799986  0.609653   1.6889914  0.562397\n 1.722508   2.357421   0.82997847 0.3914218  0.7554712  0.7469959\n 1.0278301  0.6210809  0.61784744 0.27160645 0.42565727 2.369876\n 0.49060583 1.404324   1.3797569  0.3243518  1.5842443  1.0679951\n 0.7449708  0.7095995  0.21675205 1.8046227  0.18822956 1.3663712\n 0.87160635 0.15653849 1.3412757  0.33618164 1.3504405  1.3842916\n 1.582706   0.44551706 0.12447262 1.939188   1.3233776  1.2606401\n 1.0720949  1.6604071  1.0643635  0.6810026  0.5136161  0.13116598\n 0.30931044 0.6301675  0.88460064 0.8546133  0.6226082  2.4678464\n 1.4550433  0.19742632 0.44524622 1.0722494  0.6948514  1.1333785\n 1.6832457  0.26739645 1.2733753  0.6509056  0.5260339  0.61404085\n 1.129858   1.6109109  0.07608318 1.6192393  3.708552   1.1871591\n 1.7343845  0.32389832 0.80343723 1.6654153  0.43231773 1.6833658\n 2.1511464  0.18792629 2.6023898  0.6097994  0.4998958  0.5329771\n 1.6706734  0.39013767 0.19462204 1.5066442  1.6484561  2.1244593\n 1.0206394  1.2239189  0.3792858  0.299448   0.15639973 2.0806084\n 1.1304264  0.9941149  0.76214504 1.0047026  0.57793045 2.0901499\n 0.20542383 0.6355009  0.62236786 1.6971931  0.23985815 0.37806177\n 1.7533846  0.23704672 0.357769   1.3263321  0.53515434 0.42376757\n 0.06991005 2.117084   0.5015063  1.1622148  1.0848207  1.4245877\n 0.52456856 0.8573303  0.83214855 0.96987677 0.573369   0.22206831\n 1.0149789  0.3046608  0.556674   1.318862   0.63925076 1.1923647\n 1.2884674  0.6590743  1.4150553  0.68541193 0.3931074  0.74255276\n 1.7154007  0.48651075 1.4669433  1.2241793  1.80793    2.385539\n 0.5831146  0.42184544 2.661026   0.9176216  0.84863806 0.24355125\n 1.4259396  0.8608751  1.6099644  0.7714462  1.3380222  0.7616954\n 1.8671093  0.9331484  0.38337088 0.41013336 0.09056425 1.5653877\n 2.0590954  3.1927338  2.7353506  0.7017307  1.410614   2.32477\n 0.14132404 0.4906149  0.37052727 0.5963335  1.7655888  2.8221302\n 0.62295437 0.1394968  1.1906013  1.042275   1.2221642  1.829165\n 1.7747097  1.0264578  1.3302851  0.51744366 1.5161543  1.1150637\n 0.42476082 0.66916466 2.2314415  1.9765635 ]", "mean_td_error": 1.041430950164795, "actor_loss": -16.395986557006836, "critic_loss": 0.8312416672706604, "alpha_loss": -17.874839782714844, "alpha_value": 0.09536632895469666, "target_entropy": -6, "mean_q": 15.895597457885742, "max_q": 29.623788833618164, "min_q": 3.503880739212036, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 18000, "episodes_total": 18, "training_iteration": 18, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_14-54-38", "timestamp": 1587048878, "time_this_iter_s": 86.504323720932, "time_total_s": 718.3552606105804, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 718.3552606105804, "timesteps_since_restore": 18000, "iterations_since_restore": 18, "perf": {"cpu_util_percent": 92.36620689655172, "ram_util_percent": 11.400000000000004}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -122.57195501513026, "episode_reward_min": -197.27206323126262, "episode_reward_mean": -161.62024023945594, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-168.3127202810995, -186.89900215583162, -122.57195501513026, -175.5015193033528, -129.8645292408784, -126.89015868142866, -163.13253008120802, -186.1859298772641, -159.5719945271033, -197.27206323126262], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26013667306332006, "mean_processing_ms": 0.11964462621220709, "mean_inference_ms": 1.174501025392812}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -142.76929303405268, "episode_reward_min": -286.016112222877, "episode_reward_mean": -219.69200257299298, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-142.76929303405268, -286.016112222877, -247.13998468717926, -221.35721960061747, -201.17740332023865], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.32585162141044527, "mean_processing_ms": 0.4532877089105075, "mean_inference_ms": 1.5605615659324212}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 9500, "num_steps_trained": 2302720, "num_steps_sampled": 19000, "sample_time_ms": 3.075, "replay_time_ms": 25.576, "grad_time_ms": 53.101, "update_time_ms": 0.005, "opt_peak_throughput": 4820.965, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[16.25642  ]\n [14.105185 ]\n [13.1784   ]\n [13.522843 ]\n [12.620496 ]\n [13.394448 ]\n [14.387615 ]\n [14.477642 ]\n [10.078371 ]\n [12.356803 ]\n [14.967194 ]\n [14.772843 ]\n [13.312571 ]\n [15.008466 ]\n [17.186216 ]\n [12.880978 ]\n [14.718035 ]\n [15.68236  ]\n [15.635672 ]\n [17.019024 ]\n [13.860893 ]\n [15.119754 ]\n [13.103295 ]\n [12.964832 ]\n [17.070938 ]\n [13.119051 ]\n [13.132166 ]\n [14.846264 ]\n [18.106962 ]\n [15.284573 ]\n [13.980055 ]\n [14.533065 ]\n [14.727464 ]\n [15.459601 ]\n [11.815386 ]\n [13.111084 ]\n [13.03066  ]\n [16.471304 ]\n [14.424362 ]\n [12.335604 ]\n [11.244779 ]\n [13.793731 ]\n [16.282913 ]\n [18.739466 ]\n [16.728298 ]\n [16.29301  ]\n [15.627032 ]\n [14.063084 ]\n [14.387623 ]\n [18.368027 ]\n [14.855825 ]\n [11.186842 ]\n [13.003455 ]\n [15.109659 ]\n [20.872648 ]\n [18.584532 ]\n [14.285432 ]\n [12.379071 ]\n [15.803488 ]\n [ 8.7597475]\n [18.626259 ]\n [17.169353 ]\n [11.100618 ]\n [16.424652 ]\n [16.664196 ]\n [21.89086  ]\n [10.424007 ]\n [13.52992  ]\n [22.038445 ]\n [14.780288 ]\n [12.966468 ]\n [12.639645 ]\n [13.345813 ]\n [12.980277 ]\n [15.404328 ]\n [16.573034 ]\n [10.39832  ]\n [14.627172 ]\n [24.753326 ]\n [13.485126 ]\n [22.410433 ]\n [13.123506 ]\n [11.26436  ]\n [23.011572 ]\n [16.518347 ]\n [13.967982 ]\n [16.904486 ]\n [14.360552 ]\n [20.064142 ]\n [15.479044 ]\n [15.027597 ]\n [16.006516 ]\n [12.39381  ]\n [16.215034 ]\n [ 9.314459 ]\n [12.253189 ]\n [ 9.953859 ]\n [21.782833 ]\n [13.965078 ]\n [13.460302 ]\n [13.931805 ]\n [15.494343 ]\n [14.610162 ]\n [15.211682 ]\n [14.935242 ]\n [11.543209 ]\n [19.815594 ]\n [15.107473 ]\n [13.585119 ]\n [16.712187 ]\n [18.886095 ]\n [12.369984 ]\n [12.684213 ]\n [14.516542 ]\n [16.467943 ]\n [14.881986 ]\n [15.126591 ]\n [14.845473 ]\n [13.206691 ]\n [18.761728 ]\n [14.062096 ]\n [18.331503 ]\n [16.481384 ]\n [20.144533 ]\n [13.657622 ]\n [14.017899 ]\n [16.900202 ]\n [13.794574 ]\n [12.508991 ]\n [14.249567 ]\n [13.833307 ]\n [14.227322 ]\n [16.995676 ]\n [21.365168 ]\n [17.547586 ]\n [13.730126 ]\n [17.14057  ]\n [22.95902  ]\n [13.637658 ]\n [14.0665865]\n [16.539991 ]\n [19.508404 ]\n [11.971416 ]\n [17.020054 ]\n [15.176872 ]\n [14.948403 ]\n [18.058746 ]\n [24.074879 ]\n [10.5440035]\n [23.509047 ]\n [12.941128 ]\n [12.901778 ]\n [14.072653 ]\n [11.254467 ]\n [17.640388 ]\n [16.33296  ]\n [15.464834 ]\n [14.047557 ]\n [17.32625  ]\n [13.470158 ]\n [ 5.47106  ]\n [13.025004 ]\n [14.666293 ]\n [12.681033 ]\n [10.144863 ]\n [13.938454 ]\n [19.011625 ]\n [11.317633 ]\n [12.320836 ]\n [15.770952 ]\n [16.02216  ]\n [15.492056 ]\n [12.980277 ]\n [20.50107  ]\n [15.36905  ]\n [13.025483 ]\n [14.257191 ]\n [23.697033 ]\n [17.403475 ]\n [15.648771 ]\n [16.019442 ]\n [13.527681 ]\n [13.591051 ]\n [16.286976 ]\n [15.222967 ]\n [15.221513 ]\n [11.978075 ]\n [13.7826395]\n [12.298938 ]\n [22.757765 ]\n [17.795004 ]\n [16.454409 ]\n [12.060937 ]\n [13.0541   ]\n [18.14234  ]\n [19.444538 ]\n [13.933773 ]\n [15.07334  ]\n [13.111884 ]\n [19.345184 ]\n [ 9.586322 ]\n [21.274855 ]\n [14.019268 ]\n [14.468035 ]\n [12.008976 ]\n [19.857847 ]\n [17.423931 ]\n [13.784986 ]\n [15.363981 ]\n [19.526257 ]\n [15.488163 ]\n [13.311003 ]\n [15.370376 ]\n [13.918301 ]\n [11.309878 ]\n [13.295762 ]\n [29.946527 ]\n [23.769876 ]\n [12.8127   ]\n [14.2864895]\n [14.204075 ]\n [14.248375 ]\n [12.303172 ]\n [15.3983345]\n [14.399299 ]\n [14.930748 ]\n [19.104174 ]\n [14.653507 ]\n [18.308758 ]\n [14.476113 ]\n [12.64464  ]\n [13.322982 ]\n [14.750348 ]\n [22.986454 ]\n [11.712236 ]\n [14.443051 ]\n [13.311003 ]\n [15.879404 ]\n [15.024757 ]\n [12.524091 ]\n [16.025301 ]\n [19.326466 ]\n [14.945102 ]\n [14.387809 ]\n [18.74928  ]\n [14.133839 ]\n [15.568602 ]\n [14.569989 ]\n [12.976251 ]\n [13.686752 ]\n [13.000121 ]\n [15.350127 ]\n [14.023847 ]\n [ 9.779056 ]\n [12.944135 ]\n [13.644215 ]]", "q_t_selected": "[16.25642   14.105185  13.1784    13.522843  12.620496  13.394448\n 14.387615  14.477642  10.078371  12.356803  14.967194  14.772843\n 13.312571  15.008466  17.186216  12.880978  14.718035  15.68236\n 15.635672  17.019024  13.860893  15.119754  13.103295  12.964832\n 17.070938  13.119051  13.132166  14.846264  18.106962  15.284573\n 13.980055  14.533065  14.727464  15.459601  11.815386  13.111084\n 13.03066   16.471304  14.424362  12.335604  11.244779  13.793731\n 16.282913  18.739466  16.728298  16.29301   15.627032  14.063084\n 14.387623  18.368027  14.855825  11.186842  13.003455  15.109659\n 20.872648  18.584532  14.285432  12.379071  15.803488   8.7597475\n 18.626259  17.169353  11.100618  16.424652  16.664196  21.89086\n 10.424007  13.52992   22.038445  14.780288  12.966468  12.639645\n 13.345813  12.980277  15.404328  16.573034  10.39832   14.627172\n 24.753326  13.485126  22.410433  13.123506  11.26436   23.011572\n 16.518347  13.967982  16.904486  14.360552  20.064142  15.479044\n 15.027597  16.006516  12.39381   16.215034   9.314459  12.253189\n  9.953859  21.782833  13.965078  13.460302  13.931805  15.494343\n 14.610162  15.211682  14.935242  11.543209  19.815594  15.107473\n 13.585119  16.712187  18.886095  12.369984  12.684213  14.516542\n 16.467943  14.881986  15.126591  14.845473  13.206691  18.761728\n 14.062096  18.331503  16.481384  20.144533  13.657622  14.017899\n 16.900202  13.794574  12.508991  14.249567  13.833307  14.227322\n 16.995676  21.365168  17.547586  13.730126  17.14057   22.95902\n 13.637658  14.0665865 16.539991  19.508404  11.971416  17.020054\n 15.176872  14.948403  18.058746  24.074879  10.5440035 23.509047\n 12.941128  12.901778  14.072653  11.254467  17.640388  16.33296\n 15.464834  14.047557  17.32625   13.470158   5.47106   13.025004\n 14.666293  12.681033  10.144863  13.938454  19.011625  11.317633\n 12.320836  15.770952  16.02216   15.492056  12.980277  20.50107\n 15.36905   13.025483  14.257191  23.697033  17.403475  15.648771\n 16.019442  13.527681  13.591051  16.286976  15.222967  15.221513\n 11.978075  13.7826395 12.298938  22.757765  17.795004  16.454409\n 12.060937  13.0541    18.14234   19.444538  13.933773  15.07334\n 13.111884  19.345184   9.586322  21.274855  14.019268  14.468035\n 12.008976  19.857847  17.423931  13.784986  15.363981  19.526257\n 15.488163  13.311003  15.370376  13.918301  11.309878  13.295762\n 29.946527  23.769876  12.8127    14.2864895 14.204075  14.248375\n 12.303172  15.3983345 14.399299  14.930748  19.104174  14.653507\n 18.308758  14.476113  12.64464   13.322982  14.750348  22.986454\n 11.712236  14.443051  13.311003  15.879404  15.024757  12.524091\n 16.025301  19.326466  14.945102  14.387809  18.74928   14.133839\n 15.568602  14.569989  12.976251  13.686752  13.000121  15.350127\n 14.023847   9.779056  12.944135  13.644215 ]", "twin_q_t_selected": "[15.767252  14.350292  12.420548  13.520594  13.099509  13.756248\n 14.962944  14.375942   9.886036  12.136508  15.859975  14.691284\n 13.45833   15.314327  17.582752  12.684584  14.276215  15.945499\n 14.279254  17.092073  13.752726  14.315542  12.777429  12.540773\n 16.427094  13.170295  12.977144  14.0041685 19.409063  15.753377\n 13.986701  14.695806  14.715672  15.649511  12.371945  13.585056\n 14.040139  16.468197  14.606666  12.263642  10.163006  14.106003\n 15.842996  19.004154  16.136261  16.615042  14.951722  13.8026495\n 13.888581  17.47904   14.327171  10.284461  12.7863865 15.588642\n 21.783562  18.536995  13.719595  12.337235  16.711983  10.065883\n 18.595484  16.632359  10.317564  17.163769  17.13693   22.888716\n  9.986894  12.989187  21.410582  14.584408  12.839197  12.764924\n 14.217379  13.290547  14.350962  16.967789  10.242646  13.7394495\n 24.897184  13.502159  22.794136  13.64829   11.527058  23.362995\n 17.116474  14.53296   17.308804  14.334418  20.215237  15.549963\n 14.192518  16.633753  11.953099  15.895868   9.344998  12.294236\n 11.6111555 21.596046  13.233383  13.199926  13.198821  16.452442\n 14.2447195 14.29697   14.704112  11.617844  19.507065  15.112819\n 12.913872  16.880913  19.521544  12.651148  13.237356  14.063725\n 15.928324  15.2826805 14.388265  14.518787  13.719723  17.754406\n 14.620902  19.273834  15.517089  18.497114  13.806     13.758821\n 17.088842  13.785525  13.624788  13.514314  13.678896  14.539165\n 17.811432  21.799616  17.868052  13.104754  18.081127  24.329535\n 12.767307  14.001551  15.491572  20.450758  12.276737  18.102158\n 15.43411   15.049722  17.912664  24.580645  10.486332  24.546684\n 13.182438  13.311463  14.087029  12.179678  17.39645   16.66904\n 15.713692  14.269058  18.199131  13.088441   6.032248  12.615964\n 14.543934  13.033129   9.718677  13.626726  17.858578  11.075137\n 12.789845  16.251146  16.587893  14.812029  13.290547  18.915926\n 15.3956175 13.020294  12.483585  23.53141   16.661112  14.864876\n 16.295324  15.113538  14.220515  16.725828  14.918029  15.030815\n 13.098778  13.698638  13.265792  23.001945  17.611578  15.655846\n 13.955112  13.23847   19.13184   18.52585   13.31514   14.448195\n 13.381645  17.964697   9.881992  22.41837   13.567925  14.0625\n 11.730121  19.508867  18.310995  13.539739  15.26391   18.647812\n 15.609209  13.254161  15.459221  14.246451  12.362103  12.272383\n 29.529806  24.533392  13.015496  14.472903  13.886857  13.301894\n 12.324122  14.88792   13.899145  14.539363  17.40143   13.953068\n 18.100168  14.201298  13.461963  14.056727  14.254263  23.04144\n 10.943282  14.944864  13.254161  15.341477  15.127542  12.803242\n 16.74617   18.5154    14.112256  14.505758  18.35879   13.482901\n 15.379072  14.25314   13.209903  13.556603  12.673201  15.580552\n 13.814695   9.883063  13.69886   13.276498 ]", "q_t_selected_target": "[16.849638  13.915512  14.179519  12.92154   12.720944  14.492006\n 14.423954  14.344127   9.053972  11.306722  15.876731  14.299526\n 14.498866  15.538624  17.743269  10.856838  14.258832  14.349892\n 14.592715  17.86935   14.892372  12.455814  13.939002  13.238099\n 18.929115  14.578144  13.642159  14.613185  19.038671  14.089598\n 15.160033  15.679712  16.243692  14.467333  12.643491  12.56263\n 10.708353  14.784494  15.111643  13.8449     8.935651  14.675286\n 16.48997   19.38719   15.375587  16.724371  15.366541  14.198705\n 14.072435  19.172712  13.309452  10.528994  11.891753  14.296301\n 20.943521  17.419851  14.289871  13.713872  16.703167   8.577156\n 17.424622  15.082529  11.052127  18.249203  16.413286  21.118467\n  9.251959  12.30863   20.664505  13.680846  13.894028  12.031819\n 13.712772  12.287632  14.508092  14.770548   8.968101  13.212513\n 27.590364  12.27892   23.789057  12.589689  11.768102  21.423416\n 17.995502  14.105959  18.085629  14.0263405 19.198545  13.165575\n 14.14623   16.978024  13.106743  16.739145   8.388046  13.276346\n  9.460582  22.251442  14.793857  14.467773  15.307114  17.589388\n 14.5111475 14.279486  14.932559  12.437253  18.2564    15.064751\n 12.217602  17.839594  18.424776  13.22838   11.311411  14.819928\n 14.658875  13.7187805 15.205975  13.224628  13.426222  19.186739\n 13.796009  17.314821  17.220531  20.376827  13.474758  13.272081\n 16.217325  12.19258   12.8994665 14.097421  14.423346  14.491626\n 19.619738  22.532072  17.982693  13.918682  18.948002  23.309174\n 11.749014  15.8500595 16.307068  18.453756  13.581981  17.870672\n 16.66702   13.020003  18.720028  24.645378   9.636639  24.616138\n 12.222972  11.236137  13.261876  10.138401  15.894842  18.062\n 15.160637  13.229107  17.85968   13.096237   4.6842303 10.617735\n 13.653286  13.303738   9.924666  14.277823  19.67229   11.135014\n 11.689065  15.975611  18.100838  15.018514  12.290604  19.979607\n 16.877028  14.20548   14.292661  24.623188  17.370672  14.563845\n 16.525803  14.707973  13.96238   17.592245  14.994984  15.576306\n 13.525726  13.460179  13.715731  22.246077  19.173677  17.336199\n 11.731371  13.31321   17.404572  18.563168  12.99864   16.657064\n 13.987293  19.71149    9.8371525 23.272978  12.620335  13.45238\n 11.723791  19.973103  18.973814  12.606849  14.679328  20.637266\n 15.358759  13.700088  14.840994  15.073031  10.047023  13.775042\n 31.265104  24.76532   13.795873  14.942535  15.089651  14.20416\n 12.406299  14.92978   13.21507   14.351088  18.218493  16.363123\n 19.862112  14.02297   12.579158  13.821086  17.820179  21.649334\n 10.944521  13.678082  13.622086  15.491899  15.860442  11.552763\n 18.02      18.569096  14.985492  15.681577  17.92849   15.203761\n 16.18999   14.925223  14.85859   13.658061  14.008794  14.091866\n 13.306175   8.700735  14.381162  13.472684 ]", "q_tp1_best_masked": "[18.026089  14.593844  15.190994  14.472853  13.032564  14.748528\n 14.450666  14.690619  10.4684305 12.145261  16.574217  15.688924\n 14.418626  15.74065   18.7028    12.363394  14.158838  14.608002\n 14.657437  18.961683  15.044457  13.698743  13.833257  13.82067\n 19.571625  16.196045  14.370589  15.193101  19.639692  14.750606\n 15.022298  15.819089  15.876393  14.68406   13.573075  13.723919\n 12.441914  16.379808  14.389457  15.221638  10.131077  15.237197\n 16.314356  20.390368  15.300344  16.461918  16.229162  15.099294\n 14.371651  20.380947  13.403112  11.161524  13.007417  14.896143\n 20.579723  17.526335  15.666338  15.048512  16.768097  10.320039\n 17.520214  15.489474  12.621647  18.595825  16.819641  21.494518\n 11.333246  12.990476  19.946938  14.302118  15.249328  13.341424\n 14.462512  12.404143  15.080692  15.653605  11.054594  13.836803\n 27.62119   13.054995  23.38636   13.195557  13.232272  20.796986\n 18.168705  14.074755  18.331848  14.2925825 19.28911   14.394719\n 14.585308  17.467787  13.306782  17.003258  10.015024  14.659767\n 11.990959  21.90662   16.445024  15.104899  16.657253  19.092583\n 15.813789  14.89199   13.870941  13.075445  18.473179  14.673978\n 13.254653  18.230347  17.867216  13.800525  11.795561  15.212271\n 14.879213  15.522933  15.505997  14.576396  13.957384  19.022047\n 14.333279  16.992853  17.046919  21.706823  14.854837  13.240085\n 16.19903   12.903189  12.864019  14.749054  15.056513  15.05691\n 19.417099  23.626352  17.593056  13.405728  19.689953  22.563473\n 12.505289  17.12356   17.363737  18.846256  13.738136  17.507048\n 15.872677  13.742796  18.938152  23.486973  11.724101  24.872622\n 12.428082  12.391812  13.946595  11.460402  16.016308  19.043049\n 15.723405  12.994116  17.097757  13.557758   6.689671  12.386882\n 14.355775  13.641651  11.244821  15.152159  18.810356  12.01631\n 13.288061  17.192032  18.841932  14.762244  12.407145  20.080162\n 18.148458  14.940155  15.386442  24.110746  17.200903  15.126353\n 17.084703  15.005423  13.215874  17.60338   15.142607  15.039711\n 14.048457  14.036213  14.711253  21.676983  18.952559  17.113506\n 11.7720995 13.95093   17.366499  18.502556  13.991025  17.661478\n 15.729691  20.570864  11.105889  23.175764  12.803204  13.8882885\n 12.645734  19.644672  18.142996  14.521796  15.359575  20.714024\n 14.479435  14.295853  14.8580885 15.235003  12.0054455 15.068606\n 30.053637  24.982843  15.010459  14.783891  14.421358  14.909839\n 13.62685   15.856785  13.526262  15.515269  19.215864  16.425453\n 19.5652    14.511277  14.046572  13.961998  18.543226  22.1778\n 12.747381  14.241841  14.217063  16.557194  15.722174  12.031129\n 19.113503  19.348812  16.170788  15.65331   17.377953  14.770751\n 16.535166  15.122281  15.822776  13.921319  14.348226  14.68536\n 14.535962   9.655528  15.687331  13.976203 ]", "policy_t": "[[-0.8372657  -0.9467461   0.20487821 -0.12173438 -0.19604135  0.61603284]\n [-0.33775687  0.5853267   0.9606575  -0.2598474   0.95437646  0.28920782]\n [ 0.07926857  0.4652871   0.86721444 -0.04083329 -0.05380988  0.9852327 ]\n ...\n [-0.9353635  -0.19922376 -0.70849735  0.5269203   0.5964868   0.21938372]\n [ 0.32578993 -0.89820474  0.9934529  -0.7857266  -0.00577939  0.03620267]\n [ 0.26619267  0.48458445 -0.69187814 -0.58877313 -0.65832657 -0.2872579 ]]", "td_error": "[0.83780193 0.3122263  1.3800445  0.60017824 0.23950672 0.9166579\n 0.2876644  0.08266544 0.92823124 0.9399338  0.4631467  0.43253756\n 1.1134157  0.3772273  0.35878468 1.9259424  0.2382927  1.4640379\n 0.6782088  0.81380177 1.0855627  2.2618337  0.99864006 0.48529625\n 2.1800995  1.4334712  0.5875044  0.4210477  0.65105057 1.4293771\n 1.1766553  1.0652771  1.5221248  1.0872235  0.5498252  0.78544044\n 2.8270464  1.685256   0.59612894 1.5452771  1.7682414  0.7254195\n 0.42701483 0.5153799  1.0566921  0.27034473 0.33765507 0.26583815\n 0.24952078 1.2491789  1.2820463  0.45119047 1.0031676  1.0528498\n 0.45545673 1.140912   0.2873578  1.3557186  0.45424747 0.835659\n 1.1862497  1.818327   0.39152718 1.4549923  0.48727703 1.2713203\n 0.9534917  0.95092344 1.060008   1.0015016  0.9911952  0.670465\n 0.4357829  0.8477802  0.52668333 1.9998636  1.3523827  0.970798\n 2.765109   1.2147222  1.1867723  0.7962084  0.37239265 1.7638674\n 1.178092   0.28248882 0.9789839  0.32114458 0.941144   2.3489285\n 0.4638281  0.65788937 0.9332881  0.6836939  0.94168234 1.0026336\n 1.3219256  0.5620022  1.1946259  1.1376591  1.7418008  1.6159954\n 0.18272114 0.47484064 0.11556482 0.85672665 1.4049301  0.04539537\n 1.0318937  1.0430441  0.7790432  0.71781445 1.6493735  0.5297942\n 1.539258   1.3635526  0.4485469  1.4575019  0.25651598 0.92867184\n 0.5454898  1.4878473  1.2212949  1.0560036  0.2570529  0.6162782\n 0.7771969  1.5974693  0.5578985  0.36762667 0.66724396 0.15592146\n 2.2161837  0.9496803  0.27487373 0.5012417  1.3371534  0.68525696\n 1.4534688  1.8159909  0.5242095  1.5258245  1.4579039  0.54105186\n 1.3615298  1.9790592  0.73432255 0.31761646 0.8785291  0.58827305\n 0.8388109  1.8704834  0.81796503 1.5786715  1.6235771  1.5609999\n 0.42862606 0.92920065 0.43644047 0.19085836 1.0674236  2.2027493\n 0.9518275  0.4466567  0.21309328 0.49523354 1.2371893  0.12124777\n 0.8662758  0.24009705 1.7958117  0.3400135  0.8448086  0.792572\n 1.4946938  1.182591   0.9222727  1.0089664  0.3711815  0.69297886\n 0.36841965 0.7929282  0.31473207 1.0858431  0.15246916 0.45014238\n 0.9872999  0.2804594  0.9333658  0.6337786  1.4703865  1.2810717\n 1.2766538  0.16692448 1.2325182  0.4593439  0.62581635 1.8962965\n 0.7405286  1.05655    0.14783525 1.4263659  1.1732621  0.8128872\n 0.1457572  0.28974533 1.1063509  1.0555134  0.6346178  1.5502319\n 0.1899271  0.41750574 0.5738044  0.9906554  1.7889681  0.9909692\n 1.5269375  0.6136856  0.8817744  0.56283903 1.0441852  0.47324038\n 0.09265137 0.25520706 0.9341521  0.38396788 0.85137177 2.0598354\n 1.657649   0.31573534 0.4741435  0.3668728  3.3178735  1.3646126\n 0.38447714 1.0158753  0.33950377 0.26896334 0.7842927  1.1109033\n 1.634265   0.40553284 0.45681286 1.2347932  0.6255455  1.3953915\n 0.7161522  0.5136585  1.7655134  0.06507444 1.172133   1.3734741\n 0.61309576 1.1303244  1.0596642  0.1838584 ]", "mean_td_error": 0.9159755706787109, "actor_loss": -15.697895050048828, "critic_loss": 0.6134855151176453, "alpha_loss": -18.074594497680664, "alpha_value": 0.07285314053297043, "target_entropy": -6, "mean_q": 15.169074058532715, "max_q": 29.9465274810791, "min_q": 5.471059799194336, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 19000, "episodes_total": 19, "training_iteration": 19, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_14-56-20", "timestamp": 1587048980, "time_this_iter_s": 86.39620685577393, "time_total_s": 804.7514674663544, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 804.7514674663544, "timesteps_since_restore": 19000, "iterations_since_restore": 19, "perf": {"cpu_util_percent": 92.3, "ram_util_percent": 11.400000000000004}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -79.90466803920238, "episode_reward_min": -144.84382047930498, "episode_reward_mean": -127.54884303029398, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-139.6477859748138, -111.09493455258418, -144.84382047930498, -141.14688205081123, -104.29305546660405, -143.24178928030238, -138.85297588412425, -143.50169010236272, -79.90466803920238, -128.96082847282986], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2606221674259094, "mean_processing_ms": 0.11978769499878092, "mean_inference_ms": 1.1777127771811584}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -132.53824395239025, "episode_reward_min": -247.13998468717926, "episode_reward_mean": -188.99642891889567, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-132.53824395239025, -247.13998468717926, -221.35721960061747, -201.17740332023865, -142.76929303405268], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.3298995186707245, "mean_processing_ms": 0.4574355377141102, "mean_inference_ms": 1.5859293327150235}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 10000, "num_steps_trained": 2558720, "num_steps_sampled": 20000, "sample_time_ms": 4.197, "replay_time_ms": 25.756, "grad_time_ms": 65.487, "update_time_ms": 0.004, "opt_peak_throughput": 3909.17, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[15.510311 ]\n [15.085221 ]\n [12.17616  ]\n [12.989252 ]\n [23.261705 ]\n [13.393524 ]\n [23.064217 ]\n [12.730292 ]\n [15.173396 ]\n [14.3497715]\n [21.17283  ]\n [15.469215 ]\n [12.738938 ]\n [18.740559 ]\n [11.474563 ]\n [22.900192 ]\n [15.692092 ]\n [11.776881 ]\n [17.062082 ]\n [13.33435  ]\n [13.727253 ]\n [17.458696 ]\n [15.891262 ]\n [14.187217 ]\n [25.554157 ]\n [13.061436 ]\n [13.5956135]\n [13.512587 ]\n [12.8116045]\n [14.286874 ]\n [18.902077 ]\n [15.896188 ]\n [14.503308 ]\n [15.363829 ]\n [17.65081  ]\n [11.82872  ]\n [11.853663 ]\n [14.369286 ]\n [21.364424 ]\n [15.372534 ]\n [20.515522 ]\n [21.63404  ]\n [11.14337  ]\n [19.146526 ]\n [13.435995 ]\n [15.0329   ]\n [14.859394 ]\n [12.076312 ]\n [17.472433 ]\n [25.03613  ]\n [17.51597  ]\n [13.589522 ]\n [17.431875 ]\n [16.863895 ]\n [19.81443  ]\n [13.594382 ]\n [15.090331 ]\n [15.115477 ]\n [19.565498 ]\n [13.688251 ]\n [14.342032 ]\n [14.1244135]\n [21.198952 ]\n [17.184673 ]\n [16.953053 ]\n [20.372652 ]\n [17.73352  ]\n [16.113667 ]\n [15.141809 ]\n [14.373209 ]\n [13.606344 ]\n [22.6929   ]\n [15.9471855]\n [18.064617 ]\n [14.001093 ]\n [15.601689 ]\n [33.147587 ]\n [13.777996 ]\n [19.50393  ]\n [13.685755 ]\n [19.71202  ]\n [15.510311 ]\n [13.549083 ]\n [18.04378  ]\n [13.277989 ]\n [25.811573 ]\n [11.013728 ]\n [13.552199 ]\n [14.696421 ]\n [13.6409445]\n [17.863937 ]\n [15.310088 ]\n [13.305904 ]\n [11.5167265]\n [15.117092 ]\n [25.813448 ]\n [15.259302 ]\n [13.687935 ]\n [14.536646 ]\n [13.496979 ]\n [19.377287 ]\n [16.070818 ]\n [18.759502 ]\n [21.119734 ]\n [24.213964 ]\n [12.889897 ]\n [18.91664  ]\n [14.403135 ]\n [18.543196 ]\n [13.877118 ]\n [12.942237 ]\n [19.392315 ]\n [13.7043085]\n [15.916769 ]\n [18.370773 ]\n [19.026083 ]\n [14.068676 ]\n [13.97296  ]\n [14.334607 ]\n [14.244455 ]\n [16.558758 ]\n [23.202795 ]\n [14.242541 ]\n [14.497788 ]\n [12.124496 ]\n [13.040243 ]\n [16.292868 ]\n [13.436946 ]\n [12.677056 ]\n [ 4.1482987]\n [14.286259 ]\n [17.151041 ]\n [11.358037 ]\n [14.229927 ]\n [ 8.900298 ]\n [17.975807 ]\n [13.824242 ]\n [21.01361  ]\n [12.886617 ]\n [22.44645  ]\n [13.950299 ]\n [13.471613 ]\n [14.675376 ]\n [11.892917 ]\n [19.809597 ]\n [11.14337  ]\n [14.152794 ]\n [15.057593 ]\n [ 8.171027 ]\n [24.058802 ]\n [15.753769 ]\n [15.375219 ]\n [28.48235  ]\n [16.974934 ]\n [13.474634 ]\n [ 8.117142 ]\n [15.940627 ]\n [14.02689  ]\n [14.930494 ]\n [12.310808 ]\n [17.729212 ]\n [26.665865 ]\n [20.768215 ]\n [15.466716 ]\n [14.648238 ]\n [13.651292 ]\n [16.827028 ]\n [12.096313 ]\n [18.525278 ]\n [13.771801 ]\n [14.766549 ]\n [18.112843 ]\n [16.296083 ]\n [18.27953  ]\n [18.867434 ]\n [17.360718 ]\n [19.401224 ]\n [13.433666 ]\n [24.117716 ]\n [12.105952 ]\n [19.774307 ]\n [17.006859 ]\n [12.765286 ]\n [14.106752 ]\n [15.84055  ]\n [18.418383 ]\n [12.38921  ]\n [12.759658 ]\n [12.423656 ]\n [23.41607  ]\n [11.663522 ]\n [14.06982  ]\n [26.7238   ]\n [16.411356 ]\n [13.244446 ]\n [20.535679 ]\n [22.368382 ]\n [22.266731 ]\n [17.12701  ]\n [ 8.385473 ]\n [17.063519 ]\n [15.989547 ]\n [14.310789 ]\n [12.849595 ]\n [12.401782 ]\n [26.514524 ]\n [ 7.3288016]\n [12.409189 ]\n [13.194505 ]\n [24.109362 ]\n [17.2832   ]\n [23.126612 ]\n [14.554297 ]\n [13.048568 ]\n [23.414295 ]\n [13.633607 ]\n [15.310582 ]\n [14.828937 ]\n [14.094834 ]\n [16.989256 ]\n [17.686401 ]\n [11.5336485]\n [17.604126 ]\n [13.104558 ]\n [20.27032  ]\n [12.166003 ]\n [23.23774  ]\n [17.548166 ]\n [13.685817 ]\n [13.287924 ]\n [13.648995 ]\n [15.053397 ]\n [16.374317 ]\n [26.249527 ]\n [22.573982 ]\n [12.1466875]\n [14.540061 ]\n [13.780626 ]\n [14.663105 ]\n [17.521515 ]\n [13.536968 ]\n [15.0207205]\n [22.007269 ]\n [17.385136 ]\n [15.357169 ]\n [18.39457  ]\n [13.663589 ]\n [17.984272 ]\n [14.448046 ]\n [11.68292  ]\n [14.194257 ]\n [11.06768  ]\n [18.759502 ]\n [12.702094 ]\n [13.059839 ]\n [15.582083 ]]", "q_t_selected": "[15.510311  15.085221  12.17616   12.989252  23.261705  13.393524\n 23.064217  12.730292  15.173396  14.3497715 21.17283   15.469215\n 12.738938  18.740559  11.474563  22.900192  15.692092  11.776881\n 17.062082  13.33435   13.727253  17.458696  15.891262  14.187217\n 25.554157  13.061436  13.5956135 13.512587  12.8116045 14.286874\n 18.902077  15.896188  14.503308  15.363829  17.65081   11.82872\n 11.853663  14.369286  21.364424  15.372534  20.515522  21.63404\n 11.14337   19.146526  13.435995  15.0329    14.859394  12.076312\n 17.472433  25.03613   17.51597   13.589522  17.431875  16.863895\n 19.81443   13.594382  15.090331  15.115477  19.565498  13.688251\n 14.342032  14.1244135 21.198952  17.184673  16.953053  20.372652\n 17.73352   16.113667  15.141809  14.373209  13.606344  22.6929\n 15.9471855 18.064617  14.001093  15.601689  33.147587  13.777996\n 19.50393   13.685755  19.71202   15.510311  13.549083  18.04378\n 13.277989  25.811573  11.013728  13.552199  14.696421  13.6409445\n 17.863937  15.310088  13.305904  11.5167265 15.117092  25.813448\n 15.259302  13.687935  14.536646  13.496979  19.377287  16.070818\n 18.759502  21.119734  24.213964  12.889897  18.91664   14.403135\n 18.543196  13.877118  12.942237  19.392315  13.7043085 15.916769\n 18.370773  19.026083  14.068676  13.97296   14.334607  14.244455\n 16.558758  23.202795  14.242541  14.497788  12.124496  13.040243\n 16.292868  13.436946  12.677056   4.1482987 14.286259  17.151041\n 11.358037  14.229927   8.900298  17.975807  13.824242  21.01361\n 12.886617  22.44645   13.950299  13.471613  14.675376  11.892917\n 19.809597  11.14337   14.152794  15.057593   8.171027  24.058802\n 15.753769  15.375219  28.48235   16.974934  13.474634   8.117142\n 15.940627  14.02689   14.930494  12.310808  17.729212  26.665865\n 20.768215  15.466716  14.648238  13.651292  16.827028  12.096313\n 18.525278  13.771801  14.766549  18.112843  16.296083  18.27953\n 18.867434  17.360718  19.401224  13.433666  24.117716  12.105952\n 19.774307  17.006859  12.765286  14.106752  15.84055   18.418383\n 12.38921   12.759658  12.423656  23.41607   11.663522  14.06982\n 26.7238    16.411356  13.244446  20.535679  22.368382  22.266731\n 17.12701    8.385473  17.063519  15.989547  14.310789  12.849595\n 12.401782  26.514524   7.3288016 12.409189  13.194505  24.109362\n 17.2832    23.126612  14.554297  13.048568  23.414295  13.633607\n 15.310582  14.828937  14.094834  16.989256  17.686401  11.5336485\n 17.604126  13.104558  20.27032   12.166003  23.23774   17.548166\n 13.685817  13.287924  13.648995  15.053397  16.374317  26.249527\n 22.573982  12.1466875 14.540061  13.780626  14.663105  17.521515\n 13.536968  15.0207205 22.007269  17.385136  15.357169  18.39457\n 13.663589  17.984272  14.448046  11.68292   14.194257  11.06768\n 18.759502  12.702094  13.059839  15.582083 ]", "twin_q_t_selected": "[15.38604   15.884724  11.119402  13.4854765 21.412287  14.074719\n 21.59973   12.8647375 14.879186  13.601745  20.69542   14.826424\n 13.019089  17.90526   11.222286  21.502361  14.217101  11.248983\n 17.410841  13.457477  13.304171  16.43465   15.736412  14.374805\n 26.045872  13.024858  14.115184  13.170326  13.992295  14.145523\n 19.725557  16.107517  15.0024185 15.015277  17.511871  11.541538\n 11.886595  14.875548  22.013966  15.311439  19.622784  21.135674\n 10.262899  19.432907  13.406974  15.783475  14.642455  12.735279\n 17.604166  24.107769  16.758404  12.892254  17.196215  16.49512\n 19.7307    13.006366  14.027129  14.961845  19.924196  14.302813\n 14.342471  14.158102  21.15922   17.795073  16.328844  20.42324\n 17.96528   15.429846  14.731439  13.868199  13.591825  23.528507\n 15.885857  18.333708  12.957033  15.720394  32.21379   12.789496\n 18.723604  14.164744  19.236776  15.38604   13.533653  16.746592\n 13.0324955 26.904657  10.731084  14.256984  13.521752  14.163512\n 17.734106  14.741945  13.36663   11.472199  14.205285  25.009108\n 15.861043  14.673761  13.725379  13.513291  18.118568  15.382533\n 18.70764   21.18312   25.348162  13.608778  18.671108  14.52853\n 18.111605  13.409329  13.674662  19.705946  13.687134  15.257469\n 19.256132  19.448471  15.224015  13.17271   14.597181  14.527975\n 16.666895  22.805082  13.826118  14.066027  11.377504  12.543073\n 16.78192   13.371757  12.038858   4.202155  15.160563  15.4315405\n 10.498704  14.153782   8.475107  18.272663  13.22897   22.479324\n 12.972666  22.086563  12.934158  13.316699  13.914088  13.04685\n 19.131905  10.262899  12.832035  15.710465   8.05065   24.274601\n 16.158266  14.655088  29.076818  15.943052  13.895702   8.481028\n 15.2034    13.945824  14.076007  12.278557  17.469625  25.835224\n 21.375677  15.848656  14.275294  13.65466   16.053566  12.718887\n 17.647818  14.052037  14.42873   18.29754   15.986404  18.062117\n 18.722229  15.402966  18.46894   12.872071  25.784742  11.96182\n 18.648098  17.42587   13.209277  13.94107   15.903799  17.476938\n 11.172844  12.080646  13.394679  23.422554  11.517797  13.77266\n 26.92656   16.091715  13.062259  18.749182  22.01198   22.651596\n 16.797005   8.478619  15.987398  15.513052  14.494276  12.869796\n 12.303557  24.690584   7.845646  12.5472765 13.928864  23.913118\n 18.311935  22.421602  14.163208  13.6909075 23.113314  13.379169\n 14.305718  14.134814  14.735383  18.017855  16.317438  11.865225\n 17.362835  13.389957  20.253199  12.627849  22.247993  16.658789\n 13.0659275 14.052449  14.162978  14.465392  17.97285   26.82321\n 22.515175  11.740872  14.144506  12.725788  14.801412  16.261984\n 13.704281  14.115799  23.710709  18.054827  16.1991    17.964033\n 13.903971  17.712698  14.715997  11.225011  12.876918  11.5655575\n 18.70764   13.910108  12.703042  17.652788 ]", "q_t_selected_target": "[14.929761  15.468254  11.628673  13.467124  22.334867  13.255999\n 22.887936  11.435925  15.619474  14.76249   22.555256  16.000101\n 12.7561865 17.693045  12.490621  24.776499  14.10084   13.53168\n 18.77553   13.764201  12.358317  16.642569  17.868498  14.930403\n 26.370958  12.906488  13.944655  13.981505  14.727833  14.196126\n 19.954573  15.494751  15.03037   14.434384  16.55606   12.715853\n 13.275444  15.547355  19.452005  15.362041  22.77961   21.617306\n 12.947724  19.854305  13.071629  16.96318   14.919162  11.66125\n 19.238678  25.93196   16.67622   13.853452  15.409734  16.087051\n 19.908321  13.081827  12.563189  14.497106  20.401546  12.743758\n 13.957304  13.7252865 20.712238  18.341476  18.318472  21.961948\n 19.25342   16.535147  13.220582  13.136127  13.559908  21.73496\n 16.93213   17.956482  12.7094345 14.315341  33.002678  13.616949\n 19.10602   14.177071  20.25106   14.821213  13.699768  18.616695\n 13.318832  26.750776  11.567973  15.069809  14.369985  13.272944\n 17.274965  15.513993  12.081712  12.467343  13.808348  23.661718\n 17.075184  14.190788  14.253569  14.469456  18.592484  15.334597\n 21.546661  20.188972  23.926058  13.593779  19.644453  13.605986\n 16.849913  12.189918  11.985075  20.882664  14.200933  15.719197\n 18.651493  19.259613  13.661554  13.803936  14.320423  13.414273\n 17.48982   22.09644   11.289478  16.045357  11.188689  12.353766\n 18.335075  14.019866  12.80962    2.6931605 14.06377   15.010377\n 10.341017  15.462967   7.0408783 18.172928  14.127001  20.86956\n 12.408645  22.439585  13.375782  13.9657135 14.0402355 11.757991\n 21.748161  12.227384  13.292099  16.453058   6.9485097 25.82592\n 15.526785  14.71238   30.477911  15.974911  13.681499   7.215706\n 14.579485  13.198439  15.183926  12.971949  16.681742  25.009796\n 22.462864  13.859818  13.319424  14.803968  17.247162  10.918433\n 18.875399  13.304272  13.839451  17.8597    17.021297  19.591814\n 18.994423  17.05164   20.74718   16.160255  26.120722  12.6032295\n 21.020885  18.068365  11.102352  13.977893  17.88843   19.3694\n 11.444479  11.736249  12.893926  22.534504  12.300211  12.801233\n 25.567488  17.73433   13.47599   22.29604   22.349709  23.059866\n 15.284546   7.1253433 17.12988   16.998705  15.033215  13.481901\n 12.8184185 24.635098   6.0380993 13.8480215 11.019917  25.40601\n 16.205494  22.836184  15.525374  14.33603   25.951723  12.90365\n 14.294525  14.891132  16.721117  18.993414  17.447056  11.955778\n 15.617418  15.513928  20.482386  11.537029  23.538212  18.64048\n 11.830246  14.413818  14.739233  15.102196  17.038141  25.34459\n 23.530113  11.88193   14.182962  13.737388  14.266715  18.043276\n 15.317762  13.574259  22.81223   16.644339  15.461111  19.277657\n 14.247303  17.668016  14.039797  12.021591  14.254787  11.487747\n 21.697077  13.344551  12.558277  16.241644 ]", "q_tp1_best_masked": "[16.25053   15.845594  11.807111  13.765594  23.280113  13.548207\n 22.278051  12.631843  14.668458  14.440858  21.757668  15.091961\n 13.92116   18.356237  13.0592785 25.273195  15.139262  15.566856\n 20.094028  14.008809  12.558522  17.44981   18.57051   14.240007\n 26.896833  14.074935  14.868254  14.090631  16.140388  14.68509\n 19.9476    16.029924  15.187426  15.559616  16.46473   13.389115\n 15.185402  16.552029  19.61282   14.528324  22.404924  21.0992\n 13.2203245 19.61068   13.475223  17.842947  15.992184  12.396585\n 19.61024   25.432737  17.272919  13.517349  16.13036   16.283953\n 21.06367   12.953357  13.451023  14.933032  19.849766  13.607786\n 14.25256   13.453892  20.27716   17.504272  18.61312   21.258127\n 18.693375  15.417619  13.960037  14.211206  14.061678  20.995564\n 18.535206  18.561016  13.192895  14.562278  31.376068  13.949828\n 18.801851  15.275345  19.666737  16.140886  13.754703  19.159407\n 13.368125  26.167086  13.140488  15.952159  14.554368  15.315662\n 18.105997  16.729015  12.998029  13.559682  13.6514015 22.70841\n 17.907856  15.282659  14.911309  15.060942  19.07641   15.546404\n 22.015642  20.068878  23.484936  13.158191  19.218     14.340214\n 17.225187  13.746431  12.939207  21.325644  14.49742   16.295792\n 18.622717  19.625893  14.246306  13.179193  14.568253  14.260954\n 18.174938  21.808167  12.821787  16.773958  11.761688  12.804888\n 19.059933  14.42844   13.275561   4.5580416 14.827072  15.786186\n 11.452314  16.589508   8.741533  18.702421  15.081139  19.867514\n 13.074085  22.557194  14.209543  13.966601  14.774666  12.496862\n 21.455173  12.492707  13.96951   17.228983   8.026119  25.693388\n 15.987703  14.875621  29.853666  16.000584  14.018015   8.547307\n 14.523262  14.080431  15.646316  13.251034  17.594381  24.858027\n 22.155138  14.514904  14.266677  15.58904   18.286085  11.180234\n 20.098667  13.820646  13.960169  18.877258  17.503271  19.692177\n 20.862297  16.601276  20.386532  16.437788  25.781176  13.270647\n 21.4621    18.257505  11.588748  14.545612  18.644823  20.896626\n 11.842353  13.23612   13.314064  21.945778  13.144058  13.087801\n 23.95991   17.752457  14.984973  23.001247  21.984274  23.702587\n 15.407189   8.698199  18.289143  17.19302   14.675993  14.647758\n 13.605395  23.951717   8.233173  14.753963  11.5461445 24.504637\n 17.140661  22.678196  16.352486  14.329894  26.033058  13.666264\n 13.726797  15.645599  17.694883  19.170284  19.275095  13.226723\n 15.786887  16.674314  21.545101  12.242734  24.171555  19.729113\n 11.695392  15.154913  15.386606  15.69019   16.767523  25.194025\n 23.049816  13.496833  13.104392  14.313482  14.680374  18.337097\n 15.927015  14.301277  23.18011   17.25911   16.359732  19.591639\n 13.652697  17.699568  13.497659  13.570715  13.581149  13.548342\n 22.167578  13.613734  13.248504  16.760256 ]", "policy_t": "[[ 0.7415782   0.75966096  0.79859686  0.19884455  0.31807637  0.35131156]\n [ 0.749694   -0.6073098  -0.53800005 -0.6446336  -0.93910724 -0.44719815]\n [ 0.7992077  -0.60615677  0.30948615  0.91177905  0.9265406   0.96576595]\n ...\n [ 0.98524106 -0.9382215  -0.6055435   0.8068141  -0.22235626 -0.86632496]\n [-0.87512517 -0.79554933  0.4325738   0.83071136  0.5472225  -0.46808028]\n [ 0.9785147   0.9918345  -0.9874699   0.64862204  0.10765457 -0.07667786]]", "td_error": "[0.5184145  0.3997512  0.52837896 0.2481122  0.9247093  0.4781232\n 0.73224354 1.3615904  0.5931835  0.7867322  1.621131   0.8522816\n 0.1400752  0.6298647  1.1421962  2.575222   0.8537569  2.0187478\n 1.5390682  0.36828804 1.1573944  0.512023   2.0546608  0.64939165\n 0.57094383 0.13665867 0.25978518 0.640049   1.3258829  0.07067537\n 0.64075565 0.50710154 0.27750635 0.75516844 1.02528    1.0307236\n 1.4053149  0.9249377  2.2371893  0.03054762 2.7104568  0.24918365\n 2.2445898  0.56458855 0.3498559  1.5549932  0.16823721 0.74454546\n 1.7003784  1.3600092  0.46096706 0.6125636  1.9043112  0.59245586\n 0.1357565  0.29400826 1.9955416  0.5415554  0.6566992  1.2517734\n 0.38494778 0.41597128 0.46684742 0.8516035  1.6775236  1.564002\n 1.4040194  0.76339054 1.716042   0.9845767  0.03917646 1.3757429\n 1.0156097  0.24268055 0.7696285  1.3457007  0.46689796 0.49424982\n 0.39016247 0.25182104 0.7766619  0.62696266 0.15840006 1.2215099\n 0.16358995 0.54654217 0.69556713 1.1652174  0.58733416 0.6292839\n 0.52405643 0.48797655 1.2545552  0.97288036 0.8528409  1.7495594\n 1.5150113  0.49291325 0.40563345 0.96432066 0.62935925 0.39207888\n 2.8130903  0.9624548  0.85500526 0.35944033 0.85057926 0.85984707\n 1.4774876  1.4533062  1.3233743  1.3335333  0.5052123  0.32964993\n 0.4426794  0.21119404 0.9847913  0.40012503 0.1454711  0.97194195\n 0.87699413 0.9074993  2.7448516  1.7634492  0.5623112  0.43789148\n 1.7976818  0.61551476 0.45166254 1.4820664  0.6596408  1.2809138\n 0.5873537  1.2711124  1.6468244  0.14842796 0.6003952  0.87690735\n 0.5209966  0.17994308 0.50807047 0.5715575  0.38064384 0.7118926\n 2.2774105  1.5242491  0.6603794  1.0690289  1.1623287  1.6592188\n 0.4292326  0.36006546 1.6983271  0.51594067 0.2105341  1.0833788\n 0.99252844 0.7879181  0.680675   0.6772661  0.9176769  1.2407484\n 1.3909178  1.7978673  1.1423426  1.1509924  0.80686474 1.4891672\n 0.7888508  0.6076474  0.7581887  0.34549236 0.8800535  1.420991\n 0.19959164 0.9788761  1.8120985  3.0073867  1.1694927  0.56934357\n 1.8096828  0.85200024 1.8849297  0.0828414  2.016255   1.4217396\n 0.6081829  0.68390274 0.4855113  0.88480854 0.70955133 1.120007\n 1.2576923  1.4827938  0.32263803 2.6536093  0.17820072 0.6007023\n 1.6774616  1.3067026  0.60442257 1.2474055  0.630682   0.62220573\n 0.4657488  0.9674568  1.5491245  1.3697886  2.5417676  1.3947697\n 1.5920734  0.35250473 1.1666217  0.9662924  2.6879187  0.6027379\n 0.51362514 0.40925694 2.3060083  1.4898586  0.6844816  0.25634146\n 1.8660622  2.2666707  0.22062588 0.85989666 0.7953453  1.5370026\n 1.5456262  0.74363184 0.83324623 0.3428011  0.7992668  1.1917791\n 0.98553467 0.20290756 0.19777727 0.5274191  0.46554327 1.1515265\n 1.6971378  0.9940009  0.85171986 1.0756426  0.42096567 1.0983553\n 0.4635234  0.18046856 0.5422244  0.5676255  0.71920013 0.24893856\n 2.9635057  0.60400677 0.3231635  1.0353527 ]", "mean_td_error": 0.9412298202514648, "actor_loss": -16.418167114257812, "critic_loss": 0.6791400909423828, "alpha_loss": -16.328298568725586, "alpha_value": 0.056099530309438705, "target_entropy": -6, "mean_q": 16.1053466796875, "max_q": 33.147586822509766, "min_q": 4.148298740386963, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 20000, "episodes_total": 20, "training_iteration": 20, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_14-58-03", "timestamp": 1587049083, "time_this_iter_s": 86.17124104499817, "time_total_s": 890.9227085113525, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 890.9227085113525, "timesteps_since_restore": 20000, "iterations_since_restore": 20, "perf": {"cpu_util_percent": 92.2, "ram_util_percent": 11.428965517241382}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -54.2892066311024, "episode_reward_min": -108.33169098872314, "episode_reward_mean": -79.42398849456995, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-79.383425959757, -108.33169098872314, -95.5825122314989, -54.2892066311024, -56.13136117213024, -89.48150482888866, -76.11373561670126, -66.61280587298828, -104.63207976987567, -63.681561874033804], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26048163945772373, "mean_processing_ms": 0.11972254377276395, "mean_inference_ms": 1.1784203771909456}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -107.19239853543543, "episode_reward_min": -221.35721960061747, "episode_reward_mean": -161.00691168854692, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-107.19239853543543, -221.35721960061747, -201.17740332023865, -142.76929303405268, -132.53824395239025], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.3333074107648476, "mean_processing_ms": 0.46091422616871264, "mean_inference_ms": 1.6070805653834923}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 10500, "num_steps_trained": 2814720, "num_steps_sampled": 21000, "sample_time_ms": 3.995, "replay_time_ms": 23.97, "grad_time_ms": 52.494, "update_time_ms": 0.004, "opt_peak_throughput": 4876.706, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[21.66732  ]\n [17.919266 ]\n [14.868612 ]\n [21.82039  ]\n [27.1566   ]\n [12.327976 ]\n [13.246018 ]\n [13.869911 ]\n [13.944616 ]\n [14.002168 ]\n [12.748415 ]\n [14.936845 ]\n [13.557244 ]\n [13.081231 ]\n [25.844124 ]\n [22.02036  ]\n [18.6041   ]\n [12.07021  ]\n [13.544419 ]\n [21.610132 ]\n [13.223472 ]\n [13.949129 ]\n [17.8913   ]\n [11.804916 ]\n [17.86421  ]\n [23.195965 ]\n [13.86094  ]\n [18.60657  ]\n [20.65529  ]\n [17.7536   ]\n [18.412031 ]\n [12.559606 ]\n [15.207    ]\n [ 9.169755 ]\n [13.999484 ]\n [15.164163 ]\n [13.163815 ]\n [12.15759  ]\n [14.220899 ]\n [11.19068  ]\n [20.824327 ]\n [14.492959 ]\n [18.540997 ]\n [10.881041 ]\n [12.44362  ]\n [11.894808 ]\n [12.555035 ]\n [18.962938 ]\n [12.263154 ]\n [12.976077 ]\n [12.701574 ]\n [17.73796  ]\n [13.431926 ]\n [ 9.5687895]\n [24.11338  ]\n [22.554031 ]\n [11.303975 ]\n [19.37743  ]\n [11.659667 ]\n [17.685383 ]\n [13.533614 ]\n [21.24443  ]\n [12.992031 ]\n [23.052996 ]\n [14.488107 ]\n [14.539244 ]\n [17.495047 ]\n [13.436084 ]\n [19.226175 ]\n [21.64991  ]\n [14.416005 ]\n [17.620228 ]\n [13.645375 ]\n [11.875755 ]\n [15.849973 ]\n [17.471796 ]\n [13.160791 ]\n [15.077098 ]\n [13.156028 ]\n [13.718406 ]\n [11.777956 ]\n [ 9.83034  ]\n [21.545813 ]\n [19.5329   ]\n [15.915123 ]\n [12.69669  ]\n [16.889154 ]\n [19.479843 ]\n [ 9.71002  ]\n [16.075983 ]\n [14.373283 ]\n [15.848733 ]\n [17.707129 ]\n [18.277601 ]\n [13.987772 ]\n [12.209732 ]\n [18.265116 ]\n [10.329195 ]\n [12.558372 ]\n [11.905671 ]\n [13.137611 ]\n [11.554239 ]\n [17.223604 ]\n [19.2398   ]\n [19.390324 ]\n [11.888917 ]\n [13.301527 ]\n [13.766331 ]\n [15.965991 ]\n [17.048958 ]\n [12.800107 ]\n [13.169065 ]\n [17.103077 ]\n [18.904663 ]\n [11.802963 ]\n [18.849215 ]\n [17.734793 ]\n [22.651203 ]\n [14.994108 ]\n [11.743562 ]\n [12.22622  ]\n [12.969976 ]\n [12.555264 ]\n [14.118566 ]\n [11.346831 ]\n [18.729525 ]\n [15.662374 ]\n [22.7194   ]\n [23.146147 ]\n [13.289786 ]\n [12.082402 ]\n [ 9.789275 ]\n [12.780103 ]\n [17.145994 ]\n [18.489511 ]\n [13.137183 ]\n [ 8.60689  ]\n [17.288923 ]\n [17.78659  ]\n [16.400322 ]\n [13.306308 ]\n [18.479681 ]\n [ 5.232655 ]\n [11.988345 ]\n [15.335499 ]\n [20.370584 ]\n [16.923502 ]\n [14.894458 ]\n [12.281156 ]\n [12.745146 ]\n [14.138069 ]\n [14.118008 ]\n [11.485332 ]\n [24.36631  ]\n [24.520658 ]\n [16.675148 ]\n [18.3401   ]\n [14.848649 ]\n [15.791403 ]\n [15.1374235]\n [13.221083 ]\n [20.272264 ]\n [18.09711  ]\n [12.62368  ]\n [14.641923 ]\n [28.659643 ]\n [17.006428 ]\n [12.463448 ]\n [21.01752  ]\n [17.872288 ]\n [10.331683 ]\n [13.940873 ]\n [12.573375 ]\n [13.763288 ]\n [12.465075 ]\n [11.060417 ]\n [12.525634 ]\n [23.24183  ]\n [19.879932 ]\n [14.14821  ]\n [13.07419  ]\n [18.905123 ]\n [12.431833 ]\n [14.223492 ]\n [14.114509 ]\n [22.419315 ]\n [13.808068 ]\n [17.921768 ]\n [ 7.7907023]\n [11.610321 ]\n [14.513279 ]\n [14.739113 ]\n [15.795047 ]\n [15.340074 ]\n [21.922955 ]\n [15.669863 ]\n [12.684501 ]\n [20.519701 ]\n [16.486712 ]\n [11.624497 ]\n [12.952575 ]\n [11.506472 ]\n [12.060031 ]\n [19.854551 ]\n [22.152672 ]\n [12.541863 ]\n [11.496801 ]\n [17.331602 ]\n [12.289661 ]\n [ 9.672172 ]\n [10.462111 ]\n [15.933041 ]\n [14.813226 ]\n [21.653233 ]\n [18.385403 ]\n [13.944616 ]\n [13.942047 ]\n [14.741119 ]\n [11.4747095]\n [11.169111 ]\n [14.154347 ]\n [21.510944 ]\n [23.718903 ]\n [19.521564 ]\n [13.330538 ]\n [10.880133 ]\n [16.66531  ]\n [18.278566 ]\n [ 6.580232 ]\n [18.81393  ]\n [16.655378 ]\n [17.187666 ]\n [10.447622 ]\n [17.693027 ]\n [22.052443 ]\n [12.201563 ]\n [11.939383 ]\n [13.251634 ]\n [11.598039 ]\n [14.403817 ]\n [20.516386 ]\n [10.712056 ]\n [16.61191  ]\n [14.980959 ]\n [13.006865 ]\n [25.53484  ]\n [13.449913 ]\n [ 9.572924 ]\n [12.077389 ]\n [21.238743 ]\n [12.820909 ]\n [11.611234 ]\n [14.482884 ]\n [ 9.203977 ]\n [20.452394 ]\n [13.811074 ]]", "q_t_selected": "[21.66732   17.919266  14.868612  21.82039   27.1566    12.327976\n 13.246018  13.869911  13.944616  14.002168  12.748415  14.936845\n 13.557244  13.081231  25.844124  22.02036   18.6041    12.07021\n 13.544419  21.610132  13.223472  13.949129  17.8913    11.804916\n 17.86421   23.195965  13.86094   18.60657   20.65529   17.7536\n 18.412031  12.559606  15.207      9.169755  13.999484  15.164163\n 13.163815  12.15759   14.220899  11.19068   20.824327  14.492959\n 18.540997  10.881041  12.44362   11.894808  12.555035  18.962938\n 12.263154  12.976077  12.701574  17.73796   13.431926   9.5687895\n 24.11338   22.554031  11.303975  19.37743   11.659667  17.685383\n 13.533614  21.24443   12.992031  23.052996  14.488107  14.539244\n 17.495047  13.436084  19.226175  21.64991   14.416005  17.620228\n 13.645375  11.875755  15.849973  17.471796  13.160791  15.077098\n 13.156028  13.718406  11.777956   9.83034   21.545813  19.5329\n 15.915123  12.69669   16.889154  19.479843   9.71002   16.075983\n 14.373283  15.848733  17.707129  18.277601  13.987772  12.209732\n 18.265116  10.329195  12.558372  11.905671  13.137611  11.554239\n 17.223604  19.2398    19.390324  11.888917  13.301527  13.766331\n 15.965991  17.048958  12.800107  13.169065  17.103077  18.904663\n 11.802963  18.849215  17.734793  22.651203  14.994108  11.743562\n 12.22622   12.969976  12.555264  14.118566  11.346831  18.729525\n 15.662374  22.7194    23.146147  13.289786  12.082402   9.789275\n 12.780103  17.145994  18.489511  13.137183   8.60689   17.288923\n 17.78659   16.400322  13.306308  18.479681   5.232655  11.988345\n 15.335499  20.370584  16.923502  14.894458  12.281156  12.745146\n 14.138069  14.118008  11.485332  24.36631   24.520658  16.675148\n 18.3401    14.848649  15.791403  15.1374235 13.221083  20.272264\n 18.09711   12.62368   14.641923  28.659643  17.006428  12.463448\n 21.01752   17.872288  10.331683  13.940873  12.573375  13.763288\n 12.465075  11.060417  12.525634  23.24183   19.879932  14.14821\n 13.07419   18.905123  12.431833  14.223492  14.114509  22.419315\n 13.808068  17.921768   7.7907023 11.610321  14.513279  14.739113\n 15.795047  15.340074  21.922955  15.669863  12.684501  20.519701\n 16.486712  11.624497  12.952575  11.506472  12.060031  19.854551\n 22.152672  12.541863  11.496801  17.331602  12.289661   9.672172\n 10.462111  15.933041  14.813226  21.653233  18.385403  13.944616\n 13.942047  14.741119  11.4747095 11.169111  14.154347  21.510944\n 23.718903  19.521564  13.330538  10.880133  16.66531   18.278566\n  6.580232  18.81393   16.655378  17.187666  10.447622  17.693027\n 22.052443  12.201563  11.939383  13.251634  11.598039  14.403817\n 20.516386  10.712056  16.61191   14.980959  13.006865  25.53484\n 13.449913   9.572924  12.077389  21.238743  12.820909  11.611234\n 14.482884   9.203977  20.452394  13.811074 ]", "twin_q_t_selected": "[21.03496   19.016722  15.767493  21.42508   26.519098  12.392951\n 13.378724  13.484818  14.228306  13.699054  13.005686  15.036783\n 13.752848  13.077318  25.207047  22.448898  18.800207  13.717593\n 12.326439  23.164543  13.823107  12.959943  18.388494  11.715638\n 17.290947  23.823076  13.578128  19.015535  21.084763  18.084608\n 19.119627  11.861093  15.105902  10.403876  14.23398   14.821188\n 12.3899    12.304697  15.075353  11.923997  21.277197  14.422893\n 18.188961  10.61005   11.773954  12.275117  13.561944  18.113365\n 13.088238  12.874773  13.5764265 18.638025  13.655888   9.115794\n 23.242989  22.728073  11.606672  20.288576  11.641333  19.123154\n 12.918462  21.683537  13.642981  21.838728  15.37899   14.395694\n 17.115576  14.2304125 19.559906  20.358318  15.229941  18.407267\n 13.270744  10.87345   16.378532  18.470482  13.953628  15.39155\n 13.458232  13.475662  11.956625  11.508651  21.673693  18.430725\n 17.515255  13.0424795 16.844692  19.070936   8.845209  16.192728\n 14.892059  16.575525  17.807192  18.580553  13.490873  11.551329\n 18.941433  12.250463  11.65961   11.820591  13.436789  11.252981\n 16.022448  19.78239   19.368092  12.115918  12.512739  13.281693\n 15.518832  16.80425   12.948919  13.364265  16.527813  19.33203\n 12.70099   20.025465  17.194391  22.089571  14.500611  12.700942\n 12.238696  13.096628  12.220418  14.02306   10.260141  18.994793\n 15.100354  22.56457   23.302092  14.375034  12.204689   9.140507\n 12.371032  17.03208   18.965721  12.864606   8.497194  17.69451\n 17.799109  17.290714  14.598283  19.610287   4.386162  12.054859\n 15.40821   20.288946  18.48937   15.800883  13.338372  12.412712\n 14.373425  13.466839  10.051921  23.70812   24.034342  17.602648\n 18.87395   14.482007  15.93185   15.343267  12.269975  21.489443\n 18.41914   11.742448  14.74874   28.200987  17.325624  12.772197\n 21.277504  16.879097  10.799539  14.303664  11.46119   15.06166\n 11.748109  11.187869  13.579781  23.42024   20.353966  14.909414\n 12.171301  18.022272  13.761047  13.88609   13.004415  23.000301\n 12.942312  18.810995   8.066957  11.097073  14.717006  14.963738\n 14.353728  15.112609  21.976948  15.2415085 12.287946  20.705477\n 18.692183  11.492616  12.267475  11.130025  11.966988  19.974813\n 22.256268  12.407364  11.774435  17.964256  12.66939    8.926465\n 10.958221  15.30435   15.255471  20.578339  18.529118  14.228306\n 13.223815  14.148357  12.527568  11.531786  14.772988  21.835897\n 23.653347  18.889952  13.627896  10.352641  16.578018  17.982601\n  6.6570296 17.903301  17.348303  17.359972  11.607665  17.4234\n 22.182508  12.114582  12.050943  13.428367  12.057266  15.26427\n 21.124773  10.440879  17.148682  15.25423   13.056956  24.845304\n 13.94252   10.638993  13.065129  21.36022   14.163925  11.710111\n 14.886151   8.67488   19.312664  12.606758 ]", "q_t_selected_target": "[20.840818  18.86846   14.590434  21.962795  27.379654  12.041993\n 14.0465555 13.872869  15.587807  15.042735  12.611677  14.735886\n 13.766729  11.615851  26.438965  22.303074  16.457846  13.849778\n 13.481716  22.886335  15.070766  13.543726  19.198925  12.806276\n 17.8787    22.617407  12.367085  19.289679  22.592648  18.366991\n 18.935654  13.350903  16.589136   8.9922905 14.355549  16.9417\n 12.837413  11.142708  13.611772  11.007474  19.030275  13.366676\n 17.340944  10.793804  11.294937  12.590614  12.800523  17.820068\n 10.988709  12.607578  12.482192  19.910612  14.511476   8.615486\n 24.77177   22.177887  10.844556  19.605251  11.138497  18.257553\n 13.065353  23.654781  12.627256  21.569036  15.240193  17.050293\n 18.237661  14.133882  19.927559  21.595943  13.300046  19.61491\n 13.129137  11.71986   14.962958  18.200827  13.267611  16.092606\n 14.081688  13.283014  13.451118  11.459792  21.065344  20.08019\n 16.89672   12.850172  15.350493  20.069748   8.960952  16.849348\n 14.535613  15.005667  18.84833   16.603989  13.593034  12.59839\n 21.571     10.208172  10.907512  12.11606   13.064568  11.910844\n 17.113607  19.229269  18.70657   11.705314  12.476013  14.418266\n 15.423843  17.666807  13.90705   14.024064  13.480462  19.836454\n 13.812854  21.142208  18.616514  21.92672   14.806281  11.13471\n 13.183955  12.078554  12.520725  12.898813  11.190564  18.35408\n 15.320813  20.239267  23.639439  14.161328  11.830466  10.99109\n 13.237146  18.29774   18.46033   12.980219  10.351737  17.301277\n 18.096575  16.006699  14.671299  19.822256   5.143263  11.131229\n 16.294062  18.301819  19.154985  13.965555  13.406493  13.459778\n 14.534947  13.742509  10.787251  24.981592  24.02637   17.327383\n 18.685606  15.492522  16.802395  16.33217   12.299996  19.175737\n 19.875193  12.369161  15.153891  26.960228  17.73503   12.915117\n 20.416811  19.134544  10.989323  14.348847  13.1292305 13.859643\n 11.74886   11.4687195 13.244641  21.930916  20.551853  13.818305\n 13.351995  19.07366   13.507079  13.098394  14.915968  21.427015\n 14.117435  18.6804     7.608449  12.069429  13.977377  15.283102\n 14.771087  14.836371  21.505493  16.744593  12.149152  21.826864\n 16.226103  10.118935  12.308684  12.338152  12.070289  19.279478\n 22.791542  11.908593  11.977501  17.446674  12.945194   9.720012\n 10.504699  16.833199  13.77208   22.113766  18.05118   15.828581\n 14.7122    14.173865  10.9546175 11.327367  13.010419  21.918043\n 24.470549  19.057549  12.12453   10.148791  17.273342  18.950287\n  5.001792  16.627028  14.712062  17.747375  11.033026  16.599468\n 23.159525  12.506786  13.104924  13.862986  11.391268  15.072785\n 20.251434  11.000413  16.725012  15.705983  12.596177  25.918098\n 13.16275   11.500928  13.440231  20.221891  13.968221  12.809357\n 14.117676   9.1261425 21.591896  13.02356  ]", "q_tp1_best_masked": "[21.307676  19.459929  14.2213955 21.236115  27.320894  12.614467\n 14.019975  14.18322   15.210752  14.633174  13.348665  13.3166895\n 13.763631  11.850849  24.916101  22.132538  16.779     15.806825\n 13.950477  23.713903  14.735644  13.723928  20.301718  13.25417\n 18.954596  21.796696  12.832053  19.990194  24.033941  19.620766\n 18.781693  13.925409  15.526518  10.780057  13.62996   16.616978\n 12.82118   11.183642  15.772643  11.951732  19.93778   15.049977\n 17.285845  12.131016  12.397118  13.214692  14.450847  16.95322\n 12.319299  13.187247  13.218388  18.689075  14.06776   10.7268915\n 23.811806  21.689209  13.065192  18.630095  12.285847  20.123579\n 14.211716  24.01732   13.977557  20.558887  16.579115  16.174587\n 18.459545  14.605762  21.376778  21.793821  13.538331  20.091225\n 13.384102  12.048541  15.457827  18.962925  13.821463  15.671626\n 14.273769  14.569287  14.562174  12.201141  20.23384   20.433655\n 19.268394  14.224476  15.177876  20.940939  10.704645  17.459488\n 15.1383295 14.45313   19.389591  16.784916  14.030768  13.59763\n 22.652922  11.666172  12.805294  12.850136  13.4277725 12.910168\n 17.421673  19.75527   18.89515   13.343067  11.808746  15.672265\n 16.490477  18.75407   14.020619  14.480283  14.091494  20.614496\n 16.523487  21.519478  19.589464  21.391321  14.549867  12.475769\n 13.841201  13.074257  12.809661  14.328619  12.006153  20.002235\n 14.874095  20.298298  23.953585  13.819284  12.001026  11.863812\n 14.0260105 18.23893   19.34816   13.715612  11.978175  16.508064\n 18.019886  17.27205   17.681887  20.792206   7.479797  12.068493\n 15.843083  18.192629  20.099794  15.516126  13.804546  14.22072\n 14.655781  13.169205  11.605066  25.258522  24.066967  16.442352\n 18.978304  15.6658325 16.676174  17.356209  13.670201  18.92722\n 20.425928  13.384781  14.637153  26.455418  19.204239  13.635758\n 20.039478  19.009617  11.389256  14.926133  13.241448  15.081426\n 13.355064  12.020761  13.94444   22.279907  20.42253   14.1907425\n 14.158808  21.212353  13.657897  15.065712  14.260591  20.674429\n 14.569053  19.377544   8.8955965 12.8358135 13.666823  15.536509\n 15.144409  14.978248  21.651606  16.283932  12.105813  22.314943\n 16.771746  11.667786  14.180669  13.455253  12.731482  19.393806\n 22.285297  12.406127  12.760515  17.030987  13.880035  10.569064\n 10.958835  16.70464   13.355307  22.781693  17.59282   15.453958\n 14.984098  14.484018  12.94986   12.743044  13.687714  22.00009\n 24.039541  18.819723  12.126568  11.368701  17.91909   18.52325\n  8.224185  17.225315  15.008002  18.370419  11.470739  16.171595\n 22.986843  12.701194  13.718665  14.299316  11.532358  15.10964\n 21.034292  11.606199  16.711405  15.4944725 12.937225  25.632622\n 13.61303   11.7937975 13.715568  20.458523  14.07506   14.221355\n 14.095476  11.59607   22.289991  13.349244 ]", "policy_t": "[[ 0.99165654 -0.8513813  -0.02730864  0.7522085  -0.683233   -0.9391444 ]\n [-0.87806493  0.7135041   0.9728317   0.96410346  0.02339149 -0.85219306]\n [ 0.8740119  -0.98169315 -0.85397446  0.52157176  0.51214015 -0.12071997]\n ...\n [-0.687367   -0.38196433 -0.9706989  -0.9793274  -0.6379152   0.51011014]\n [ 0.7195939   0.8726568  -0.74921817 -0.5517632   0.873652    0.26366305]\n [ 0.22591722 -0.44256508 -0.8461425  -0.328169    0.43794632 -0.9626896 ]]", "td_error": "[0.5103216  0.548728   0.7276187  0.34006023 0.5418043  0.31847048\n 0.73418427 0.19550371 1.5013456  1.1921244  0.26537323 0.2509284\n 0.11168337 1.4634233  0.91337967 0.21426868 2.2443075  0.95587635\n 0.6089902  0.77720547 1.5474772  0.49459314 1.0590277  1.045999\n 0.3011217  0.8921137  1.3524485  0.47862625 1.722621   0.44788742\n 0.3537979  1.1405535  1.4326854  0.79452515 0.23881674 1.9490247\n 0.38695717 1.0884356  1.0363541  0.5498643  2.0204868  1.0912495\n 1.0240345  0.13549519 0.8138499  0.50565195 0.5034547  0.7180834\n 1.6869864  0.31784678 0.6568084  1.722619   0.9675689  0.7268057\n 1.093586   0.46316528 0.61076784 0.45557308 0.51200247 0.7188854\n 0.30757618 2.1907978  0.69024944 0.87682533 0.44544172 2.5828242\n 0.93235016 0.39716434 0.53451824 0.6457958  1.5229273  1.6011629\n 0.32892275 0.5011525  1.1512942  0.49934292 0.3964181  0.8582816\n 0.77455807 0.31401968 1.583828   0.8391552  0.5444088  1.0983782\n 0.800066   0.17289495 1.5164299  0.79435825 0.43240547 0.7149925\n 0.25938797 1.2064624  1.0911694  1.8250885  0.24844933 0.71785927\n 2.9677248  1.0816569  1.201479   0.2529292  0.22263241 0.5072336\n 0.6005783  0.28182602 0.67263794 0.29710388 0.43111992 0.8942542\n 0.31856823 0.7402029  1.032537   0.7573986  3.3349829  0.7181082\n 1.5608773  1.7048683  1.1519222  0.4436674  0.24674845 1.0875416\n 0.9514971  0.95474815 0.16742325 1.1719995  0.543345   0.5080786\n 0.28100967 2.4027176  0.41531944 0.542624   0.31307936 1.5261989\n 0.66157913 1.208704   0.26728725 0.13628864 1.799695   0.20279312\n 0.30372524 0.8388195  0.7190037  0.7772722  0.42324662 0.89037275\n 0.92220736 2.0279465  1.4485493  1.3821154  0.5967293  0.8808489\n 0.27920055 0.3255844  0.7167053  0.94437695 0.25113106 0.4637499\n 0.26692486 0.8271942  0.94076824 1.091825   0.475554   1.7051163\n 1.6170673  0.44061613 0.45855904 1.470087   0.5690031  0.2972951\n 0.73070145 1.758852   0.42371178 0.22657871 1.111948   0.64918613\n 0.35848284 0.34457636 0.5270734  1.4001198  0.4349041  0.7105069\n 0.72924995 0.60996246 0.66460705 0.9563966  1.3565063  1.282793\n 0.7422452  0.44461346 0.32038093 0.7157326  0.6377654  0.4316764\n 0.72065926 0.38996983 0.444458   1.288907   0.33707142 1.2142754\n 1.3633442  1.4396219  0.3425498  1.0199037  0.05677938 0.6352043\n 0.5870724  0.5660205  0.3418827  0.3163271  0.46566868 0.4206934\n 0.24805498 1.2145033  1.2622681  0.9979801  0.40608025 1.7421198\n 1.1292691  0.296381   1.0465212  0.18133736 1.453249   0.24462223\n 0.7844238  0.3158064  1.3546872  0.46759558 0.6516781  0.8197031\n 1.6168389  1.7315884  2.2897787  0.47355652 0.5800214  0.95874596\n 1.0420494  0.34871387 1.1097612  0.52298546 0.43638468 0.43022633\n 0.5691452  0.42394543 0.2683859  0.5883889  0.43573332 0.7280264\n 0.53346634 1.3949695  0.8689723  1.07759    0.6715083  1.1486845\n 0.5668421  0.2645483  1.7093668  0.60215807]", "mean_td_error": 0.8176274299621582, "actor_loss": -15.958595275878906, "critic_loss": 0.5223331451416016, "alpha_loss": -14.692062377929688, "alpha_value": 0.043377961963415146, "target_entropy": -6, "mean_q": 15.444108963012695, "max_q": 28.659643173217773, "min_q": 5.232655048370361, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 21000, "episodes_total": 21, "training_iteration": 21, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_14-59-46", "timestamp": 1587049186, "time_this_iter_s": 86.76033973693848, "time_total_s": 977.683048248291, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 977.683048248291, "timesteps_since_restore": 21000, "iterations_since_restore": 21, "perf": {"cpu_util_percent": 92.26369863013699, "ram_util_percent": 11.5}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -38.667505531257774, "episode_reward_min": -156.05862014467667, "episode_reward_mean": -86.8559295725446, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-99.63022000890962, -99.52322630256288, -45.0444510959159, -69.93526383074712, -62.84807487545902, -81.60533391917124, -115.4380257901446, -38.667505531257774, -99.80857422660112, -156.05862014467667], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.26013916662449255, "mean_processing_ms": 0.11957848918036566, "mean_inference_ms": 1.1777942713927954}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -67.32535743090901, "episode_reward_min": -201.17740332023865, "episode_reward_mean": -130.20053925460522, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-67.32535743090901, -201.17740332023865, -142.76929303405268, -132.53824395239025, -107.19239853543543], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.3364271708239168, "mean_processing_ms": 0.46366812977978933, "mean_inference_ms": 1.6253254316294274}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 11000, "num_steps_trained": 3070720, "num_steps_sampled": 22000, "sample_time_ms": 4.161, "replay_time_ms": 22.692, "grad_time_ms": 60.194, "update_time_ms": 0.005, "opt_peak_throughput": 4252.935, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[13.089125 ]\n [10.669409 ]\n [12.256594 ]\n [11.487167 ]\n [29.18473  ]\n [19.557167 ]\n [17.131779 ]\n [12.804058 ]\n [15.829306 ]\n [17.250284 ]\n [11.7139635]\n [24.895182 ]\n [13.599424 ]\n [11.512594 ]\n [12.87165  ]\n [18.60992  ]\n [14.254973 ]\n [21.168491 ]\n [11.382504 ]\n [13.975321 ]\n [25.085321 ]\n [20.27006  ]\n [18.513899 ]\n [21.663063 ]\n [13.162043 ]\n [13.723479 ]\n [12.476106 ]\n [15.200816 ]\n [20.79189  ]\n [12.603714 ]\n [ 9.936403 ]\n [14.461192 ]\n [12.812898 ]\n [11.612575 ]\n [13.898839 ]\n [12.730106 ]\n [13.796289 ]\n [25.225481 ]\n [12.483409 ]\n [20.83634  ]\n [14.610651 ]\n [11.163573 ]\n [19.48479  ]\n [13.021865 ]\n [12.195362 ]\n [13.942645 ]\n [21.548458 ]\n [12.027931 ]\n [11.056217 ]\n [17.809338 ]\n [11.625693 ]\n [11.961715 ]\n [36.73424  ]\n [18.766916 ]\n [12.177993 ]\n [21.46902  ]\n [10.419462 ]\n [18.851645 ]\n [ 9.098983 ]\n [ 7.456752 ]\n [20.995182 ]\n [17.969446 ]\n [14.263596 ]\n [22.861134 ]\n [14.028399 ]\n [11.990314 ]\n [18.07541  ]\n [12.717509 ]\n [20.45883  ]\n [15.294967 ]\n [19.68896  ]\n [21.873745 ]\n [26.586195 ]\n [12.761228 ]\n [12.955159 ]\n [26.725252 ]\n [12.829869 ]\n [17.49684  ]\n [12.133105 ]\n [12.097842 ]\n [14.285041 ]\n [17.246716 ]\n [14.232463 ]\n [12.164955 ]\n [16.20183  ]\n [20.457157 ]\n [12.031048 ]\n [23.284552 ]\n [13.886777 ]\n [20.80863  ]\n [12.946518 ]\n [21.597181 ]\n [10.9064455]\n [17.340052 ]\n [13.840542 ]\n [13.025813 ]\n [13.352674 ]\n [13.682372 ]\n [19.807941 ]\n [12.754093 ]\n [11.559507 ]\n [10.052219 ]\n [12.527263 ]\n [11.33411  ]\n [21.860945 ]\n [11.030194 ]\n [14.719063 ]\n [15.475368 ]\n [15.261931 ]\n [19.126244 ]\n [28.966175 ]\n [14.168938 ]\n [13.359379 ]\n [12.113502 ]\n [13.224809 ]\n [ 6.2379766]\n [12.676212 ]\n [14.6240635]\n [19.956665 ]\n [13.023079 ]\n [27.882061 ]\n [14.383212 ]\n [ 9.254116 ]\n [10.966979 ]\n [22.164675 ]\n [24.00279  ]\n [12.87981  ]\n [10.199891 ]\n [15.647186 ]\n [12.22619  ]\n [12.1455555]\n [14.255552 ]\n [11.215029 ]\n [15.797319 ]\n [10.656249 ]\n [21.959726 ]\n [24.46158  ]\n [13.393715 ]\n [12.923173 ]\n [ 8.287573 ]\n [12.705426 ]\n [11.450193 ]\n [13.465414 ]\n [19.380388 ]\n [11.384253 ]\n [12.275753 ]\n [ 9.625164 ]\n [ 9.622455 ]\n [13.207509 ]\n [12.271093 ]\n [27.855856 ]\n [15.433368 ]\n [12.608495 ]\n [18.443186 ]\n [26.619322 ]\n [ 9.208589 ]\n [11.861814 ]\n [11.755029 ]\n [18.837566 ]\n [25.278784 ]\n [15.596118 ]\n [12.428643 ]\n [22.143208 ]\n [25.271627 ]\n [22.806162 ]\n [11.270202 ]\n [20.938713 ]\n [12.395306 ]\n [16.710142 ]\n [20.795437 ]\n [19.840645 ]\n [28.966175 ]\n [20.745699 ]\n [11.179518 ]\n [13.434563 ]\n [18.543652 ]\n [23.54434  ]\n [19.501822 ]\n [24.756813 ]\n [13.8935585]\n [26.757872 ]\n [17.18748  ]\n [10.078024 ]\n [29.265266 ]\n [13.036301 ]\n [10.572533 ]\n [12.503942 ]\n [12.286919 ]\n [17.92282  ]\n [25.486898 ]\n [18.787611 ]\n [23.337097 ]\n [19.745752 ]\n [11.203997 ]\n [12.1455555]\n [15.624626 ]\n [12.832595 ]\n [12.203795 ]\n [22.459816 ]\n [14.860254 ]\n [14.976187 ]\n [21.425638 ]\n [22.30195  ]\n [30.888237 ]\n [23.664677 ]\n [ 9.119986 ]\n [19.914917 ]\n [12.165252 ]\n [25.362272 ]\n [12.756181 ]\n [14.845215 ]\n [ 9.55146  ]\n [27.281517 ]\n [15.503159 ]\n [19.39788  ]\n [13.370898 ]\n [19.540873 ]\n [12.730981 ]\n [13.581244 ]\n [15.284053 ]\n [25.89429  ]\n [22.992039 ]\n [26.588383 ]\n [28.774387 ]\n [10.497117 ]\n [12.992133 ]\n [14.170507 ]\n [12.142567 ]\n [10.913807 ]\n [26.634113 ]\n [13.874899 ]\n [21.310457 ]\n [10.541003 ]\n [11.395379 ]\n [14.419083 ]\n [19.329798 ]\n [17.253542 ]\n [22.997921 ]\n [18.678509 ]\n [12.313489 ]\n [20.417124 ]\n [ 8.6556835]\n [15.208737 ]\n [21.310457 ]\n [12.244473 ]\n [10.41374  ]\n [13.424525 ]\n [12.898608 ]\n [24.490679 ]\n [24.054499 ]\n [25.406338 ]\n [13.7594595]\n [11.762929 ]\n [20.357683 ]\n [14.18196  ]\n [14.927963 ]]", "q_t_selected": "[13.089125  10.669409  12.256594  11.487167  29.18473   19.557167\n 17.131779  12.804058  15.829306  17.250284  11.7139635 24.895182\n 13.599424  11.512594  12.87165   18.60992   14.254973  21.168491\n 11.382504  13.975321  25.085321  20.27006   18.513899  21.663063\n 13.162043  13.723479  12.476106  15.200816  20.79189   12.603714\n  9.936403  14.461192  12.812898  11.612575  13.898839  12.730106\n 13.796289  25.225481  12.483409  20.83634   14.610651  11.163573\n 19.48479   13.021865  12.195362  13.942645  21.548458  12.027931\n 11.056217  17.809338  11.625693  11.961715  36.73424   18.766916\n 12.177993  21.46902   10.419462  18.851645   9.098983   7.456752\n 20.995182  17.969446  14.263596  22.861134  14.028399  11.990314\n 18.07541   12.717509  20.45883   15.294967  19.68896   21.873745\n 26.586195  12.761228  12.955159  26.725252  12.829869  17.49684\n 12.133105  12.097842  14.285041  17.246716  14.232463  12.164955\n 16.20183   20.457157  12.031048  23.284552  13.886777  20.80863\n 12.946518  21.597181  10.9064455 17.340052  13.840542  13.025813\n 13.352674  13.682372  19.807941  12.754093  11.559507  10.052219\n 12.527263  11.33411   21.860945  11.030194  14.719063  15.475368\n 15.261931  19.126244  28.966175  14.168938  13.359379  12.113502\n 13.224809   6.2379766 12.676212  14.6240635 19.956665  13.023079\n 27.882061  14.383212   9.254116  10.966979  22.164675  24.00279\n 12.87981   10.199891  15.647186  12.22619   12.1455555 14.255552\n 11.215029  15.797319  10.656249  21.959726  24.46158   13.393715\n 12.923173   8.287573  12.705426  11.450193  13.465414  19.380388\n 11.384253  12.275753   9.625164   9.622455  13.207509  12.271093\n 27.855856  15.433368  12.608495  18.443186  26.619322   9.208589\n 11.861814  11.755029  18.837566  25.278784  15.596118  12.428643\n 22.143208  25.271627  22.806162  11.270202  20.938713  12.395306\n 16.710142  20.795437  19.840645  28.966175  20.745699  11.179518\n 13.434563  18.543652  23.54434   19.501822  24.756813  13.8935585\n 26.757872  17.18748   10.078024  29.265266  13.036301  10.572533\n 12.503942  12.286919  17.92282   25.486898  18.787611  23.337097\n 19.745752  11.203997  12.1455555 15.624626  12.832595  12.203795\n 22.459816  14.860254  14.976187  21.425638  22.30195   30.888237\n 23.664677   9.119986  19.914917  12.165252  25.362272  12.756181\n 14.845215   9.55146   27.281517  15.503159  19.39788   13.370898\n 19.540873  12.730981  13.581244  15.284053  25.89429   22.992039\n 26.588383  28.774387  10.497117  12.992133  14.170507  12.142567\n 10.913807  26.634113  13.874899  21.310457  10.541003  11.395379\n 14.419083  19.329798  17.253542  22.997921  18.678509  12.313489\n 20.417124   8.6556835 15.208737  21.310457  12.244473  10.41374\n 13.424525  12.898608  24.490679  24.054499  25.406338  13.7594595\n 11.762929  20.357683  14.18196   14.927963 ]", "twin_q_t_selected": "[12.620965  11.789514  11.266491  12.254383  28.57882   18.91333\n 17.729273  12.491895  16.341215  16.739727  11.666857  26.474867\n 12.628205  10.6528845 13.112458  18.186657  14.163302  21.084614\n 12.681671  12.686834  26.641582  21.005625  18.279146  21.627558\n 12.416721  13.614643  12.589368  15.259585  21.975855  13.067789\n 10.259253  13.788428  12.3070755 11.384541  14.0632715 13.374279\n 14.384594  25.577312  12.103761  20.741413  13.506413   9.552023\n 20.876043  12.743532  11.78774   13.705919  21.374577  11.181769\n 11.249606  18.616924  11.481855  11.543189  38.28058   18.228243\n 11.630502  20.718992  11.020611  18.50905    7.5556297  8.099059\n 21.36436   17.581474  14.083169  22.288803  14.788599  12.220444\n 18.743652  12.64443   20.274254  13.961637  18.476038  22.379244\n 27.137087  12.173     12.326159  25.87192   12.37348   18.993217\n 12.739303  11.8296585 13.467795  17.233683  14.062986  12.368388\n 16.403973  20.351421  12.706069  23.472672  14.237769  20.395086\n 12.667912  19.379465  11.891324  16.956203  13.011792  12.548757\n 12.11623   13.3221855 20.515612  14.156587  11.652096  10.19079\n 12.159376  11.706688  21.7514    11.098653  15.52574   13.807227\n 14.458134  19.265265  28.262497  13.689142  13.710668  11.390967\n 13.520591   6.8509173 13.021989  16.228022  20.181816  12.208015\n 27.948864  16.033268   9.485342  11.393477  22.054602  23.80723\n 12.051823   9.781852  15.672088  12.146911  11.701125  13.231823\n 11.426486  15.691872  11.353918  21.698433  24.607183  13.316801\n 12.678755   9.118754  12.874851  11.861276  14.300439  20.114267\n 11.543417  11.848591   9.769143   9.777755  13.1869335 12.85568\n 28.291245  16.185297  12.431208  19.44124   26.986624   8.357976\n 11.921239  11.351439  18.71907   24.212645  15.90318   12.515226\n 22.41696   25.555408  22.964733  11.527454  20.260456  10.557071\n 16.95899   22.280449  19.150208  28.262497  20.568607  10.57456\n 14.26482   19.478247  24.163042  19.233328  24.469374  14.316836\n 27.047256  17.880602   9.341821  28.3265    13.352473  10.747063\n 12.457036  11.456128  18.175793  25.87033   19.689445  23.503304\n 19.50248   10.724831  11.701125  14.1702795 13.535735  12.623633\n 22.888914  14.68092   14.2287855 20.357714  23.308634  31.081028\n 23.38449    9.757171  20.99883   11.78846   24.560068  12.356671\n 14.155795  10.695507  26.470371  15.431641  19.776833  12.355359\n 18.664835  12.3797    14.232586  16.123394  24.818663  22.227848\n 26.458172  28.64892    9.867228  12.425252  14.568293  12.2427\n  9.745773  25.178074  13.640109  21.712553  10.063941  10.448779\n 14.439096  19.151705  18.81125   22.15555   18.041307  12.258028\n 20.795233   7.9895825 14.975895  21.712553  12.138851   9.6767435\n 13.573775  13.848127  23.735912  24.056929  24.391846  14.916869\n 11.213725  20.588028  14.078758  15.615829 ]", "q_t_selected_target": "[13.360759  12.1155205 11.767198  11.208621  30.407835  20.693615\n 18.04388   12.768421  17.158903  15.79884   10.527755  25.782547\n 12.184091  10.734462  13.735235  19.638617  15.691693  20.66225\n 10.574114  13.610857  26.650743  21.334534  19.719658  20.911266\n 11.866539  14.3854685 12.315698  13.260231  23.199614  13.258229\n  9.571409  14.270971  11.819949  10.956636  12.100008  13.13941\n 14.397934  25.230608  11.226111  23.285892  13.393477  11.693757\n 19.549055  11.833929  11.880309  13.675488  22.663237  11.513089\n 11.56706   17.882061  10.881494  12.235324  38.889557  17.500488\n 12.5383625 19.077068  11.159319  19.734781   8.378918   7.0185146\n 21.47094   19.581034  14.261295  23.636467  14.793797  13.244673\n 20.836187  14.058859  19.36198   13.780411  19.230883  22.811153\n 26.27977   11.933422  12.913417  26.307749  12.140968  17.690191\n 10.920766  12.369742  13.398376  17.683308  14.674847  10.641194\n 14.470036  21.366018  12.77431   23.880154  13.062601  19.857414\n 12.254387  20.850357  11.347088  15.682819  14.306329  13.346904\n 11.469098  14.159859  20.720968  13.524321  12.665004  11.64586\n 12.985413  11.618378  21.827724  11.771317  14.708999  13.966145\n 15.234781  19.390842  27.741127  12.466035  14.611209  12.235577\n 13.628093   4.1266685 13.772319  14.2985325 21.386484  12.081809\n 28.063305  13.269989  10.291092  10.641774  22.375275  23.080397\n 12.968551  10.29188   15.298209  10.744794  13.00364   12.635841\n 10.487681  15.867514  11.757169  20.798285  23.408936  14.523327\n 12.4780445  7.7978225 11.488079  10.034098  13.234089  17.994816\n 11.533177  12.87915   10.030552   8.596634  12.661874  11.194294\n 28.606403  13.830907  11.488196  21.007063  28.899515   9.331131\n 11.781366  11.623286  19.228567  24.822943  16.546507  13.027546\n 20.911362  24.613894  24.315668  12.38546   19.396883  10.789242\n 14.929425  22.137733  19.560087  27.736368  19.9641    11.165373\n 15.397099  17.522697  22.29311   18.40021   23.962027  12.774186\n 27.698568  19.401049  10.531631  28.978447  11.910828  10.244277\n 13.146892  12.988003  16.879372  25.330908  20.181946  23.648754\n 20.01964   11.301148  12.803866  14.364319  12.139741  11.017185\n 22.930162  14.114683  16.814548  20.63091   24.671324  32.026833\n 24.983807   9.245152  21.927567  10.271823  24.02617   12.664358\n 16.833387   9.364623  26.52562   15.655424  16.649635  12.886068\n 20.618504  12.834689  13.539942  14.934668  25.640583  23.790524\n 25.76768   28.12222   10.67462   13.14359   13.180638  12.089477\n 10.297656  26.684113  13.843571  22.122002  10.327114  12.131591\n 13.421378  20.316729  18.293781  21.629585  18.463165  11.927597\n 21.859795   6.894417  16.713331  22.361683  13.516626  10.678877\n 14.04135   12.686006  23.00273   24.278326  24.946472  14.729932\n 10.897739  20.587114  13.632954  16.243225 ]", "q_tp1_best_masked": "[13.898125  13.094814  12.95243   11.883446  29.895758  20.473774\n 19.240808  13.916421  17.226316  15.106102  11.051459  26.323477\n 12.401725  11.321397  14.257144  20.550468  15.903202  20.364153\n 10.942363  14.641695  26.278807  21.556099  19.057566  22.094097\n 11.512215  13.8306875 12.282222  14.348514  22.50821   14.454661\n 10.922725  14.596468  12.495177  10.620329  12.021142  13.170304\n 13.536178  25.225634  11.485211  22.954338  13.581115  13.161869\n 20.508514  12.632109  12.777276  13.818399  23.099476  11.235152\n 11.311903  18.836094  11.5947    12.661094  37.7964    17.78078\n 13.075739  19.44085   11.3216915 19.510313  10.706853   8.348124\n 21.790861  19.497492  14.343167  23.236486  14.976062  13.831548\n 21.089317  14.897866  19.71385   13.979217  20.781017  23.484648\n 26.093384  12.004849  12.989497  26.580416  12.760336  17.985064\n 12.21174   11.871594  13.387572  19.135414  13.776302  10.489302\n 15.977074  21.141848  13.279356  24.00296   12.572153  20.251974\n 12.739534  22.335598  11.843326  15.297731  15.774986  13.606514\n 11.423968  14.315132  21.013805  13.222672  13.631535  12.53588\n 13.995517  12.142987  23.00246   13.074925  14.890381  14.423837\n 16.461946  20.079918  27.211788  12.4710655 14.106856  13.24748\n 13.670665   6.4654007 14.575014  14.099547  22.051764  13.446466\n 28.691181  14.688344  11.397038  11.614968  22.63398   22.850807\n 13.872879  10.839509  15.053481  12.312039  14.293215  12.588342\n 10.506371  15.089307  12.424721  21.81521   23.258467  15.646166\n 12.451234   8.07079   12.761499  10.438918  13.665658  18.624704\n 11.998689  12.2422495 10.870156  10.298872  13.315059  12.471441\n 29.175423  14.147719  12.501902  21.998453  29.343565  11.704098\n 12.969695  12.226818  18.085236  24.685022  16.636698  13.298701\n 20.649338  23.746254  24.662518  13.221649  20.267044  11.300099\n 15.912796  22.39185   19.853584  27.206982  20.06655   12.1123085\n 16.529001  16.860485  21.787012  18.264854  23.482618  13.221567\n 26.971846  19.505587  12.467394  28.25874   11.919661  10.64495\n 13.239114  14.592168  17.164988  24.887566  20.94406   23.353006\n 20.899328  11.320369  14.091423  14.101433  13.254359  11.329402\n 22.717607  15.963458  17.314163  21.670792  24.36446   31.91923\n 24.584097   9.440687  22.312769  10.492798  23.757391  13.375425\n 17.069048  10.480244  26.532438  14.123517  16.890028  12.634453\n 21.184061  12.78725   13.889672  15.487935  26.515411  23.18022\n 25.514057  28.040049  11.179234  13.569251  12.808693  12.510917\n 10.46265   25.41528   13.388848  23.041216  11.04033   12.727688\n 13.67167   20.472322  19.300089  20.708792  18.959703  12.279899\n 22.328737   8.896967  17.187399  23.28332   13.3148775 11.479222\n 13.876526  13.115613  23.711294  23.823126  24.996365  13.847804\n 11.067679  20.963818  13.397153  16.552914 ]", "policy_t": "[[ 0.3729955  -0.20564151 -0.9741265  -0.62602425  0.87149906  0.63011074]\n [-0.7546865   0.257141   -0.9088755  -0.5392343   0.4796486  -0.7931938 ]\n [-0.7673831   0.63288856  0.87318873 -0.90340537  0.02256525 -0.5458178 ]\n ...\n [-0.7979525  -0.83284533  0.00452065  0.9378679   0.83057106  0.9790901 ]\n [ 0.62837064  0.9149728   0.71012795  0.96966267  0.73030686 -0.37520397]\n [ 0.8437474   0.697454    0.9944916   0.9907055   0.96744156  0.9870038 ]]", "td_error": "[0.50571394 0.8860593  0.49505138 0.6621542  1.5260601  1.4583664\n 0.6133547  0.15608168 1.0736427  1.196166   1.1626554  0.7898426\n 0.9297242  0.42985487 0.7431812  1.2403278  1.4825554  0.46430206\n 1.457974   0.64424324 0.7872915  0.6966915  1.3231354  0.7340441\n 0.922843   0.7164073  0.21703911 1.9699697  1.8157415  0.42247772\n 0.5264187  0.3363819  0.74003744 0.54192114 1.8810472  0.32208633\n 0.30749226 0.17591572 1.0674734  2.497016   0.6650548  1.335959\n 0.69562626 1.0487695  0.20381117 0.1487937  1.2017193  0.42308092\n 0.4141488  0.40379333 0.6722808  0.482872   1.3821468  0.9970913\n 0.6341152  2.0169382  0.43928242 1.0544338  0.77167654 0.75939083\n 0.2911682  1.8055735  0.0902133  1.0614986  0.3852973  1.1392941\n 2.4266558  1.3778892  1.0045614  0.84789085 0.6064606  0.684659\n 0.58187103 0.5336919  0.31450033 0.42666626 0.46070623 0.748189\n 1.5154381  0.40599203 0.47804165 0.44310856 0.527122   1.6254773\n 1.8328657  0.96172905 0.4057517  0.5015421  0.99967194 0.7444439\n 0.55282784 1.1088581  0.49243927 1.4653082  0.88016176 0.55961895\n 1.2653537  0.6575799  0.5591917  0.70124674 1.0592022  1.5243549\n 0.6420932  0.18628883 0.05477238 0.70689297 0.41340256 0.8340702\n 0.40189886 0.19508839 0.873209   1.4630051  1.0761857  0.48334217\n 0.25539303 2.4177785  0.92321825 1.1275101  1.3172436  0.53373814\n 0.1478424  1.938251   0.9213629  0.53845406 0.26563644 0.8246136\n 0.5027342  0.30100822 0.36142778 1.4417562  1.0802999  1.1078463\n 0.833076   0.12291813 0.7520852  1.0307951  1.1254463  1.1680689\n 0.32291937 0.90534115 1.3020597  1.6216369  0.64883757 1.752512\n 0.07958221 0.81697845 0.33339834 1.1034708  0.53534746 1.369093\n 0.5328531  1.9784255  1.0316548  2.0648499  2.0965424  0.5478487\n 0.11015987 0.20179462 0.45024872 0.5330696  0.79685783 0.55561113\n 1.368722   0.79962444 1.4302206  0.9866319  1.2027016  0.91911745\n 1.9051409  0.742506   0.34521866 0.87796783 0.69305325 0.3024788\n 1.5474072  1.4882517  1.5605812  0.96736526 0.6510668  1.3310113\n 0.7960043  1.8670073  0.82170916 0.46938324 1.2835593  0.41552067\n 0.6664028  1.1164794  1.1699343  0.3477068  0.94341755 0.22855377\n 0.39552402 0.33673477 0.88052607 0.7271733  1.044424   1.3965292\n 0.2557974  0.6559038  2.2120624  0.53396225 1.8660316  1.0422001\n 1.4592237  0.31859255 1.4706926  1.7050328  0.93499947 0.19975471\n 2.3328824  0.7588606  0.4055729  0.18802452 2.9377213  0.5077696\n 1.5156498  0.27934885 0.3669734  0.76905584 0.53781414 1.1805801\n 0.75559807 0.5894346  0.49244738 0.43489742 1.1887617  0.10315657\n 0.5840168  0.77801895 0.11739492 0.6104965  0.23853111 1.2095118\n 1.0077114  1.0759773  0.77885437 0.94715023 0.31860065 0.35816145\n 1.2536163  1.4282162  1.6210151  0.85017776 1.324964   0.63363504\n 0.5422001  0.6873622  1.1105661  0.22261238 0.507246   0.57870483\n 0.5905876  0.11517239 0.49740553 0.97132874]", "mean_td_error": 0.862372636795044, "actor_loss": -16.75823974609375, "critic_loss": 0.5557482242584229, "alpha_loss": -13.54481315612793, "alpha_value": 0.03390411287546158, "target_entropy": -6, "mean_q": 16.331439971923828, "max_q": 36.7342414855957, "min_q": 6.237976551055908, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 22000, "episodes_total": 22, "training_iteration": 22, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_15-01-27", "timestamp": 1587049287, "time_this_iter_s": 85.95246005058289, "time_total_s": 1063.635508298874, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 1063.635508298874, "timesteps_since_restore": 22000, "iterations_since_restore": 22, "perf": {"cpu_util_percent": 92.33402777777776, "ram_util_percent": 11.5}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -21.582742493025513, "episode_reward_min": -159.97641005151073, "episode_reward_mean": -96.3185725976908, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-104.02105328116475, -159.97641005151073, -98.82177222842124, -103.5710525606279, -81.76187949916688, -110.13750510467816, -21.582742493025513, -125.02338789865543, -42.88882234516263, -115.40110051449479], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.25953774280960695, "mean_processing_ms": 0.11926935558759602, "mean_inference_ms": 1.1760182797061591}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -67.32535743090901, "episode_reward_min": -142.76929303405268, "episode_reward_mean": -111.48223528404094, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-107.58588346741732, -142.76929303405268, -132.53824395239025, -107.19239853543543, -67.32535743090901], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.3391422929226929, "mean_processing_ms": 0.46625033911186, "mean_inference_ms": 1.6410987481506425}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 11500, "num_steps_trained": 3326720, "num_steps_sampled": 23000, "sample_time_ms": 2.818, "replay_time_ms": 20.763, "grad_time_ms": 53.666, "update_time_ms": 0.004, "opt_peak_throughput": 4770.206, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[10.402773 ]\n [11.211554 ]\n [10.777955 ]\n [13.765614 ]\n [16.360388 ]\n [20.856075 ]\n [ 5.9458804]\n [11.714204 ]\n [14.044776 ]\n [19.992367 ]\n [20.913801 ]\n [13.4789715]\n [11.985775 ]\n [26.42703  ]\n [19.517693 ]\n [10.3025   ]\n [16.835144 ]\n [ 5.082308 ]\n [10.246882 ]\n [19.755129 ]\n [19.94563  ]\n [10.693573 ]\n [13.847131 ]\n [27.6619   ]\n [13.099576 ]\n [24.549807 ]\n [16.269402 ]\n [18.596422 ]\n [12.628481 ]\n [16.965872 ]\n [13.484832 ]\n [11.112359 ]\n [12.556972 ]\n [11.703048 ]\n [11.781568 ]\n [10.767466 ]\n [27.519482 ]\n [13.003167 ]\n [16.910473 ]\n [12.335594 ]\n [13.776573 ]\n [21.94734  ]\n [17.285631 ]\n [14.954081 ]\n [11.20184  ]\n [16.942266 ]\n [10.419984 ]\n [22.713196 ]\n [12.080545 ]\n [19.144238 ]\n [15.511666 ]\n [19.481455 ]\n [11.608542 ]\n [22.964281 ]\n [12.6970415]\n [19.460829 ]\n [10.169926 ]\n [13.22028  ]\n [10.926289 ]\n [12.019975 ]\n [19.67689  ]\n [10.778672 ]\n [21.700888 ]\n [10.29794  ]\n [22.873041 ]\n [10.863084 ]\n [17.946972 ]\n [13.538709 ]\n [12.534022 ]\n [13.209085 ]\n [11.127987 ]\n [11.979288 ]\n [18.392326 ]\n [ 8.365199 ]\n [29.200357 ]\n [12.123636 ]\n [12.94609  ]\n [22.282343 ]\n [11.467742 ]\n [11.825335 ]\n [18.29087  ]\n [22.651127 ]\n [10.884044 ]\n [25.251486 ]\n [19.749079 ]\n [10.4737835]\n [23.312042 ]\n [12.595534 ]\n [11.169858 ]\n [10.932328 ]\n [18.514265 ]\n [10.531511 ]\n [12.409956 ]\n [11.127567 ]\n [16.164314 ]\n [23.870602 ]\n [14.431261 ]\n [13.675856 ]\n [27.248156 ]\n [31.432022 ]\n [16.056726 ]\n [11.390421 ]\n [17.222792 ]\n [11.4830065]\n [13.439082 ]\n [18.253654 ]\n [11.223946 ]\n [12.966457 ]\n [19.708185 ]\n [11.054592 ]\n [22.595835 ]\n [13.969313 ]\n [16.23434  ]\n [10.652132 ]\n [28.667847 ]\n [10.784036 ]\n [23.447542 ]\n [18.665396 ]\n [22.189072 ]\n [19.251762 ]\n [14.1185875]\n [10.244568 ]\n [ 8.207577 ]\n [ 8.393977 ]\n [11.670963 ]\n [20.763546 ]\n [15.31388  ]\n [12.930854 ]\n [13.300502 ]\n [12.071412 ]\n [12.482506 ]\n [24.511078 ]\n [11.25452  ]\n [12.635425 ]\n [13.48992  ]\n [18.634083 ]\n [10.436581 ]\n [11.697879 ]\n [15.287093 ]\n [15.442371 ]\n [23.174135 ]\n [20.537804 ]\n [22.761677 ]\n [18.018654 ]\n [17.200136 ]\n [14.359115 ]\n [23.640278 ]\n [21.528568 ]\n [13.450981 ]\n [11.422253 ]\n [12.145833 ]\n [21.154696 ]\n [10.189365 ]\n [16.880833 ]\n [12.524298 ]\n [14.849721 ]\n [10.419984 ]\n [11.035431 ]\n [22.82936  ]\n [11.813959 ]\n [11.456232 ]\n [10.997477 ]\n [12.676094 ]\n [19.996275 ]\n [21.641657 ]\n [11.413981 ]\n [12.8543415]\n [13.963589 ]\n [12.044655 ]\n [11.902968 ]\n [23.656538 ]\n [21.092175 ]\n [19.955652 ]\n [10.450772 ]\n [ 9.743134 ]\n [ 7.5866427]\n [ 9.956327 ]\n [11.714204 ]\n [ 9.956327 ]\n [ 9.953797 ]\n [26.73389  ]\n [13.973091 ]\n [12.291    ]\n [12.895971 ]\n [18.175516 ]\n [10.246236 ]\n [ 9.402393 ]\n [10.851299 ]\n [12.429429 ]\n [19.43798  ]\n [17.379045 ]\n [ 9.996204 ]\n [15.533301 ]\n [11.259257 ]\n [14.78743  ]\n [25.67586  ]\n [20.05845  ]\n [20.743484 ]\n [13.723667 ]\n [17.264929 ]\n [11.765955 ]\n [11.81831  ]\n [15.354903 ]\n [27.21396  ]\n [20.157326 ]\n [23.742424 ]\n [15.145269 ]\n [ 8.96593  ]\n [22.423613 ]\n [18.232494 ]\n [24.651316 ]\n [12.258455 ]\n [19.466951 ]\n [12.333423 ]\n [14.481642 ]\n [18.315247 ]\n [11.5503645]\n [19.84091  ]\n [12.005414 ]\n [11.408629 ]\n [12.693155 ]\n [ 9.687774 ]\n [12.998546 ]\n [20.41478  ]\n [12.5101795]\n [12.183522 ]\n [25.789156 ]\n [11.371258 ]\n [24.558546 ]\n [12.123737 ]\n [10.120142 ]\n [12.653237 ]\n [10.760416 ]\n [18.263905 ]\n [17.290026 ]\n [11.058221 ]\n [22.481745 ]\n [16.525343 ]\n [20.994059 ]\n [11.479035 ]\n [30.47669  ]\n [16.740131 ]\n [11.280443 ]\n [ 8.006344 ]\n [12.273586 ]\n [13.029921 ]\n [11.210681 ]\n [11.2700615]\n [15.588879 ]\n [11.60985  ]\n [10.801009 ]\n [14.598922 ]\n [20.304039 ]\n [12.552572 ]\n [26.288475 ]\n [13.157144 ]]", "q_t_selected": "[10.402773  11.211554  10.777955  13.765614  16.360388  20.856075\n  5.9458804 11.714204  14.044776  19.992367  20.913801  13.4789715\n 11.985775  26.42703   19.517693  10.3025    16.835144   5.082308\n 10.246882  19.755129  19.94563   10.693573  13.847131  27.6619\n 13.099576  24.549807  16.269402  18.596422  12.628481  16.965872\n 13.484832  11.112359  12.556972  11.703048  11.781568  10.767466\n 27.519482  13.003167  16.910473  12.335594  13.776573  21.94734\n 17.285631  14.954081  11.20184   16.942266  10.419984  22.713196\n 12.080545  19.144238  15.511666  19.481455  11.608542  22.964281\n 12.6970415 19.460829  10.169926  13.22028   10.926289  12.019975\n 19.67689   10.778672  21.700888  10.29794   22.873041  10.863084\n 17.946972  13.538709  12.534022  13.209085  11.127987  11.979288\n 18.392326   8.365199  29.200357  12.123636  12.94609   22.282343\n 11.467742  11.825335  18.29087   22.651127  10.884044  25.251486\n 19.749079  10.4737835 23.312042  12.595534  11.169858  10.932328\n 18.514265  10.531511  12.409956  11.127567  16.164314  23.870602\n 14.431261  13.675856  27.248156  31.432022  16.056726  11.390421\n 17.222792  11.4830065 13.439082  18.253654  11.223946  12.966457\n 19.708185  11.054592  22.595835  13.969313  16.23434   10.652132\n 28.667847  10.784036  23.447542  18.665396  22.189072  19.251762\n 14.1185875 10.244568   8.207577   8.393977  11.670963  20.763546\n 15.31388   12.930854  13.300502  12.071412  12.482506  24.511078\n 11.25452   12.635425  13.48992   18.634083  10.436581  11.697879\n 15.287093  15.442371  23.174135  20.537804  22.761677  18.018654\n 17.200136  14.359115  23.640278  21.528568  13.450981  11.422253\n 12.145833  21.154696  10.189365  16.880833  12.524298  14.849721\n 10.419984  11.035431  22.82936   11.813959  11.456232  10.997477\n 12.676094  19.996275  21.641657  11.413981  12.8543415 13.963589\n 12.044655  11.902968  23.656538  21.092175  19.955652  10.450772\n  9.743134   7.5866427  9.956327  11.714204   9.956327   9.953797\n 26.73389   13.973091  12.291     12.895971  18.175516  10.246236\n  9.402393  10.851299  12.429429  19.43798   17.379045   9.996204\n 15.533301  11.259257  14.78743   25.67586   20.05845   20.743484\n 13.723667  17.264929  11.765955  11.81831   15.354903  27.21396\n 20.157326  23.742424  15.145269   8.96593   22.423613  18.232494\n 24.651316  12.258455  19.466951  12.333423  14.481642  18.315247\n 11.5503645 19.84091   12.005414  11.408629  12.693155   9.687774\n 12.998546  20.41478   12.5101795 12.183522  25.789156  11.371258\n 24.558546  12.123737  10.120142  12.653237  10.760416  18.263905\n 17.290026  11.058221  22.481745  16.525343  20.994059  11.479035\n 30.47669   16.740131  11.280443   8.006344  12.273586  13.029921\n 11.210681  11.2700615 15.588879  11.60985   10.801009  14.598922\n 20.304039  12.552572  26.288475  13.157144 ]", "twin_q_t_selected": "[10.398555  11.425497  11.680774  13.194681  15.707824  21.7553\n  6.6207476 11.857187  14.096927  18.407091  21.149754  12.868738\n 11.323336  26.328962  19.441393  10.921945  16.342646   6.319886\n 10.256633  18.950863  20.518358   8.536484  13.236561  26.993118\n 12.592781  24.60911   14.872226  18.524296  12.522434  17.026503\n 13.276925  11.512728  12.880046  11.432804  12.1749     9.809458\n 28.016249  12.098699  16.827158  11.297542  13.510587  22.115723\n 17.39807   15.133052  10.938279  17.525663  10.776483  23.17379\n 12.653543  18.924292  15.001226  19.539822  10.379534  22.857683\n 12.119232  18.940933  10.837086  12.124447  10.873069  11.545606\n 18.650156  11.229437  20.89418   10.22765   23.524036  10.648301\n 18.168417  13.023448  11.635212  13.759474  12.072327  11.847657\n 17.674873   8.6135645 29.398851  11.205162  12.115666  20.491428\n 12.05976   10.839798  19.15948   23.145939  10.005547  25.115425\n 19.650776  10.437253  23.580286  11.9368305 10.995817  10.917173\n 18.727514  11.0356045 12.312902  11.001424  16.562584  21.99971\n 14.132539  12.951724  27.148745  31.77176   16.440401  11.558924\n 17.675419  10.933621  14.331955  19.886837  10.587921  12.092184\n 19.006979  10.909614  22.009165  14.5792885 19.062395  10.518652\n 28.057774  11.573107  24.2032    19.842703  22.821575  19.230698\n 14.754704  11.683151   7.6703324  8.506556  12.098005  20.245955\n 14.693251  12.230875  12.791569  12.125634  11.555887  25.755796\n 11.895051  12.656457  12.4541645 19.25175   10.750489  10.967458\n 15.147533  15.065666  21.108967  20.827892  22.76487   19.978636\n 17.32067   12.622765  24.192951  20.831661  14.444415  11.653042\n 11.700834  21.595133   9.4344    17.217964  13.243079  15.593526\n 10.776483  11.938597  21.665249  10.567818  10.783963  11.245839\n 12.695685  18.70835   21.35136   10.824188  13.335768  13.828426\n 12.509145  12.172893  23.882778  21.41918   20.62761   10.761075\n 10.2497015  7.4633727  9.699073  11.857187   9.699073  10.437038\n 25.70399   13.764796  12.557113  13.173861  18.293676  10.820407\n  8.923168  11.330033  12.769255  20.39639   17.014261   9.89199\n 15.31071   10.657051  15.209229  26.187178  20.828108  21.174599\n 13.786302  17.809345  13.063704  11.343971  16.52151   25.32408\n 20.389536  22.946096  13.707905   9.277855  21.412231  18.679716\n 23.370312  12.429637  20.313885  11.893516  14.661467  17.09265\n 11.34536   19.834785  11.748793  11.798799  12.1564     8.448978\n 13.689792  19.872616  12.699295  12.5238    24.728914  10.886165\n 24.068285  12.0869     9.599723  12.086835  11.60074   18.516874\n 17.828222  11.180456  22.151037  15.66171   22.637905  10.435557\n 31.226883  16.288889  11.724704   7.7438645 12.373849  12.475145\n 11.081259  10.501446  16.057932  11.461687  10.285519  15.362674\n 21.312056  11.938225  25.310978  12.727208 ]", "q_t_selected_target": "[11.154294  11.912231  10.372752  13.636791  17.395077  22.348698\n  5.6571517 12.43071   14.275052  19.002117  22.159878  11.061351\n  9.879514  26.641512  16.54706   10.671705  16.034628   3.543146\n 10.546888  20.290047  21.477182   9.242408  13.730732  24.496836\n 13.021573  22.072296  16.026712  18.513159  11.9805975 18.445778\n 13.890836  11.644586  13.3723135 10.883585  11.478099  11.872429\n 28.27082   12.950318  16.851353  11.186704  14.224132  21.108234\n 17.425613  13.541122  12.110273  17.274437   9.824763  21.96509\n 12.078257  19.884026  17.273458  18.696665  12.165802  22.697695\n 13.347137  19.715952   8.690709  12.317377   9.790745  11.358649\n 19.808332  10.011284  21.065384   9.153713  22.654379   9.954457\n 19.509447  14.745725  10.581877  13.756941   9.471483  10.278692\n 17.165764   6.1046457 30.320568  11.5604    12.231738  20.111263\n 10.979674  10.87071   17.683376  22.194191   9.512248  23.71621\n 20.623016   9.745761  23.420418  11.742201  11.224745  10.871934\n 18.541847  11.843197  11.797706   8.990041  15.046677  24.231865\n 13.905731  12.555069  27.192904  32.828403  16.60951   10.379595\n 17.673758  11.018368  12.395958  18.21891   10.748379  13.543279\n 19.75859   10.327105  22.49816   10.975534  16.254204  11.810732\n 25.596516  11.325106  23.45138   18.62175   21.696259  19.887506\n 12.729507   9.79588    6.4309616  6.9515615 11.840885  20.043165\n 12.669708  12.741743  15.025015  12.339088  12.038781  24.913462\n  9.47323   10.355973  12.206056  18.670824  10.157305  10.407521\n 16.7713    12.539017  21.436054  19.799892  21.884834  20.212801\n 15.23895   13.74383   24.418514  21.225487  12.3229    12.506093\n 12.114903  21.571247   9.000208  17.676292  12.976596  13.485226\n  9.697577  12.169379  24.183117  11.71377   11.0849085 10.176749\n 13.439437  18.691233  19.569675  10.350751  13.09836   12.49334\n 12.175521  11.68686   25.12106   20.403706  18.512451  10.260481\n  8.995085   6.643595   9.807576  12.406493   9.899507  11.35291\n 24.838081  12.12615   12.631251  11.804336  19.139355  10.919205\n 10.235537   9.856576  11.913233  19.304455  18.85737    9.504164\n 15.844144  10.859318  15.005112  27.52657   19.153814  19.093674\n 12.231473  18.035992  13.183256  10.777214  13.806717  25.127102\n 21.026846  24.228111  15.759124   8.496236  22.703661  18.272074\n 23.75516   13.009424  18.460085  11.218532  15.112199  16.81541\n 11.129847  19.67192   11.979785  10.296662  11.617646   7.795631\n 12.328919  20.263996  13.995891  11.29156   24.19669   10.4555435\n 25.022638  12.199914   9.553459  11.825672  10.494707  19.32207\n 16.958986  11.96153   23.891872  16.570171  19.987188  11.200042\n 29.468029  17.89148   10.518134   6.1875224 11.84054   13.012024\n 10.810944  11.339656  16.23855   13.11952   10.506446  14.100367\n 23.540096  12.471838  26.260876  13.0781355]", "q_tp1_best_masked": "[11.748332  12.430902  11.315584  13.728044  19.31273   22.705433\n  7.370539  12.217993  13.763698  21.081615  22.864567  11.66562\n 10.410362  26.87604   17.486444  11.483519  16.256329   5.9136467\n 12.0077095 19.91617   21.901861   9.648341  14.617405  24.986732\n 13.656858  22.016216  16.689405  18.906416  12.038799  19.130701\n 13.556588  11.761013  13.895884  11.089596  12.2169285 12.881084\n 28.25354   13.428318  17.25549   10.911433  13.660895  21.708178\n 19.355633  13.3463125 13.224427  17.266619  10.772156  22.053436\n 11.54104   19.678732  19.436901  18.84344   13.192589  21.913168\n 13.552866  20.018808   9.825353  12.708892  11.095013  11.7425165\n 20.346672  10.727158  20.933353  10.0804405 21.56278   11.28785\n 20.32885   16.185297   9.935824  13.227955  10.885116  11.039816\n 17.60527    6.7181454 29.657011  10.939335  11.194306  20.053602\n 11.130326  11.869969  18.11417   22.638535  11.231262  23.912144\n 21.673853  10.585135  23.33029   13.030543  11.702928  11.820711\n 19.262299  12.28781   11.00186    9.825577  16.676731  25.076889\n 13.347151  12.947246  27.07418   32.06314   17.259869  11.32501\n 18.180742  11.016302  13.254245  18.519123  10.947562  12.879948\n 19.206257  11.257912  21.324972  11.324922  16.626127  12.123239\n 24.70311   11.631975  23.183767  18.44504   21.62349   19.997309\n 13.666618   9.877516   8.055463   8.221219  11.907481  20.197039\n 12.549793  13.206238  15.681838  12.026574  11.98525   23.659035\n 10.984852  10.805553  12.262873  20.04006   11.075222  11.319139\n 16.650585  13.361709  21.43684   19.904549  21.661034  20.009077\n 16.13056   12.663842  24.660547  22.672249  11.167801  12.453682\n 12.626787  22.176645  10.473004  17.507284  13.320636  13.5211935\n 10.643685  11.708529  24.707802  12.476659  11.525951  10.980606\n 13.405158  19.520573  19.663103  10.744935  13.686559  13.3656025\n 13.358126  12.111289  25.049612  18.404623  18.973856  11.140267\n 10.451575   8.192137  10.77638   12.193531  10.869239  11.768437\n 25.252396  11.990382  12.323047  12.103174  19.403875  11.374297\n 11.280198  10.224432  13.185671  19.684649  19.73326   10.62135\n 17.77876   10.181822  16.93254   27.891462  18.583378  18.992481\n 12.187362  18.57365   13.049241  11.536305  15.144356  25.138165\n 20.81543   26.272186  16.695972  10.291057  22.589443  19.199463\n 23.765512  13.68207   19.636839  12.655185  14.575438  17.461275\n 11.184132  20.114155  12.406091  10.997008  12.721322   9.173452\n 13.604726  20.602985  14.7424135 12.1761675 23.890701  10.693344\n 25.479656  11.279455   9.763921  11.994216  11.3474245 19.930191\n 16.569035  12.225125  24.668186  16.940573  21.093761  12.231309\n 28.504757  18.73044   10.982282   6.658129  12.026499  12.589735\n 10.972793  11.25514   16.661491  13.70657   11.022769  13.561248\n 23.65532   13.002999  26.739277  13.333528 ]", "policy_t": "[[ 0.7413783  -0.28794336  0.8025751   0.21756923 -0.97582054  0.6199908 ]\n [ 0.8431548  -0.3235054   0.92090976  0.8406668  -0.9684262   0.52728915]\n [ 0.12781274 -0.9360581  -0.96993524  0.16761506  0.11998832  0.88735414]\n ...\n [ 0.91520417 -0.0400672  -0.84567916 -0.8108587  -0.19207895  0.02060521]\n [ 0.997499   -0.94257635 -0.838734   -0.5083266  -0.6493665  -0.99805665]\n [-0.6718271   0.7491722  -0.04444987  0.8919605  -0.1822452   0.61559606]]", "td_error": "[0.75363016 0.59370613 0.8566122  0.2854662  1.360971   1.0430098\n 0.6261623  0.6450143  0.20420074 0.7926378  1.1281004  2.112504\n 1.7750416  0.26351547 2.9324827  0.30972242 0.5542669  2.157951\n 0.29513073 0.9370508  1.2451887  1.0785446  0.30528498 2.8306732\n 0.25339746 2.507162   0.6985879  0.0472002  0.5948601  1.4495907\n 0.5099573  0.33204222 0.6538048  0.68434095 0.50013494 1.5839672\n 0.50295544 0.45223427 0.04165745 0.6298642  0.5805516  0.9232969\n 0.08376312 1.5024438  1.0402136  0.29169846 0.7734699  0.9784031\n 0.28878784 0.849761   2.0170121  0.8139734  1.1717639  0.21328735\n 0.9390006  0.5150709  1.8127966  0.5479164  1.1089339  0.42414093\n 0.6448097  0.9927707  0.4033537  1.1090817  0.5441599  0.8012352\n 1.4517527  1.4646463  1.5027404  0.27519417 2.1286736  1.6347804\n 0.867836   2.384736   1.0209637  0.4592371  0.41521168 1.2756224\n 0.7840767  0.4927683  1.0417986  0.7043419  0.9325471  1.467246\n 0.923089   0.7097573  0.1341219  0.5239816  0.14190722 0.05281687\n 0.1066246  1.059639   0.56372356 2.0744548  1.3167725  1.2967091\n 0.37616873 0.7587209  0.04970551 1.226512   0.36094666 1.0950775\n 0.22631359 0.27469254 1.4895606  0.8513355  0.31801224 1.013958\n 0.4010086  0.6549983  0.29333496 3.2987661  1.4140272  1.2253399\n 2.7662945  0.39453554 0.3778286  0.6322994  0.80906487 0.6462765\n 1.7071385  1.1679792  1.507993   1.4987049  0.213521   0.46158504\n 2.333857   0.3499894  1.9789796  0.2405653  0.4633093  0.6223593\n 2.1015553  2.2899675  0.76598644 0.30883408 0.43623018 0.92514706\n 1.5539861  2.715002   1.0325842  0.88295555 0.87843895 1.2141562\n 2.021453   0.868175   0.5018997  0.34845352 1.6247983  0.9684458\n 0.22249937 0.22021866 0.8116746  0.626894   0.35939074 1.7363977\n 0.90065575 0.6823654  1.935813   0.6230707  0.33613443 0.9449086\n 0.7535472  0.6610794  1.9268332  0.7683339  0.24071312 1.402668\n 0.23224497 0.3510704  1.3514013  0.8519716  1.7791796  0.34544277\n 1.0013328  0.8814125  0.1286273  0.62079763 0.1286273  1.1574922\n 1.3808594  1.7427936  0.2071948  1.2305803  0.90475845 0.38588333\n 1.0727558  1.2340903  0.68610907 0.612731   1.660717   0.4399333\n 0.4221382  0.30110312 0.21089983 1.5950508  1.289465   1.8653679\n 1.5235114  0.49885464 0.7684269  0.80392647 2.1314893  1.1419182\n 0.7534151  0.88385105 1.3325367  0.6256566  0.78573895 0.22361088\n 0.640502   0.6653781  1.4303331  0.8949375  0.54064465 0.8885374\n 0.31801558 0.16592693 0.12831068 1.3070517  0.8071313  1.2727451\n 1.0152493  0.27108192 1.3911533  1.0621009  1.0623455  0.6731677\n 0.7092228  0.09459543 0.30647326 0.544364   0.6858711  0.9316797\n 0.6001377  0.8421912  1.5754814  0.476645   1.8287935  0.521739\n 1.3837576  1.3769693  0.9844394  1.6875818  0.48317766 0.27738762\n 0.33502626 0.45390224 0.41514492 1.5837517  0.25774527 0.8804312\n 2.732049   0.30717373 0.48874855 0.21496773]", "mean_td_error": 0.9072623252868652, "actor_loss": -15.856893539428711, "critic_loss": 0.6570618152618408, "alpha_loss": -13.016185760498047, "alpha_value": 0.02670930325984955, "target_entropy": -6, "mean_q": 15.451761245727539, "max_q": 31.432022094726562, "min_q": 5.082307815551758, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 23000, "episodes_total": 23, "training_iteration": 23, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_15-03-09", "timestamp": 1587049389, "time_this_iter_s": 86.58055973052979, "time_total_s": 1150.2160680294037, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 1150.2160680294037, "timesteps_since_restore": 23000, "iterations_since_restore": 23, "perf": {"cpu_util_percent": 92.26319444444445, "ram_util_percent": 11.5}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": 43.38410601120486, "episode_reward_min": -75.64248949209393, "episode_reward_mean": -29.897701673090914, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-31.977626262125387, -17.559467642217186, -1.8420493565286016, -33.0774688998567, 43.38410601120486, -67.88179802141097, -75.64248949209393, -51.921944420692775, -10.798309050300809, -51.659969596887635], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2593755920036119, "mean_processing_ms": 0.11919049810203992, "mean_inference_ms": 1.1761901396264123}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -54.587965716185884, "episode_reward_min": -132.53824395239025, "episode_reward_mean": -93.84596982046759, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-54.587965716185884, -132.53824395239025, -107.19239853543543, -67.32535743090901, -107.58588346741732], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.34162171505677186, "mean_processing_ms": 0.4685855988394284, "mean_inference_ms": 1.6560381889954179}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 12000, "num_steps_trained": 3582720, "num_steps_sampled": 24000, "sample_time_ms": 2.745, "replay_time_ms": 25.423, "grad_time_ms": 53.065, "update_time_ms": 0.004, "opt_peak_throughput": 4824.247, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[10.482318 ]\n [15.4909725]\n [13.74747  ]\n [10.777111 ]\n [11.590501 ]\n [ 8.433707 ]\n [11.044705 ]\n [12.112407 ]\n [14.74282  ]\n [20.511063 ]\n [25.82965  ]\n [11.482941 ]\n [12.198431 ]\n [11.495227 ]\n [10.223735 ]\n [ 8.628889 ]\n [17.401928 ]\n [10.647667 ]\n [10.702992 ]\n [ 8.991001 ]\n [19.23994  ]\n [ 7.533518 ]\n [11.745535 ]\n [12.189043 ]\n [10.322019 ]\n [17.229258 ]\n [ 9.851897 ]\n [10.521684 ]\n [ 9.67934  ]\n [10.214328 ]\n [17.282415 ]\n [14.719634 ]\n [20.006828 ]\n [24.730816 ]\n [ 9.885221 ]\n [10.111479 ]\n [ 8.035009 ]\n [22.47493  ]\n [11.87842  ]\n [ 9.288986 ]\n [ 7.8875327]\n [17.87438  ]\n [13.014067 ]\n [18.508612 ]\n [25.289566 ]\n [10.4580765]\n [12.064433 ]\n [11.5495205]\n [11.372007 ]\n [15.516958 ]\n [27.628832 ]\n [20.161116 ]\n [11.537425 ]\n [14.578079 ]\n [23.83442  ]\n [18.69614  ]\n [21.58259  ]\n [ 7.1864705]\n [25.754946 ]\n [10.418503 ]\n [ 9.777034 ]\n [22.147547 ]\n [25.114004 ]\n [10.33607  ]\n [10.327638 ]\n [10.4527855]\n [12.81468  ]\n [11.323194 ]\n [23.679058 ]\n [11.198617 ]\n [ 9.094824 ]\n [13.568323 ]\n [10.095698 ]\n [ 8.650118 ]\n [24.49082  ]\n [27.41356  ]\n [31.157705 ]\n [11.701703 ]\n [25.438715 ]\n [ 9.099829 ]\n [24.723469 ]\n [14.877024 ]\n [17.785912 ]\n [13.487407 ]\n [25.085646 ]\n [ 6.521483 ]\n [25.56614  ]\n [20.714626 ]\n [25.085646 ]\n [25.842175 ]\n [ 8.411467 ]\n [12.539088 ]\n [34.11791  ]\n [11.547562 ]\n [15.1979065]\n [22.919224 ]\n [12.080608 ]\n [12.569796 ]\n [ 9.527271 ]\n [17.943903 ]\n [12.406668 ]\n [10.674228 ]\n [12.323292 ]\n [16.387732 ]\n [10.107465 ]\n [28.988459 ]\n [14.765267 ]\n [10.614569 ]\n [11.69136  ]\n [18.069489 ]\n [18.648357 ]\n [ 9.012326 ]\n [18.793795 ]\n [12.442904 ]\n [ 9.494122 ]\n [11.481446 ]\n [20.736286 ]\n [24.758362 ]\n [14.648172 ]\n [22.091726 ]\n [10.447247 ]\n [17.315119 ]\n [ 9.404843 ]\n [10.488131 ]\n [10.011154 ]\n [ 7.469505 ]\n [16.248123 ]\n [ 9.12196  ]\n [ 8.947837 ]\n [ 7.32302  ]\n [10.608262 ]\n [12.711921 ]\n [10.294037 ]\n [ 9.699685 ]\n [16.855661 ]\n [24.506052 ]\n [30.808014 ]\n [10.185793 ]\n [11.69177  ]\n [28.54255  ]\n [20.868887 ]\n [17.070627 ]\n [28.001366 ]\n [15.435515 ]\n [23.808426 ]\n [10.248328 ]\n [10.978941 ]\n [24.009645 ]\n [20.910732 ]\n [14.063831 ]\n [11.405452 ]\n [27.56547  ]\n [10.051888 ]\n [10.671581 ]\n [17.947672 ]\n [10.490455 ]\n [12.711475 ]\n [11.77032  ]\n [ 9.437443 ]\n [10.7807255]\n [ 9.013187 ]\n [31.61583  ]\n [12.389892 ]\n [11.421689 ]\n [12.139164 ]\n [11.941209 ]\n [10.7314005]\n [10.546225 ]\n [17.644789 ]\n [11.029962 ]\n [10.0651045]\n [21.762375 ]\n [19.876925 ]\n [ 8.414169 ]\n [27.825699 ]\n [12.127927 ]\n [11.208157 ]\n [16.273188 ]\n [12.579013 ]\n [24.22119  ]\n [12.806029 ]\n [10.224116 ]\n [11.652933 ]\n [14.448276 ]\n [10.61142  ]\n [21.23368  ]\n [21.357096 ]\n [12.804265 ]\n [17.252806 ]\n [11.162541 ]\n [10.732193 ]\n [14.000081 ]\n [ 9.2931185]\n [21.141651 ]\n [12.428338 ]\n [10.325758 ]\n [11.876141 ]\n [24.640781 ]\n [24.852352 ]\n [29.92008  ]\n [ 9.480497 ]\n [11.327163 ]\n [10.710024 ]\n [22.697124 ]\n [ 9.785771 ]\n [15.851454 ]\n [21.115944 ]\n [ 8.406646 ]\n [12.087815 ]\n [18.419088 ]\n [34.748634 ]\n [24.06495  ]\n [27.851124 ]\n [22.379972 ]\n [ 9.781908 ]\n [11.048808 ]\n [29.528248 ]\n [13.47086  ]\n [15.136791 ]\n [12.751956 ]\n [19.694868 ]\n [24.274403 ]\n [22.699326 ]\n [18.693748 ]\n [19.033022 ]\n [ 8.226827 ]\n [11.289368 ]\n [ 9.012314 ]\n [10.580332 ]\n [12.761689 ]\n [ 8.817594 ]\n [27.599525 ]\n [15.872011 ]\n [10.903967 ]\n [ 9.525399 ]\n [ 8.791343 ]\n [27.297958 ]\n [11.680146 ]\n [23.823668 ]\n [ 8.798675 ]\n [16.952433 ]\n [12.253649 ]\n [ 6.8298106]\n [13.747    ]\n [21.080137 ]\n [11.824924 ]\n [17.953697 ]\n [ 9.819325 ]\n [11.040326 ]\n [24.119478 ]\n [17.070627 ]\n [ 9.045957 ]\n [13.05884  ]\n [24.714277 ]\n [22.301859 ]\n [10.962247 ]]", "q_t_selected": "[10.482318  15.4909725 13.74747   10.777111  11.590501   8.433707\n 11.044705  12.112407  14.74282   20.511063  25.82965   11.482941\n 12.198431  11.495227  10.223735   8.628889  17.401928  10.647667\n 10.702992   8.991001  19.23994    7.533518  11.745535  12.189043\n 10.322019  17.229258   9.851897  10.521684   9.67934   10.214328\n 17.282415  14.719634  20.006828  24.730816   9.885221  10.111479\n  8.035009  22.47493   11.87842    9.288986   7.8875327 17.87438\n 13.014067  18.508612  25.289566  10.4580765 12.064433  11.5495205\n 11.372007  15.516958  27.628832  20.161116  11.537425  14.578079\n 23.83442   18.69614   21.58259    7.1864705 25.754946  10.418503\n  9.777034  22.147547  25.114004  10.33607   10.327638  10.4527855\n 12.81468   11.323194  23.679058  11.198617   9.094824  13.568323\n 10.095698   8.650118  24.49082   27.41356   31.157705  11.701703\n 25.438715   9.099829  24.723469  14.877024  17.785912  13.487407\n 25.085646   6.521483  25.56614   20.714626  25.085646  25.842175\n  8.411467  12.539088  34.11791   11.547562  15.1979065 22.919224\n 12.080608  12.569796   9.527271  17.943903  12.406668  10.674228\n 12.323292  16.387732  10.107465  28.988459  14.765267  10.614569\n 11.69136   18.069489  18.648357   9.012326  18.793795  12.442904\n  9.494122  11.481446  20.736286  24.758362  14.648172  22.091726\n 10.447247  17.315119   9.404843  10.488131  10.011154   7.469505\n 16.248123   9.12196    8.947837   7.32302   10.608262  12.711921\n 10.294037   9.699685  16.855661  24.506052  30.808014  10.185793\n 11.69177   28.54255   20.868887  17.070627  28.001366  15.435515\n 23.808426  10.248328  10.978941  24.009645  20.910732  14.063831\n 11.405452  27.56547   10.051888  10.671581  17.947672  10.490455\n 12.711475  11.77032    9.437443  10.7807255  9.013187  31.61583\n 12.389892  11.421689  12.139164  11.941209  10.7314005 10.546225\n 17.644789  11.029962  10.0651045 21.762375  19.876925   8.414169\n 27.825699  12.127927  11.208157  16.273188  12.579013  24.22119\n 12.806029  10.224116  11.652933  14.448276  10.61142   21.23368\n 21.357096  12.804265  17.252806  11.162541  10.732193  14.000081\n  9.2931185 21.141651  12.428338  10.325758  11.876141  24.640781\n 24.852352  29.92008    9.480497  11.327163  10.710024  22.697124\n  9.785771  15.851454  21.115944   8.406646  12.087815  18.419088\n 34.748634  24.06495   27.851124  22.379972   9.781908  11.048808\n 29.528248  13.47086   15.136791  12.751956  19.694868  24.274403\n 22.699326  18.693748  19.033022   8.226827  11.289368   9.012314\n 10.580332  12.761689   8.817594  27.599525  15.872011  10.903967\n  9.525399   8.791343  27.297958  11.680146  23.823668   8.798675\n 16.952433  12.253649   6.8298106 13.747     21.080137  11.824924\n 17.953697   9.819325  11.040326  24.119478  17.070627   9.045957\n 13.05884   24.714277  22.301859  10.962247 ]", "twin_q_t_selected": "[10.62888   16.04054   13.555563  10.528736  10.869665   8.987662\n 11.005916  11.984653  14.197546  21.032425  26.52014   12.234813\n 12.787874  11.277863   9.939981   9.288437  19.117363  10.798579\n 11.11135   10.390178  18.417648   8.352331  11.560935  12.177479\n 10.949638  18.740469   9.619807  11.024806  10.081906  10.900173\n 16.772772  15.3303795 19.942429  23.574257  10.083901   9.450929\n  8.907241  23.552565  11.965663   8.765683   7.559143  17.53322\n 13.448924  19.034424  25.846127  10.856364  11.115131  11.958222\n 11.817114  15.16143   26.083921  20.093962  12.590944  15.760838\n 24.90106   19.698853  20.296726   7.873256  26.045517  10.199308\n 10.49955   23.369524  25.873096  11.012847  11.064629  10.069539\n 11.997445  10.244929  23.42999   11.198651   9.511862  12.338574\n 10.889655   9.021076  23.814339  26.754261  32.24482   12.27418\n 24.340117   8.9065695 24.038479  13.45313   19.246897  13.31576\n 25.85862    6.6368012 25.517086  21.591208  25.85862   26.64394\n  9.250334  11.989397  34.187283  11.945816  14.043635  21.217695\n 12.204298  12.363184   9.403831  18.411537  11.270866  11.401107\n 12.762638  17.435562  11.17868   29.138762  13.342744  11.2707405\n 10.900396  17.795675  20.21787    9.559804  18.160734  12.1992445\n 10.0618    10.203852  18.981236  24.653585  14.764815  22.192247\n 10.96292   17.005852   9.476296  11.029372  10.288991   6.8736186\n 15.99927    8.68234    7.771926   8.48172   10.342351  11.375273\n  9.172584   9.107579  17.29381   24.69004   30.845182  10.508754\n 11.489145  28.248623  20.822887  18.711527  27.655273  18.012632\n 24.40956   10.305278  10.46696   24.894794  19.754496  12.870885\n 11.361396  28.493267   9.756132  11.656006  17.288939  11.371583\n 11.69267   12.927653   8.920665   9.748817   9.546644  31.497046\n 11.954459  11.931492  12.888967  11.661087   9.55488   10.367121\n 16.692884  10.802674  10.032896  22.134665  19.214565   9.642752\n 26.140432  13.176373  10.877868  17.501091  12.607105  24.035397\n 13.291333   9.780985  12.218517  14.690064  10.280005  22.137041\n 21.428358  11.795505  17.598463  11.421202  10.776705  13.207969\n  9.524971  20.453112  12.169281  10.278833  12.047665  23.069519\n 25.684553  31.324833   8.391251  12.852781  10.391122  22.044699\n 10.330078  15.962437  20.937675   9.298849  11.394334  19.137314\n 35.27292   22.787336  27.86847   24.031275  10.208807  11.6151705\n 28.918877  13.852205  16.086163  10.897406  19.372591  23.806736\n 21.416367  19.802202  19.76289    9.10101   11.474781   8.052246\n 11.081113  11.55768    8.087067  26.507343  17.437386  11.17114\n 10.193214   9.443073  27.980883  11.617209  24.756084   8.274413\n 16.897583  12.198089   7.7109013 13.848119  21.50915   11.648291\n 17.231064  11.854003  10.793205  24.113688  18.711527   9.925511\n 13.7614    23.496656  22.42812   10.715784 ]", "q_t_selected_target": "[11.668681  13.342158  14.241043  11.010405  10.6727495  8.257002\n 11.7926    12.071588  12.230101  22.015924  26.453365  10.430318\n 13.321178  10.786163  10.686677   8.601172  16.250887   9.65347\n 10.063153   8.57763   18.979177   8.159412  10.511369  12.15142\n 10.427256  19.659101   8.333257  10.393459  10.67721   10.515386\n 18.309183  13.840547  18.095898  25.72544   10.535138   9.934559\n  7.5338535 24.077581  12.130916   7.5807886  7.3489537 16.34094\n 13.308155  21.095814  26.19028   10.369612  12.614469  13.263482\n 12.084019  15.863636  27.731342  17.670763  11.828255  13.322767\n 24.537592  20.891525  22.446945   9.191634  25.543316  10.408074\n 10.492547  21.256073  22.70535   10.4249115  9.699669   9.679176\n 13.850767  10.98045   25.65214   10.820221   9.904458  13.601884\n  9.139782   8.9349    22.9247    27.962576  32.68138   12.800607\n 24.77947   10.081802  24.63162   14.1125965 17.37884   16.010695\n 27.129984   5.2655163 24.206114  22.98243   27.128384  27.076387\n  9.675151  11.415612  35.0401    10.877879  14.553139  21.604595\n  9.827755  12.553967  10.164174  16.678392  11.386911  11.094947\n 11.509153  17.712551  11.040845  29.4929    14.879608  10.342841\n 10.953813  17.142225  20.45145    8.203598  18.353365  12.535103\n  7.9273896  9.198033  20.664352  25.82994   16.180159  24.024376\n 11.725145  17.778446   9.641655  12.608394   9.20325    7.856216\n 18.32046    8.448905   8.881525   7.320751  10.335686  12.884647\n  9.605047   9.420614  17.45459   25.119204  31.395367  11.107294\n 12.266101  27.679836  23.155054  18.920595  24.768078  17.948584\n 23.96486    9.313991   9.671798  23.707308  20.813557  13.018868\n 11.862828  29.307928  10.133217  11.404364  15.604467  12.0396385\n 12.846907  11.530369   8.1458845 10.651891  10.158754  32.361713\n 10.854472  10.792126  11.032228  11.305729   9.155362  11.3536215\n 17.411985  11.130233  10.414723  22.995848  20.763382   9.564196\n 25.31077   12.105497  11.074899  16.348751  12.190476  25.032167\n 14.314885  10.643022  11.623942  13.670029  10.340831  21.963987\n 21.146837  11.638173  18.858898  11.968317   9.972506  13.121571\n  8.327555  19.193993  11.274387  10.192589  11.638989  24.64686\n 24.685753  32.016617   8.0002775 12.460233  11.280561  21.033909\n  8.801288  16.437479  20.695505   9.222987  12.337894  19.474424\n 35.42002   23.456852  27.586248  24.288527   9.3612    10.130683\n 30.601408  14.33722   13.523772  11.515541  19.207287  22.732506\n 21.252989  19.981907  17.102394   8.845089  12.232762   9.311973\n 10.912198  12.691326   7.925295  28.331354  15.671413  11.560382\n 11.396838   8.625385  29.773542  10.551036  23.462599   8.873157\n 15.714929  13.338213   8.339656  11.900605  20.70912   10.406966\n 16.948538  11.900124  10.725567  21.673508  19.097172  10.130757\n 14.008448  25.084867  21.049364  12.126998 ]", "q_tp1_best_masked": "[12.242516  14.612915  13.925018  11.379345  10.868989   9.621759\n 12.121496  11.854468  13.573229  23.049278  26.385368  10.716738\n 12.8872595 11.377218  10.452377   9.621933  17.74358   10.060708\n 10.172038  10.589793  20.206547  10.956726  11.104899  13.245624\n 11.263831  20.991215   9.660701  10.780227  11.165084  11.764667\n 19.138332  14.350244  17.68222   26.01334   10.967248  11.29512\n  8.592044  24.205637  11.983452   8.701047   9.276859  16.253075\n 14.182339  22.00325   25.867907  11.821904  12.433807  13.192615\n 12.647682  16.703758  28.731098  18.068876  12.79827   14.253613\n 23.663296  22.176683  24.069124  10.3180275 26.472147  10.661592\n 11.008438  21.790041  22.789291  11.0035    11.041536   9.654629\n 14.47719   11.162658  25.624697  11.507412   9.981837  12.2728615\n 10.412766  10.326411  23.405487  28.045202  32.75617   11.821116\n 25.056303  10.338986  25.269703  15.020745  18.492722  14.787159\n 27.409185   5.72681   23.54935   22.664646  27.407568  26.232227\n 10.2845545 12.0362625 33.523304  12.016016  15.995531  22.21462\n 10.47944   11.517723  10.82974   16.882658  11.444933  11.422625\n 11.457746  19.539186  12.144683  28.842707  15.8980665 10.115738\n 10.893254  17.729265  22.947107   7.944939  18.546638  13.399307\n  8.134629   9.595429  21.70936   25.590397  16.057125  25.401152\n 11.404186  17.540064  11.206283  13.197924  10.22503    9.797634\n 20.214333   9.704118   9.9532795  8.154799  10.29247   12.510517\n 10.34558   11.116883  17.063377  25.389744  31.332767  12.225343\n 13.008903  27.116602  22.988127  20.05268   25.191196  18.51096\n 24.515865   9.744512  10.417337  24.021948  20.665277  13.303358\n 11.739442  30.092417  10.824163  10.789536  16.402536  13.002437\n 12.501196  11.563604   9.656997  11.210881  11.327064  32.09301\n 10.178514  11.040403  10.568776  11.5262375 10.219528  11.731538\n 17.294113  10.76354   10.871173  24.126844  20.973164  10.897509\n 25.099995  12.867972  11.256752  16.608768  12.278481  25.807673\n 14.431483  10.674656  10.916394  14.673115  10.517542  22.661564\n 21.729647  11.099537  19.400513  12.349238   9.190952  12.836512\n  9.431307  19.936216  11.311307  10.591228  12.177898  23.867083\n 24.311668  32.724865   9.868985  11.89932   12.176219  20.953398\n  9.130225  17.186975  22.19462    9.519109  12.060521  19.73795\n 34.401764  24.20163   27.827343  25.443802   9.573885  11.645937\n 30.386711  14.754377  15.544966  13.694022  19.229448  23.631155\n 21.87042   20.106655  18.963032   9.249016  12.677984  10.838483\n 10.44632   11.764849   9.034679  28.244614  16.649933  11.322394\n 13.036883  10.652353  29.057558  11.623714  22.769606   9.154874\n 16.14057   13.374249   9.013249  13.796448  20.884739  10.576215\n 17.173594  11.835947  10.874122  22.607712  20.23104   10.635549\n 13.486762  25.099388  21.584288  12.449411 ]", "policy_t": "[[-0.14319694 -0.739213   -0.52533746  0.70946884  0.7078891   0.90950775]\n [-0.7458918   0.5622635  -0.7808879   0.9993032   0.98405504  0.99659157]\n [-0.19931954  0.3728423  -0.32140625 -0.09334463 -0.4285277   0.3889978 ]\n ...\n [-0.8435885   0.7986078  -0.8963149   0.99614906  0.9898684   0.99892807]\n [ 0.9866855  -0.6255372  -0.5267405   0.9685812  -0.11485279  0.17246258]\n [-0.8138028   0.45966613 -0.7659063  -0.7052449  -0.01067561 -0.07540554]]", "td_error": "[1.1130824  2.4235983  0.58952665 0.357481   0.55733347 0.4536829\n 0.76728916 0.06387663 2.2400823  1.2441807  0.3452444  1.4285588\n 0.8280258  0.6003814  0.6048188  0.35749054 2.0087585  1.069653\n 0.844018   1.1129594  0.41114616 0.40940666 1.1418662  0.03184128\n 0.31380987 1.6742382  1.4025955  0.37978554 0.7965865  0.3429227\n 1.2815895  1.1844602  1.8787308  1.5729027  0.55057716 0.33027506\n 0.9372716  1.0638342  0.20887423 1.4465461  0.37438416 1.3628597\n 0.21742868 2.324296   0.6224346  0.28760862 1.0246863  1.5096107\n 0.48945808 0.5244417  0.87496567 2.4567757  0.5267596  1.8466911\n 0.5333204  1.6940289  1.507287   1.6617708  0.35691547 0.10959721\n 0.36125803 1.5024624  2.7881994  0.33838844 0.99646425 0.58198595\n 1.4447045  0.5391321  2.0976152  0.3784132  0.6011152  0.6484351\n 1.3528948  0.18547916 1.2278795  0.878665   0.98011875 0.812665\n 0.54929924 1.0786033  0.34249496 0.71194696 1.1375647  2.6091113\n 1.6578512  1.3136258  1.3354988  1.8295126  1.656251   0.83333015\n 0.8442507  0.8486304  0.8875046  0.8688097  0.57713556 0.8507643\n 2.3146982  0.10330582 0.6986232  1.4993277  0.56790066 0.36343956\n 1.0338116  0.8009043  0.5356078  0.42929077 0.82560253 0.59981346\n 0.39548206 0.79035664 1.0183363  1.0824671  0.31653023 0.21402836\n 1.8505712  1.6446157  0.8775253  1.1239672  1.4736648  1.8823891\n 1.020062   0.61796093 0.20108509 1.8496423  0.94682264 0.68465424\n 2.1967626  0.4532447  0.5879555  0.5816188  0.13962078 0.8410506\n 0.56072664 0.29605293 0.3798542  0.52115726 0.5687685  0.76002073\n 0.67564344 0.71574974 2.309167   1.0295181  3.0602417  1.2885585\n 0.30056667 0.9628124  1.0511527  0.74491215 0.5781183  0.5964732\n 0.47940445 1.2785597  0.22920656 0.4922123  2.0138378  1.1086197\n 0.64483404 0.8186178  1.0331693  0.515954   0.87883854 0.8052759\n 1.3177032  0.88446474 1.4818368  0.49541903 0.9877782  0.8969488\n 0.47595215 0.21391487 0.36572313 1.047328   1.2176371  0.6142912\n 1.6722956  0.5466523  0.16514444 0.6139517  0.40258265 0.9038744\n 1.2662039  0.640471   0.31178284 0.8991413  0.16570711 0.45168018\n 0.24588966 0.6617117  1.4332638  0.6764455  0.7819433  0.4824543\n 1.08149    1.6033888  1.0244222  0.10970688 0.32291317 0.7917099\n 0.5826998  1.3941603  0.93559647 0.7628093  0.7299886  1.3370028\n 1.2566371  0.5305338  0.33130455 0.44610167 0.5968199  0.69622326\n 0.40924454 0.6388073  0.27354813 1.0829029  0.6341572  1.2013063\n 1.3778458  0.6756878  2.0877047  0.9272752  0.32644272 1.3080635\n 0.80485725 0.73393154 2.2955618  0.43709183 0.850688   0.77969265\n 0.25039053 0.6020045  0.52703524 1.2779198  0.98328495 0.5228286\n 1.5375314  0.49182272 2.134122   1.097642   0.8272772  0.3366127\n 1.2100792  1.1123443  1.0692999  1.896954   0.5855236  1.3296413\n 0.6438427  1.0634594  0.19119883 2.4430752  1.2060947  0.64502335\n 0.59832764 0.97940063 1.3156252  1.2879825 ]", "mean_td_error": 0.919719934463501, "actor_loss": -15.795334815979004, "critic_loss": 0.6499859690666199, "alpha_loss": -8.038761138916016, "alpha_value": 0.021353619173169136, "target_entropy": -6, "mean_q": 15.35252857208252, "max_q": 34.748634338378906, "min_q": 6.521482944488525, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 24000, "episodes_total": 24, "training_iteration": 24, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_15-04-51", "timestamp": 1587049491, "time_this_iter_s": 86.76611113548279, "time_total_s": 1236.9821791648865, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 1236.9821791648865, "timesteps_since_restore": 24000, "iterations_since_restore": 24, "perf": {"cpu_util_percent": 92.28689655172414, "ram_util_percent": 11.56413793103448}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": -6.618491324886982, "episode_reward_min": -124.47338402438638, "episode_reward_mean": -70.51799651046954, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-124.47338402438638, -48.28415371809784, -115.74773636966083, -79.54727202283495, -71.98453495461911, -6.618491324886982, -69.59523236546126, -47.60021430572019, -96.0982243276544, -45.23072169137354], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2596366355866401, "mean_processing_ms": 0.11936270033970328, "mean_inference_ms": 1.1785625117525882}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
{"episode_reward_max": -51.95215464214519, "episode_reward_min": -107.58588346741732, "episode_reward_mean": -77.72875195841857, "episode_len_mean": 1000.0, "episodes_this_iter": 1, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-51.95215464214519, -107.19239853543543, -67.32535743090901, -107.58588346741732, -54.587965716185884], "episode_lengths": [1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.3435049886978353, "mean_processing_ms": 0.47043809452365987, "mean_inference_ms": 1.6695214494604909}, "off_policy_estimator": {}, "info": {"exploration_infos": [{}], "num_target_updates": 12500, "num_steps_trained": 3838720, "num_steps_sampled": 25000, "sample_time_ms": 4.195, "replay_time_ms": 21.374, "grad_time_ms": 47.378, "update_time_ms": 0.004, "opt_peak_throughput": 5403.339, "opt_samples": 256.0, "learner": {"default_policy": {"q_t": "[[ 8.554902 ]\n [11.599884 ]\n [14.262619 ]\n [10.109997 ]\n [17.586493 ]\n [10.178805 ]\n [ 8.603028 ]\n [10.942974 ]\n [20.562878 ]\n [11.997537 ]\n [12.420673 ]\n [ 9.697485 ]\n [12.980411 ]\n [26.716072 ]\n [ 7.878586 ]\n [29.677061 ]\n [22.11471  ]\n [ 9.195393 ]\n [ 8.920673 ]\n [ 8.971488 ]\n [30.675648 ]\n [17.076677 ]\n [25.978682 ]\n [23.09806  ]\n [11.027409 ]\n [24.977352 ]\n [27.796768 ]\n [ 9.610915 ]\n [11.565942 ]\n [ 9.382692 ]\n [ 9.677345 ]\n [ 9.347847 ]\n [26.397354 ]\n [11.845891 ]\n [17.59347  ]\n [ 8.588298 ]\n [18.84241  ]\n [25.165018 ]\n [ 9.884624 ]\n [16.846643 ]\n [12.055561 ]\n [28.345148 ]\n [21.717386 ]\n [ 9.5476675]\n [10.294207 ]\n [10.143527 ]\n [28.224382 ]\n [15.506352 ]\n [25.996637 ]\n [ 8.51591  ]\n [19.976744 ]\n [23.453691 ]\n [10.480514 ]\n [16.289497 ]\n [22.751394 ]\n [ 9.587768 ]\n [20.136414 ]\n [10.583555 ]\n [10.9105625]\n [26.303043 ]\n [19.116043 ]\n [ 9.678897 ]\n [10.426569 ]\n [20.25273  ]\n [12.6414175]\n [20.44506  ]\n [ 9.152822 ]\n [20.15804  ]\n [12.256261 ]\n [11.044708 ]\n [ 9.714832 ]\n [ 8.535914 ]\n [16.478605 ]\n [22.231647 ]\n [15.289838 ]\n [20.085287 ]\n [22.147436 ]\n [ 9.421564 ]\n [10.402131 ]\n [12.358389 ]\n [19.792023 ]\n [22.147436 ]\n [ 9.634182 ]\n [16.226955 ]\n [ 9.237995 ]\n [14.865157 ]\n [10.757286 ]\n [12.266262 ]\n [20.981995 ]\n [30.464127 ]\n [26.23332  ]\n [ 9.22823  ]\n [10.52945  ]\n [24.15002  ]\n [20.549007 ]\n [10.787048 ]\n [26.565866 ]\n [16.079178 ]\n [ 8.276229 ]\n [ 9.809976 ]\n [24.851826 ]\n [13.7066965]\n [26.84179  ]\n [18.254337 ]\n [12.039912 ]\n [10.241224 ]\n [11.918541 ]\n [17.9552   ]\n [12.620762 ]\n [ 0.8599596]\n [ 9.010877 ]\n [13.295709 ]\n [17.17072  ]\n [19.433823 ]\n [24.924116 ]\n [ 9.373662 ]\n [11.421372 ]\n [ 8.20323  ]\n [10.157867 ]\n [14.259752 ]\n [22.413778 ]\n [11.067316 ]\n [14.029496 ]\n [21.70993  ]\n [27.365995 ]\n [10.499236 ]\n [21.497728 ]\n [11.860762 ]\n [ 7.3130674]\n [10.674786 ]\n [19.114994 ]\n [ 9.866037 ]\n [ 9.237785 ]\n [11.386732 ]\n [18.231369 ]\n [14.093857 ]\n [10.2831745]\n [11.766898 ]\n [26.451088 ]\n [11.263306 ]\n [12.853715 ]\n [ 8.349167 ]\n [15.95854  ]\n [11.054368 ]\n [23.156498 ]\n [12.223848 ]\n [ 7.6121764]\n [10.662527 ]\n [12.629007 ]\n [10.788931 ]\n [ 9.401265 ]\n [17.883202 ]\n [25.859257 ]\n [22.869644 ]\n [14.394656 ]\n [19.398369 ]\n [27.950756 ]\n [11.706652 ]\n [20.359856 ]\n [ 8.975734 ]\n [ 8.198519 ]\n [ 8.816106 ]\n [13.289416 ]\n [ 8.663912 ]\n [10.795837 ]\n [19.024122 ]\n [10.806161 ]\n [ 9.743491 ]\n [26.543291 ]\n [11.190751 ]\n [10.07666  ]\n [13.201257 ]\n [ 9.998086 ]\n [16.749435 ]\n [26.210958 ]\n [14.660723 ]\n [11.378601 ]\n [ 9.007052 ]\n [ 9.994303 ]\n [ 7.9823976]\n [26.254868 ]\n [19.866728 ]\n [19.013199 ]\n [10.410084 ]\n [17.579508 ]\n [ 8.10289  ]\n [20.564075 ]\n [ 9.053785 ]\n [11.66695  ]\n [13.671295 ]\n [24.971046 ]\n [25.11554  ]\n [25.146633 ]\n [18.117325 ]\n [24.711376 ]\n [11.130388 ]\n [13.623478 ]\n [17.95068  ]\n [21.938519 ]\n [10.8883915]\n [10.895742 ]\n [ 9.96623  ]\n [26.516027 ]\n [ 8.627958 ]\n [11.36932  ]\n [12.707648 ]\n [13.591005 ]\n [10.410695 ]\n [ 7.6769   ]\n [27.03488  ]\n [21.653458 ]\n [20.43906  ]\n [10.272618 ]\n [10.0875025]\n [11.32689  ]\n [18.952536 ]\n [ 9.340537 ]\n [11.137126 ]\n [ 8.628632 ]\n [10.617227 ]\n [17.158148 ]\n [10.749789 ]\n [12.401062 ]\n [10.435037 ]\n [18.12641  ]\n [ 9.95278  ]\n [ 8.636977 ]\n [11.75296  ]\n [20.261127 ]\n [ 9.040468 ]\n [ 9.989777 ]\n [18.930035 ]\n [32.74995  ]\n [10.552166 ]\n [11.458858 ]\n [ 8.866206 ]\n [16.445435 ]\n [13.005883 ]\n [12.543263 ]\n [26.945803 ]\n [24.486532 ]\n [ 7.6391683]\n [ 6.8571253]\n [10.5106945]\n [11.998282 ]\n [11.018216 ]\n [13.534211 ]\n [27.833717 ]\n [11.700587 ]\n [ 9.9102545]\n [ 9.921192 ]\n [11.550318 ]\n [10.208009 ]\n [22.23816  ]\n [16.733213 ]\n [14.307504 ]]", "q_t_selected": "[ 8.554902  11.599884  14.262619  10.109997  17.586493  10.178805\n  8.603028  10.942974  20.562878  11.997537  12.420673   9.697485\n 12.980411  26.716072   7.878586  29.677061  22.11471    9.195393\n  8.920673   8.971488  30.675648  17.076677  25.978682  23.09806\n 11.027409  24.977352  27.796768   9.610915  11.565942   9.382692\n  9.677345   9.347847  26.397354  11.845891  17.59347    8.588298\n 18.84241   25.165018   9.884624  16.846643  12.055561  28.345148\n 21.717386   9.5476675 10.294207  10.143527  28.224382  15.506352\n 25.996637   8.51591   19.976744  23.453691  10.480514  16.289497\n 22.751394   9.587768  20.136414  10.583555  10.9105625 26.303043\n 19.116043   9.678897  10.426569  20.25273   12.6414175 20.44506\n  9.152822  20.15804   12.256261  11.044708   9.714832   8.535914\n 16.478605  22.231647  15.289838  20.085287  22.147436   9.421564\n 10.402131  12.358389  19.792023  22.147436   9.634182  16.226955\n  9.237995  14.865157  10.757286  12.266262  20.981995  30.464127\n 26.23332    9.22823   10.52945   24.15002   20.549007  10.787048\n 26.565866  16.079178   8.276229   9.809976  24.851826  13.7066965\n 26.84179   18.254337  12.039912  10.241224  11.918541  17.9552\n 12.620762   0.8599596  9.010877  13.295709  17.17072   19.433823\n 24.924116   9.373662  11.421372   8.20323   10.157867  14.259752\n 22.413778  11.067316  14.029496  21.70993   27.365995  10.499236\n 21.497728  11.860762   7.3130674 10.674786  19.114994   9.866037\n  9.237785  11.386732  18.231369  14.093857  10.2831745 11.766898\n 26.451088  11.263306  12.853715   8.349167  15.95854   11.054368\n 23.156498  12.223848   7.6121764 10.662527  12.629007  10.788931\n  9.401265  17.883202  25.859257  22.869644  14.394656  19.398369\n 27.950756  11.706652  20.359856   8.975734   8.198519   8.816106\n 13.289416   8.663912  10.795837  19.024122  10.806161   9.743491\n 26.543291  11.190751  10.07666   13.201257   9.998086  16.749435\n 26.210958  14.660723  11.378601   9.007052   9.994303   7.9823976\n 26.254868  19.866728  19.013199  10.410084  17.579508   8.10289\n 20.564075   9.053785  11.66695   13.671295  24.971046  25.11554\n 25.146633  18.117325  24.711376  11.130388  13.623478  17.95068\n 21.938519  10.8883915 10.895742   9.96623   26.516027   8.627958\n 11.36932   12.707648  13.591005  10.410695   7.6769    27.03488\n 21.653458  20.43906   10.272618  10.0875025 11.32689   18.952536\n  9.340537  11.137126   8.628632  10.617227  17.158148  10.749789\n 12.401062  10.435037  18.12641    9.95278    8.636977  11.75296\n 20.261127   9.040468   9.989777  18.930035  32.74995   10.552166\n 11.458858   8.866206  16.445435  13.005883  12.543263  26.945803\n 24.486532   7.6391683  6.8571253 10.5106945 11.998282  11.018216\n 13.534211  27.833717  11.700587   9.9102545  9.921192  11.550318\n 10.208009  22.23816   16.733213  14.307504 ]", "twin_q_t_selected": "[ 8.772053   11.316023   13.596267   10.539951   17.980583   10.558571\n  8.280794   11.766095   19.73836    11.340285   12.1430435  10.154867\n 12.957789   26.104643    7.478563   30.534302   23.08356     9.55675\n  9.205633    9.742276   31.429684   17.203424   26.280197   23.423527\n  9.823316   25.958414   27.580315   10.246761   11.184202    9.032448\n  9.966389   10.155076   25.599377   11.810265   17.650736    8.338669\n 18.898815   23.432602   10.924411   16.421986   12.81092    27.332205\n 20.515772   10.067248   10.543177   10.698391   27.288515   15.068699\n 25.913841    8.244905   20.384188   23.95542    10.231575   14.888371\n 21.778334    8.776446   21.326426   10.520517   10.801552   25.595358\n 18.654198    9.116558   10.215646   20.272686   13.17428    19.864952\n  9.94588    19.91268    12.4514675  10.668587    7.7930803   8.427934\n 15.7025795  22.077085   15.870035   21.203178   24.081196   10.332594\n 10.792758   12.170466   20.869686   24.081196    9.834622   15.2613945\n 10.100173   14.35326    10.332114   12.165685   19.683987   30.05134\n 25.305403    9.414025   10.581082   22.59826    20.728203   10.15445\n 26.613285   15.777812    8.00306     9.115697   26.387445   13.111155\n 26.593056   18.526743   11.4750185   9.728173   10.143563   19.062902\n 12.35082    -0.21363479  9.55174    13.594898   16.737135   18.132172\n 25.178577    9.26171    11.097266    8.920284   10.024049   14.710714\n 21.692745   10.13978    15.326915   21.483469   26.449877   11.193721\n 20.579071   11.124007    8.580381   10.74487    20.173388    9.687924\n  8.569147   10.807343   16.580706   13.599621   11.463403   11.85604\n 25.409325   10.696934   13.035556    9.087392   13.789869    9.044096\n 22.66699    13.132882    7.290574   10.988182   12.432877   10.800053\n  9.360505   17.736755   25.207254   21.584576   14.734862   18.207964\n 26.549397   10.797695   20.157885    8.173476    8.101168    9.261445\n 14.783541    8.464169   11.387032   18.981792   10.7459      8.4441805\n 24.83388    10.520709    9.861279   14.142223    8.879532   16.773232\n 26.515944   13.916491   11.161587    9.177764   10.719458    8.475451\n 26.464638   19.559776   17.967901   10.524471   16.868788    8.641163\n 20.94357     8.806108   11.574926   15.272898   24.34386    24.934675\n 24.99589    18.970013   21.846352   11.527332   13.671424   18.226555\n 21.634073   10.721341   10.332993    9.83906    26.76537     7.163051\n 12.363503   12.061169   14.815806   10.523869    8.009075   27.347504\n 21.193834   19.43116     9.7687235  10.130627   10.858422   20.528294\n  8.466456   10.905234    8.582616    9.821644   17.681707    9.91411\n 12.647436    9.942847   19.98722     9.197225    9.540904   11.056906\n 19.649097    8.640615    8.685546   19.577932   31.795683   10.440091\n 11.700081    9.272054   16.097717   12.038907   12.575514   26.256653\n 25.43483     8.095252    6.5499425  11.035961   10.4447565  10.1526\n 12.536381   28.824192   12.128967   10.42079    10.271927   11.37145\n 10.992212   22.673885   16.974033   13.337138  ]", "q_t_selected_target": "[ 6.57511   11.362162  14.887365   9.219837  18.324575  10.050423\n  6.879407  11.98111   20.783318  13.290104  12.830538   9.982855\n 11.044735  26.250679   7.043057  32.282543  23.829247   9.367178\n  9.870317   8.959923  31.209795  14.556999  25.045158  22.71981\n 10.341144  26.014845  24.675964   9.3364525 13.250571   7.678002\n  8.587642   8.415427  25.149862  12.877922  16.75941    8.005026\n 16.805304  24.728895  12.408857  14.971405  11.595725  28.495386\n 21.909735   8.516287   7.161936   9.635985  26.307241  15.359346\n 24.661928   8.228589  19.504198  24.701265  10.330527  15.209235\n 23.446688   9.986627  19.936687  10.103961  10.399127  27.686792\n 19.509552   7.670124   9.533196  20.073755  12.631713  21.683977\n  8.167907  19.654226  12.655918   9.999487   9.201053   7.019007\n 17.449606  21.67264   16.21612   19.44627   23.00467   10.106867\n  9.974758  12.643312  18.61891   23.113905   8.133348  11.885419\n  9.508959  15.341913  10.325122  11.8087    19.805115  28.138115\n 25.793755   8.418733  10.279327  23.551577  19.33212   10.137696\n 25.822775  18.327831   6.1498036 10.359778  23.906952  12.2709465\n 27.251879  17.246061  12.1267395 10.051525  11.455189  16.52619\n 11.620076  -0.7711376  8.664883  13.661987  14.59795   16.98988\n 25.255045   8.512258  10.665723   7.8840904 11.72981   12.152052\n 20.33866   11.335323  15.266737  19.520222  26.560734  11.214529\n 22.445358  10.404931   7.536047  10.19365   17.337614   9.440462\n  8.159171  11.708063  16.444536  12.614677  10.160236  11.170184\n 25.88083   10.008152  10.700945   9.32178   12.836016  10.087449\n 24.005138  12.409905   6.030431  10.493101  13.79687   10.211495\n  8.631997  17.017033  26.352253  22.450115  17.002924  19.449547\n 26.155142  11.704389  20.339996   7.9406257  7.731767   8.653569\n 12.962901   8.01374   10.434935  16.925169  11.980932   7.1804557\n 25.893032  11.93047    9.619017  14.175438   9.674831  15.940741\n 25.616331  11.519763  12.654133   8.7197895 10.006188   8.089884\n 27.223167  18.700897  19.765469  11.357406  16.503296   8.61575\n 20.556204   7.2270665 12.560567  13.514785  26.507792  25.098627\n 23.710112  17.594162  23.745047  11.882985  12.744742  17.38942\n 20.453465   9.313819   9.543856   9.713183  25.545872   6.9533854\n  9.581781  13.507032  13.825835  10.589804   8.785977  27.712666\n 22.092415  18.762764   9.43505   10.463893  10.629189  19.14895\n 10.262872  11.259892   9.532998   9.4184265 17.446407  10.414327\n 12.609451   8.577112  16.145826   8.922472   9.622413  10.736712\n 20.902775   7.9670267 10.280652  18.367964  30.432808  10.3279915\n 13.251306   9.027816  15.630726  12.659622  12.907013  27.206308\n 25.783863   7.4597087  8.288127   9.559531  12.016858  11.334848\n 13.82012   27.298578  13.292222   9.903858   8.023749  10.428049\n  8.955221  20.08291   17.827583  12.127095 ]", "q_tp1_best_masked": "[ 6.882277  11.220192  15.434253   9.150591  18.256104  10.268354\n  6.9003425 12.274489  20.846626  13.233285  12.465505  11.172939\n 11.457609  26.776924   7.683978  32.660084  24.064407  10.234003\n 10.423009   9.447553  31.678772  16.108763  25.262468  22.361273\n 10.275074  26.098879  24.954952   9.202711  13.507972   9.104952\n  9.252765   9.437326  25.387379  13.702102  17.1513     8.627275\n 17.385262  25.199793  13.289022  15.284615  13.164976  29.10587\n 23.208618  10.018243   7.937216   9.871944  26.519657  16.620226\n 23.86187    8.718074  20.545715  24.319462  10.475092  16.395653\n 24.79844    9.52661   19.775656  10.563924  10.462215  27.52595\n 19.136402   8.89339   10.205911  20.866425  11.84936   21.866678\n  8.417816  19.486269  11.484396  10.714519  10.144423   7.9700522\n 17.265934  21.706282  16.75367   18.9755    22.813194  10.265136\n 10.747988  12.064108  19.253626  22.923532   8.164702  12.479049\n  9.490701  14.47667   11.192011  12.110945  22.136562  28.585375\n 26.027323   8.512152  11.3237705 25.545633  19.048428  10.03638\n 25.210299  18.326563   6.1241713 11.127389  25.017221  12.819207\n 28.169607  18.24056   11.847169  10.120954  11.025418  17.405048\n 12.225188   1.47265    8.686479  12.984938  16.44992   17.923\n 24.87968    8.658102  10.32088    9.342516  12.562559  13.321823\n 20.804707  11.857162  15.170537  20.155521  26.752106  11.67527\n 23.607988  10.911284   8.781973  10.010505  17.712076  10.151781\n  8.389643  12.28473   18.042393  13.998456  10.203659  11.394824\n 24.679174  10.524848   9.675732   9.4298315 14.232431  11.776013\n 23.259615  12.469299   7.8614173  9.670903  13.447203  11.095979\n  8.795007  17.753101  25.875715  21.75163   16.741455  19.198643\n 25.835346  11.576052  22.068928   7.6630535  8.751769   9.873936\n 13.719849   8.623491  10.916071  17.395535  11.972657   7.0148153\n 26.504278  12.402573  10.553861  12.878163   9.488886  16.308548\n 25.767334  11.724011  13.492284   9.620734   9.849227   8.838547\n 27.032225  19.265081  20.04059   11.842703  16.381853   9.085439\n 21.057451   7.431222  12.533513  13.248874  26.283493  25.864847\n 24.618635  18.885096  25.168049  13.389297  13.303302  17.967384\n 20.754862   9.848625   9.1146    10.216691  26.381445   8.31825\n 11.233675  13.906682  15.134861  10.952031   9.235885  26.974545\n 23.48961   20.891054   9.952046  10.495381  10.52536   18.77315\n 10.830839  11.652285   9.7796335 10.412047  18.522894   9.865738\n 11.839097   9.859788  17.031921   9.914775  10.923162  10.64918\n 21.069605   8.900666   9.809163  17.97743   29.948202  10.949273\n 14.546241  10.505987  16.34802   11.952354  11.910949  28.12392\n 27.535404   8.109102   9.005489  10.04198   12.218482  11.454144\n 14.561792  27.687378  12.603188   9.175564   8.079231  10.85815\n  8.744233  19.653006  18.18819   13.656182 ]", "policy_t": "[[-0.9949501   0.9294338   0.9845586  -0.9668901  -0.9814048  -0.980066  ]\n [ 0.8257003  -0.97939384 -0.28852808  0.6845107  -0.08439785 -0.44121063]\n [-0.9839664  -0.6478609  -0.99259055 -0.83858496 -0.44751215  0.73710513]\n ...\n [ 0.82079196  0.50145626  0.7879255   0.92404354  0.49216843  0.16831946]\n [-0.9758786   0.36477304  0.21796966  0.89644575 -0.20652765  0.9934628 ]\n [ 0.79075754  0.4842174   0.96581435  0.9798994   0.95765257 -0.36321306]]", "td_error": "[2.0883675  0.14193058 0.95792246 1.1051369  0.54103756 0.31826544\n 1.5625043  0.626575   0.632699   1.6211929  0.54867935 0.2286911\n 1.924365   0.3057146  0.63551736 2.1768618  1.2301111  0.18067884\n 0.8071642  0.3969593  0.37701797 2.5830517  1.084281   0.5409832\n 0.6020465  0.5469618  3.012577   0.59238577 1.8754992  1.5295682\n 1.2342253  1.3360343  0.8485031  1.0498443  0.8626919  0.45845747\n 2.0653086  0.8662081  2.0043402  1.6629095  0.83751535 0.6567097\n 0.79315567 1.2911711  3.2567558  0.7849736  1.4492073  0.21882677\n 1.2933111  0.15181875 0.6762676  0.9967098  0.12446928 0.70056295\n 1.1818237  0.80451965 0.7947321  0.4480753  0.45693016 1.7375917\n 0.6244316  1.7276034  0.78791094 0.18895245 0.27613592 1.5289707\n 1.381444   0.38113308 0.30205393 0.85716057 0.960876   1.4629169\n 1.3590136  0.48172665 0.63618326 1.1979628  0.96687984 0.4555149\n 0.6226864  0.3788848  1.7119446  0.96687984 1.6010537  3.858756\n 0.43108892 0.73270464 0.21957827 0.40727377 0.649004   2.1196184\n 0.46395874 0.90239525 0.275939   0.77587986 1.3064842  0.3330531\n 0.7668009  2.3993363  1.989841   0.89694214 1.7126837  1.137979\n 0.53445625 1.1444788  0.36927414 0.25652552 0.88748884 1.9828615\n 0.86571455 1.0943     0.6164255  0.21668386 2.355977   1.7931175\n 0.20369816 0.8054285  0.59359646 0.67766666 1.6388516  2.3331814\n 1.7146025  0.7317753  0.6487093  2.076478   0.4580593  0.36805058\n 1.4069586  1.0874534  0.633657   0.51617765 2.3065767  0.33651876\n 0.7442951  0.6110258  0.9615011  1.2320614  0.7130523  0.64128494\n 0.52088165 0.9719677  2.2436905  0.60350084 2.038189   1.005136\n 1.0933943  0.4545169  1.4209445  0.33225346 1.2659283  0.58299637\n 0.748888   0.79294586 0.8189974  0.64253426 2.4381647  0.6463804\n 1.0949345  0.45447826 0.10098553 0.6339793  0.41807604 0.38520622\n 1.0735774  0.5503006  0.65649986 2.0777884  1.2049017  1.9133801\n 0.8547058  1.0747404  0.3499527  0.5036979  0.55927706 0.8205929\n 0.7471199  2.7688437  1.3840389  0.37261868 0.36257744 0.24652696\n 0.86341476 1.0123549  1.2749186  0.89012814 0.7208519  0.26913643\n 0.19761848 1.7028804  0.9396286  0.95731163 1.850338   0.09043217\n 1.3611498  0.94950676 1.4325123  0.55412483 0.90270853 0.69919777\n 1.3328314  1.4910474  1.0705118  0.18946171 1.0948267  0.94211936\n 2.2846303  1.1226239  0.61240053 0.12252188 0.9429898  0.5214739\n 0.6687689  1.1723461  0.5856209  0.35482836 0.4634676  0.787879\n 1.359375   0.23871231 0.92737436 0.8010087  0.2617798  0.41783953\n 0.12318707 1.6118298  2.9109888  0.6525302  0.53347206 0.6682205\n 0.94766235 0.8735151  0.9429908  0.8860197  1.8400087  0.16813707\n 1.6718364  0.20292377 0.64085007 0.48348808 0.3476243  0.6050806\n 0.8231821  0.40750146 1.584593   1.2137966  0.79533863 0.7494402\n 0.7848239  1.0303764  1.3774447  0.2616639  2.0728102  1.032835\n 1.6448894  2.3731117  0.9739599  1.6952257 ]", "mean_td_error": 0.9773833155632019, "actor_loss": -15.445693969726562, "critic_loss": 0.7384626865386963, "alpha_loss": -6.0492143630981445, "alpha_value": 0.01739891804754734, "target_entropy": -6, "mean_q": 15.070923805236816, "max_q": 32.74995040893555, "min_q": 0.859959602355957, "model": {}}}}, "optimizer_steps_this_iter": 1000, "timesteps_this_iter": 1000, "done": false, "timesteps_total": 25000, "episodes_total": 25, "training_iteration": 25, "experiment_id": "10178c071fc14a23a52880ea50e3bed8", "date": "2020-04-16_15-06-32", "timestamp": 1587049592, "time_this_iter_s": 84.03996515274048, "time_total_s": 1321.022144317627, "pid": 7334, "hostname": "ip-172-31-21-251", "node_ip": "172.31.21.251", "config": {"num_workers": 0, "num_envs_per_worker": 1, "rollout_fragment_length": 1, "sample_batch_size": -1, "batch_mode": "truncate_episodes", "num_gpus": 0, "train_batch_size": 256, "model": {"conv_filters": null, "conv_activation": "relu", "fcnet_activation": "tanh", "fcnet_hiddens": [256, 256], "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action_reward": false, "state_shape": null, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_action_dist": null, "custom_options": {}, "custom_preprocessor": null}, "optimizer": {}, "gamma": 0.99, "horizon": 1000, "soft_horizon": false, "no_done_at_end": true, "env_config": {}, "env": "HalfCheetah-v3", "normalize_actions": true, "clip_rewards": null, "clip_actions": false, "preprocessor_pref": "deepmind", "lr": 0.0001, "monitor": false, "log_level": "WARN", "callbacks": {"on_episode_start": null, "on_episode_step": null, "on_episode_end": null, "on_sample_end": null, "on_train_result": null, "on_postprocess_traj": null}, "ignore_worker_failures": false, "log_sys_usage": true, "use_pytorch": false, "eager": false, "eager_tracing": false, "no_eager_on_workers": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 1, "evaluation_num_episodes": 10, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "use_exec_api": false, "sample_async": false, "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 5, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "min_iter_time_s": 1, "timesteps_per_iteration": 1000, "seed": null, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "memory": 0, "object_store_memory": 0, "memory_per_worker": 0, "object_store_memory_per_worker": 0, "input": "sampler", "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {}, "policy_mapping_fn": null, "policies_to_train": null}, "twin_q": true, "use_state_preprocessor": false, "Q_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "policy_model": {"fcnet_activation": "relu", "fcnet_hiddens": [256, 256], "hidden_activation": -1, "hidden_layer_sizes": -1}, "tau": 0.005, "initial_alpha": 1.0, "target_entropy": "auto", "n_step": 1, "buffer_size": 1000000, "prioritized_replay": true, "prioritized_replay_alpha": 0.6, "prioritized_replay_beta": 0.4, "prioritized_replay_eps": 1e-06, "prioritized_replay_beta_annealing_timesteps": 20000, "final_prioritized_replay_beta": 0.4, "optimization": {"actor_learning_rate": 0.0003, "critic_learning_rate": 0.0003, "entropy_learning_rate": 0.0003}, "grad_clip": null, "learning_starts": 10005, "target_network_update_freq": 1, "worker_side_prioritization": false, "_deterministic_loss": false, "_use_beta_distribution": false, "grad_norm_clipping": -1}, "time_since_restore": 1321.022144317627, "timesteps_since_restore": 25000, "iterations_since_restore": 25, "perf": {"cpu_util_percent": 91.3718309859155, "ram_util_percent": 11.35281690140845}, "num_healthy_workers": 0, "evaluation": {"episode_reward_max": 52.57009372647768, "episode_reward_min": -85.6214159817456, "episode_reward_mean": -24.556043403049692, "episode_len_mean": 1000.0, "episodes_this_iter": 10, "policy_reward_min": {}, "policy_reward_max": {}, "policy_reward_mean": {}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-21.067217095205468, 52.57009372647768, -54.77801275130069, -9.509254763868595, -24.971220090379315, -85.6214159817456, -26.027574496078593, -7.717154348415952, -29.578232655778205, -38.86044557420216], "episode_lengths": [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]}, "sampler_perf": {"mean_env_wait_ms": 0.2594613616372805, "mean_processing_ms": 0.11927643081511415, "mean_inference_ms": 1.1771861396059664}, "off_policy_estimator": {}}, "trial_id": "00004", "experiment_tag": "4_learning_starts=10005"}
